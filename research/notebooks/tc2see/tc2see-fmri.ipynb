{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac479a-c4bc-4ac0-96e1-8b5fe900e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "dir2 = os.path.abspath('../..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08941b1f-6092-4475-be78-46d100b1a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('X:\\\\Datasets\\\\2021 TC2See fMRI Data\\\\')\n",
    "project_path = dataset_path / 'project'\n",
    "derivatives_path = dataset_path / 'derivatives'\n",
    "\n",
    "ssd_dataset_path = Path('C:\\\\Datasets\\\\2021 TC2See fMRI Data\\\\')\n",
    "ssd_derivatives_path = ssd_dataset_path / 'derivatives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a30eb2-e388-4d8e-89a3-e71202ecfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial creation of h5 dataset for TC2See\n",
    "\n",
    "import torchio as tio\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "break\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'TC2See2021.hdf5', 'w') as f:\n",
    "    for subject_name in ('sub-01', 'sub-02'):\n",
    "        subject = f.create_group(subject_name)\n",
    "        derivatives_subject_path = derivatives_path / 'fmriprep' / subject_name\n",
    "        project_subject_path = project_path / subject_name\n",
    "        \n",
    "        t1_file_path = derivatives_subject_path / 'anat' / f'{subject_name}_desc-preproc_T1w.nii.gz'\n",
    "        t1_image = tio.ScalarImage(t1_file_path)\n",
    "        t1_image.load()\n",
    "    \n",
    "        subject['anatomy/T1w'] = t1_image.data\n",
    "        for k, v in t1_image.items():\n",
    "            if k == 'data':\n",
    "                continue\n",
    "            subject['anatomy/T1w'].attrs[k] = v\n",
    "            \n",
    "        for i in tqdm(range(6)):\n",
    "            run_id = i + 1\n",
    "            \n",
    "            bold_file_name = f'{subject_name}_task-bird_run-{run_id}_space-T1w_desc-preproc_bold'\n",
    "            bold_file_path = derivatives_subject_path / 'func' / f'{bold_file_name}.nii.gz'\n",
    "            \n",
    "            bold_metadata_path = derivatives_subject_path / 'func' / f'{bold_file_name}.json'\n",
    "            with open(bold_metadata_path) as metadata_file:\n",
    "                bold_metadata = json.load(metadata_file)\n",
    "            \n",
    "            events_file_name = f'{subject_name}_task-bird_run-{run_id}_events.tsv'\n",
    "            events_path = project_subject_path / 'func' / events_file_name\n",
    "            with open(events_path) as events_file:\n",
    "                events_text = events_file.read()\n",
    "                \n",
    "            bold_image = tio.ScalarImage(bold_file_path, events=events_text, **bold_metadata)\n",
    "            bold_image.load()\n",
    "            \n",
    "            run_group = subject.create_group(f'bird_task/runs/run_{run_id}')\n",
    "            run_group['data'] = bold_image.data\n",
    "            for k, v in bold_image.items():\n",
    "                if k == 'data':\n",
    "                    continue\n",
    "                run_group.attrs[k] = v\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8cb45-04e7-441a-9b4f-ce713733ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional normalization data\n",
    "\n",
    "def require_dataset(group, name, data):\n",
    "    group.require_dataset(name, shape=data.shape, dtype=data.dtype)\n",
    "    group[name][:] = data\n",
    "\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'TC2See2021.hdf5', 'a') as f:\n",
    "    for subject_name, subject in f.items():\n",
    "        for run in tqdm(subject['bird_task/runs'].values()):\n",
    "            data = run['data'][:]\n",
    "            \n",
    "            H, W, D, T = data.shape\n",
    "            data = torch.from_numpy(data).cuda()\n",
    "                \n",
    "            voxel_mean = data.mean(dim=3).cpu().numpy()\n",
    "            voxel_std = data.std(dim=3).cpu().numpy()\n",
    "\n",
    "            volume_mean = data.mean(dim=(0, 1, 2)).cpu().numpy()\n",
    "            volume_std = data.mean(dim=(0, 1, 2)).cpu().numpy()\n",
    "\n",
    "            run_mean = data.mean().cpu().item()\n",
    "            run_std = data.std().cpu().item()\n",
    "\n",
    "            require_dataset(run, 'voxel_mean', voxel_mean)\n",
    "            require_dataset(run, 'voxel_std', voxel_std)\n",
    "            require_dataset(run, 'volume_mean', volume_mean)\n",
    "            require_dataset(run, 'volume_std', volume_std)\n",
    "\n",
    "            run.attrs['run_mean'] = run_mean\n",
    "            run.attrs['run_std'] = run_std\n",
    "\n",
    "            B = data[..., None]\n",
    "\n",
    "            A = torch.zeros_like(data)\n",
    "            A[:, :, :, torch.arange(T)] = torch.arange(T).float().cuda()\n",
    "            A = torch.stack([A, torch.ones_like(A)], dim=-1)\n",
    "\n",
    "            X, _, _, _ = torch.linalg.lstsq(A, B)\n",
    "\n",
    "            std = (A @ X - B).std(dim=3).squeeze().cpu().numpy()\n",
    "            X = X.cpu().numpy()\n",
    "\n",
    "            require_dataset(run, 'voxel_linear_trend', X)\n",
    "            require_dataset(run, 'voxel_linear_trend_std', std)\n",
    "\n",
    "            T = volume_mean.shape[0]\n",
    "            B = torch.from_numpy(volume_mean[:, None])\n",
    "            A = torch.arange(T).float()\n",
    "            A = torch.stack([A, torch.ones_like(A)], dim=-1)\n",
    "\n",
    "            X, _, _, _ = torch.linalg.lstsq(A, B)\n",
    "\n",
    "            std = (A @ X - B).std()\n",
    "            X = X.cpu().numpy()\n",
    "\n",
    "            require_dataset(run, 'volume_linear_trend', X)\n",
    "            require_dataset(run, 'volume_linear_trend_std', np.array([std]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28ad36-154e-4805-a3c0-555454ca38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the h5 dataset\n",
    "\n",
    "from research.data.tc2see_2021 import TC2See2021\n",
    "\n",
    "features_name = 'ViT-B=32'\n",
    "features_path = derivatives_path / f\"{features_name}-features.hdf5\"\n",
    "\n",
    "dataset = TC2See2021(\n",
    "    h5_path=ssd_derivatives_path / 'TC2See2021.hdf5', \n",
    "    subjects=['sub-01'],\n",
    "    window=(3, 5),\n",
    "    window_kernel=[1. / 2.] * 2,\n",
    "    normalization='voxel_linear_trend',\n",
    "    drop_out_of_window_events=True,\n",
    "    features_path=features_path,\n",
    "    feature_keys=['embedding'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bc595-9500-4c9d-9113-71f84b246021",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b192f-4071-440c-90c8-47fb2fe6eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the events\n",
    "\n",
    "@interact(i=(0, len(dataset)-1))\n",
    "def show_event(i):\n",
    "    event = dataset[i]\n",
    "    \n",
    "    if 'features' in event:\n",
    "        for k, v in event['features'].items():\n",
    "            print(k, v.shape, v.numel())\n",
    "    \n",
    "    #print(event['onset'], event['stimulus_id'], event['run_id'])\n",
    "    data = event['data']\n",
    "    #print(data.max(), data.min())\n",
    "    #print(data.shape)\n",
    "    \n",
    "    print(event.keys())\n",
    "    \n",
    "    T, H, W, D = data.shape\n",
    "    @interact(d=(0, D-1), t=(0, T-1))\n",
    "    def show_volume(d, t):\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        x = data[t, :, :, d]\n",
    "        plt.imshow(x, cmap='bwr', vmin=-3, vmax=3)\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fac09d-b425-4aab-b400-b37de5867fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache a preprocessing option\n",
    "\n",
    "from research.data.tc2see_2021 import TC2See2021\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def require_dataset(group, name, data):\n",
    "    group.require_dataset(name, shape=data.shape, dtype=data.dtype)\n",
    "    group[name][:] = data\n",
    "\n",
    "cache_name = 'window-4-6'\n",
    "subjects = ['sub-01', 'sub-02']\n",
    "target_shape = (72, 88, 74)\n",
    "preprocessing_params = dict(\n",
    "    window=(2, 4),\n",
    "    #window_kernel=[1. / 2.] * 2,\n",
    "    normalization='voxel_linear_trend',\n",
    "    drop_out_of_window_events=True,\n",
    "    #features_path=features_path,\n",
    "    #feature_keys=['embedding'],\n",
    ")\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'TC2See2021-cached.hdf5', 'a') as f:\n",
    "    for subject in subjects:\n",
    "        print(subject)\n",
    "        \n",
    "        dataset = TC2See2021(\n",
    "            h5_path=ssd_derivatives_path / 'TC2See2021.hdf5', \n",
    "            subjects=[subject],\n",
    "            **preprocessing_params\n",
    "        )\n",
    "        N = len(dataset)\n",
    "\n",
    "        sample_event = dataset[0]\n",
    "        target_shape = sample_event['data'].shape\n",
    "\n",
    "        group = f.require_group(f'{subject}/{cache_name}')\n",
    "        data = group.require_dataset('data', shape=(N, *target_shape), dtype=float)\n",
    "        for k, v in preprocessing_params.items():\n",
    "            group.attrs[k] = str(v)\n",
    "\n",
    "        group.attrs['affine'] = sample_event['affine']\n",
    "        \n",
    "        out_data = {\n",
    "            k: [] for k in ['onset', 'duration', 'class_id', 'response', \n",
    "                            'response_time', 'same', 'tr', 'run_id', 'stimulus_id']\n",
    "        }\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            event = dataset[i]\n",
    "            for k, v in out_data.items():\n",
    "                v.append(event[k])\n",
    "            data[i] = event['data']\n",
    "        for k, v in out_data.items():\n",
    "            v = np.array(v)\n",
    "            if k == 'stimulus_id':\n",
    "                v = v.astype(np.dtype('S'))\n",
    "            require_dataset(group, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c45102-234b-4848-861c-84024a318fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation maps\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "\n",
    "def pearsonr_torch(X, Y):\n",
    "    X = X.to(torch.float64)\n",
    "    Y = Y.to(torch.float64)\n",
    "    \n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    Y = Y - Y.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    X = X / torch.norm(X, dim=0, keepdim=True)\n",
    "    Y = Y / torch.norm(Y, dim=0, keepdim=True)\n",
    "    \n",
    "    X_num_correlation_dims = len(X.shape) - 1\n",
    "    Y_num_correlation_dims = len(Y.shape) - 1\n",
    "    for i in range(Y_num_correlation_dims):\n",
    "        X = X[..., None]\n",
    "    for i in range(X_num_correlation_dims):\n",
    "        Y = Y[:, None]\n",
    "    \n",
    "    return torch.einsum('b...i,b...i->...i', X, Y)\n",
    "\n",
    "\n",
    "fmri_data = h5py.File(ssd_derivatives_path / 'TC2See2021-cached.hdf5', 'r')\n",
    "\n",
    "cache_name = 'window-4-6'\n",
    "run_features = {\n",
    "    #'bigbigan-resnet50': ['z_mean'],\n",
    "    'ViT-B=32': ['embedding', *(f'transformer.resblocks.{i}' for i in range(12))],\n",
    "    #'biggan-128': ['z', 'y_embedding'],\n",
    "    #'vqgan': ['vqgan-f16-1024-pre_quant'],\n",
    "}\n",
    "\n",
    "seed = 0\n",
    "max_features = 512\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'feature-correlation-maps.hdf5', 'a') as f:\n",
    "    for subject_name, subject in fmri_data.items():\n",
    "        print(subject_name)\n",
    "\n",
    "        if cache_name not in subject:\n",
    "            continue\n",
    "            print(f'{cache_name} not found for subject {subject_name}')\n",
    "            \n",
    "        cache = subject[cache_name]\n",
    "        affine = cache.attrs['affine']\n",
    "        group = f.require_group(f'{subject_name}/{cache_name}')\n",
    "        group.attrs['affine'] = affine\n",
    "        X = cache['data']\n",
    "\n",
    "        for model_name, feature_names in run_features.items():\n",
    "            print(model_name)\n",
    "            model_features = h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r')\n",
    "\n",
    "            for feature_name in feature_names:\n",
    "                print(feature_name)\n",
    "\n",
    "                stimulus_ids = subject[f'{cache_name}/stimulus_id'][:]\n",
    "                stimulus_ids = [s.decode('utf-8') for s in stimulus_ids]\n",
    "                Y = np.stack([model_features[f'{stimulus_id}/{feature_name}'][:]\n",
    "                              for stimulus_id in stimulus_ids])\n",
    "                Y = torch.from_numpy(Y).cuda()\n",
    "                Y = Y.flatten(start_dim=1)\n",
    "\n",
    "                if Y.shape[1] > max_features:\n",
    "                    np.random.seed(seed)\n",
    "                    choice = np.random.choice(max_features, size=max_features)\n",
    "                    Y = Y[:, choice]\n",
    "\n",
    "                correlation_map_shape = X.shape[1:] + Y.shape[1:]\n",
    "                correlation_map = f.require_dataset(f'{subject_name}/{cache_name}/{model_name}/{feature_name}', correlation_map_shape, dtype=np.float32)\n",
    "\n",
    "                load_time = 0\n",
    "                compute_time = 0\n",
    "                store_time = 0\n",
    "                for i in tqdm(range(X.shape[1])):\n",
    "\n",
    "                    t = time.time()\n",
    "                    X_slice = torch.from_numpy(X[:, i]).cuda()\n",
    "                    load_time += time.time() - t\n",
    "                    \n",
    "\n",
    "                    t = time.time()\n",
    "                    r = torch.stack([\n",
    "                        pearsonr_torch(X_slice[:, j], Y).cpu()\n",
    "                        for j in range(X_slice.shape[1])\n",
    "                    ])\n",
    "                    compute_time += time.time() - t\n",
    "\n",
    "                    t = time.time()\n",
    "                    correlation_map[i] = r\n",
    "                    store_time += time.time() - t\n",
    "\n",
    "                    if i % 25 == 0:\n",
    "                        n = (i + 1)\n",
    "                        print(f'load_time={load_time / n}, compute_time={compute_time / n}, store_time={store_time / n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072419b-c6d5-4093-b1f5-f1ec99f07f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save correlation maps\n",
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "from functools import partial\n",
    "import nibabel as nib\n",
    "\n",
    "def require_dataset(group, name, data):\n",
    "    group.require_dataset(name, shape=data.shape, dtype=data.dtype)\n",
    "    group[name][:] = data\n",
    "\n",
    "\n",
    "def mean_top_k(data, k):\n",
    "    data = np.abs(data)\n",
    "    data = np.sort(data)\n",
    "    data = data[..., -k:].mean(axis=-1)\n",
    "    return data\n",
    "\n",
    "def max_feature(data):\n",
    "    data = np.abs(data)\n",
    "    data = np.max(data, axis=-1)\n",
    "    return data\n",
    "\n",
    "selection_modes = {\n",
    "    #'max': max_feature,\n",
    "    'mean-top-5': partial(mean_top_k, k=5),\n",
    "    #'mean-top-10': partial(mean_top_k, k=10),\n",
    "}\n",
    "models = ['ViT-B=32']\n",
    "cache_names = ['window-4-6']\n",
    "\n",
    "out_path = derivatives_path / 'correlation_maps'\n",
    "correlation_maps = h5py.File(ssd_derivatives_path / 'feature-correlation-maps.hdf5', 'r')\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'feature-selection-maps.hdf5', 'a') as f:\n",
    "    for subject_name, subject in correlation_maps.items():\n",
    "        subject_out_path = out_path / subject_name\n",
    "        subject_out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for cache_name, cache in subject.items():\n",
    "            if cache_names and cache_name not in cache_names:\n",
    "                continue\n",
    "            affine = cache.attrs['affine']\n",
    "            for model_name, model in cache.items():\n",
    "                if model_name not in models:\n",
    "                    continue\n",
    "\n",
    "                for feature_name, feature_correlation_map in model.items():\n",
    "                    for selection_mode, selection_func in selection_modes.items():\n",
    "                        keys = (subject_name, cache_name, model_name, feature_name, selection_mode)\n",
    "                        print(*keys)\n",
    "\n",
    "                        save_file_name = f'{\"__\".join(keys)}.nii.gz'\n",
    "\n",
    "                        data = feature_correlation_map[:]\n",
    "                        data = selection_func(data)\n",
    "                        sorted_indices_flat = np.argsort(data, axis=None)\n",
    "\n",
    "                        T, H, W, D = data.shape\n",
    "                        grid = np.zeros(shape=(4, T, H, W, D), dtype=int)\n",
    "                        grid[0] = np.arange(T)[:, None, None, None]\n",
    "                        grid[1] = np.arange(H)[None, :, None, None]\n",
    "                        grid[2] = np.arange(W)[None, None, :, None]\n",
    "                        grid[3] = np.arange(D)[None, None, None, :]\n",
    "                        grid_flat = grid.reshape(4, T * H * W * D)\n",
    "                        sorted_indices = grid_flat[:, sorted_indices_flat]\n",
    "                        \n",
    "                        image = nib.Nifti1Image(torch.tensor(data).permute(1, 2, 3, 0).numpy(), affine)\n",
    "                        nib.save(image, subject_out_path / save_file_name)\n",
    "                        #image = tio.ScalarImage(tensor=torch.tensor(data), affine=affine)\n",
    "                        #image.save(subject_out_path / save_file_name)\n",
    "\n",
    "                        group = f.require_group('/'.join(keys))\n",
    "                        require_dataset(group, 'scores', data)\n",
    "                        require_dataset(group, 'sorted_indices_flat', sorted_indices_flat)\n",
    "                        require_dataset(group, 'sorted_indices', sorted_indices)\n",
    "\n",
    "correlation_maps.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d6d00-11c8-4716-823e-24d15ddaa9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack feature selection map\n",
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "from functools import partial\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "cache_name = 'window-4-6'\n",
    "model_name = 'ViT-B=32'\n",
    "feature_names = [*(f'transformer.resblocks.{i}' for i in range(12)), 'embedding']\n",
    "selection_mode = 'mean-top-5'\n",
    "stack_name = 'depth-sequence-time-4'\n",
    "channel = 0\n",
    "\n",
    "correlation_maps = h5py.File(ssd_derivatives_path / 'feature-correlation-maps.hdf5', 'r')\n",
    "out_path = derivatives_path / 'correlation_maps'\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'feature-selection-maps.hdf5', 'a') as f:\n",
    "    for subject_name, subject in correlation_maps.items():\n",
    "        subject_out_path = out_path / subject_name\n",
    "        subject_out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        affine = correlation_maps[subject_name][cache_name].attrs['affine']\n",
    "        features = f[subject_name][cache_name][model_name]\n",
    "        data = np.stack([\n",
    "            features[feature_name][selection_mode]['scores'][channel]\n",
    "            for feature_name in feature_names\n",
    "        ])\n",
    "        print(data.shape)\n",
    "        \n",
    "        keys = (subject_name, cache_name, model_name, selection_mode, stack_name,)\n",
    "        print(*keys)\n",
    "\n",
    "        save_file_name = f'{\"__\".join(keys)}.nii.gz'\n",
    "        image = nib.Nifti1Image(torch.tensor(data).permute(1, 2, 3, 0).numpy(), affine)\n",
    "        nib.save(image, subject_out_path / save_file_name)\n",
    "\n",
    "correlation_maps.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efa782-27de-4d90-9ab9-0b2a18f772cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle estimator\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "\n",
    "def pearsonr_torch(X, Y):\n",
    "    X = X.to(torch.float64)\n",
    "    Y = Y.to(torch.float64)\n",
    "    \n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    Y = Y - Y.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    X = X / torch.norm(X, dim=0, keepdim=True)\n",
    "    Y = Y / torch.norm(Y, dim=0, keepdim=True)\n",
    "    \n",
    "    X_num_correlation_dims = len(X.shape) - 1\n",
    "    Y_num_correlation_dims = len(Y.shape) - 1\n",
    "    for i in range(Y_num_correlation_dims):\n",
    "        X = X[..., None]\n",
    "    for i in range(X_num_correlation_dims):\n",
    "        Y = Y[:, None]\n",
    "    \n",
    "    return torch.einsum('b...i,b...i->...i', X, Y)\n",
    "\n",
    "\n",
    "def max_pearsonr_partition_estimator(X, Y, partitions, seed=0):\n",
    "    N = X_slice.shape[0]\n",
    "    \n",
    "    measurements = []\n",
    "    for j in tqdm(range(partitions)):\n",
    "        np.random.seed(seed + j)\n",
    "\n",
    "        indices = np.arange(N)\n",
    "        np.random.shuffle(indices)\n",
    "        indices1, indices2 = np.split(indices, 2)\n",
    "\n",
    "        X1, Y1 = X[indices1], Y[indices1]\n",
    "        X2, Y2 = X[indices2], Y[indices2]\n",
    "\n",
    "        r1 = torch.abs(torch.stack([\n",
    "            pearsonr_torch(X1[:, k], Y1).cpu()\n",
    "            for k in range(X1.shape[1])\n",
    "        ]))\n",
    "\n",
    "        r2 = torch.abs(torch.stack([\n",
    "            pearsonr_torch(X2[:, k], Y2).cpu()\n",
    "            for k in range(X2.shape[1])\n",
    "        ]))\n",
    "\n",
    "        max_ids = r1.argmax(axis=-1)\n",
    "        measurement = torch.gather(r2, -1, max_ids[..., None])\n",
    "        measurements.append(measurement)\n",
    "    \n",
    "    r = torch.cat(measurements, dim=-1)\n",
    "    return r\n",
    "\n",
    "\n",
    "fmri_data = h5py.File(ssd_derivatives_path / 'TC2See2021-cached.hdf5', 'r')\n",
    "\n",
    "cache_name = 'window-4-6'\n",
    "run_features = {\n",
    "    #'bigbigan-resnet50': ['z_mean'],\n",
    "    'ViT-B=32': ['embedding'],#, *(f'transformer.resblocks.{i}' for i in (2, 5, 8, 11))],\n",
    "    #'biggan-128': ['z', 'y_embedding'],\n",
    "    #'vqgan': ['vqgan-f16-1024-pre_quant'],\n",
    "}\n",
    "\n",
    "seed = 0\n",
    "max_features = 512\n",
    "partitions = 50\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'feature-max-correlation-maps.hdf5', 'a') as f:\n",
    "    for subject_name, subject in fmri_data.items():\n",
    "        print(subject_name)\n",
    "\n",
    "        if cache_name not in subject:\n",
    "            continue\n",
    "            print(f'{cache_name} not found for subject {subject_name}')\n",
    "            \n",
    "        cache = subject[cache_name]\n",
    "        affine = cache.attrs['affine']\n",
    "        group = f.require_group(f'{subject_name}/{cache_name}')\n",
    "        group.attrs['affine'] = affine\n",
    "        X = cache['data']\n",
    "\n",
    "        for model_name, feature_names in run_features.items():\n",
    "            print(model_name)\n",
    "            model_features = h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r')\n",
    "\n",
    "            for feature_name in feature_names:\n",
    "                print(feature_name)\n",
    "\n",
    "                stimulus_ids = subject[f'{cache_name}/stimulus_id'][:]\n",
    "                stimulus_ids = [s.decode('utf-8') for s in stimulus_ids]\n",
    "                Y = np.stack([model_features[f'{stimulus_id}/{feature_name}'][:]\n",
    "                              for stimulus_id in stimulus_ids])\n",
    "                Y = torch.from_numpy(Y).cuda()\n",
    "                Y = Y.flatten(start_dim=1)\n",
    "\n",
    "                if Y.shape[1] > max_features:\n",
    "                    np.random.seed(seed)\n",
    "                    choice = np.random.choice(max_features, size=max_features)\n",
    "                    Y = Y[:, choice]\n",
    "\n",
    "                correlation_map_shape = X.shape[1:] + Y.shape[1:-2] + (partitions,)\n",
    "                print(correlation_map_shape)\n",
    "\n",
    "                correlation_map = f.require_dataset(f'{subject_name}/{cache_name}/{model_name}/{feature_name}', correlation_map_shape, dtype=np.float32)\n",
    "\n",
    "                load_time = 0\n",
    "                compute_time = 0\n",
    "                store_time = 0\n",
    "                for i in tqdm(range(X.shape[1])):\n",
    "\n",
    "                    t = time.time()\n",
    "                    X_slice = torch.from_numpy(X[:, i]).cuda()\n",
    "                    load_time += time.time() - t\n",
    "                    \n",
    "                    N = X_slice.shape[0]\n",
    "\n",
    "                    t = time.time()\n",
    "                    r = max_pearsonr_partition_estimator(X_slice, Y, partitions, seed=seed)\n",
    "                    compute_time += time.time() - t\n",
    "\n",
    "                    t = time.time()\n",
    "                    correlation_map[i] = r\n",
    "                    store_time += time.time() - t\n",
    "\n",
    "                    if i % 25 == 0:\n",
    "                        n = (i + 1)\n",
    "                        print(f'load_time={load_time / n}, compute_time={compute_time / n}, store_time={store_time / n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1156814-e6d7-4f9c-a774-4c2c40f7b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save correlation maps\n",
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "from functools import partial\n",
    "import nibabel as nib\n",
    "\n",
    "def require_dataset(group, name, data):\n",
    "    group.require_dataset(name, shape=data.shape, dtype=data.dtype)\n",
    "    group[name][:] = data\n",
    "\n",
    "\n",
    "models = ['ViT-B=32']\n",
    "cache_names = ['window-4-6']\n",
    "\n",
    "out_path = derivatives_path / 'correlation_maps'\n",
    "correlation_maps = h5py.File(ssd_derivatives_path / 'feature-max-correlation-maps.hdf5', 'r')\n",
    "\n",
    "with h5py.File(ssd_derivatives_path / 'feature-selection-maps.hdf5', 'a') as f:\n",
    "    for subject_name, subject in correlation_maps.items():\n",
    "        subject_out_path = out_path / subject_name\n",
    "        subject_out_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for cache_name, cache in subject.items():\n",
    "            if cache_names and cache_name not in cache_names:\n",
    "                continue\n",
    "            affine = cache.attrs['affine']\n",
    "            for model_name, model in cache.items():\n",
    "                if model_name not in models:\n",
    "                    continue\n",
    "\n",
    "                for feature_name, feature_correlation_map in model.items():\n",
    "                    keys = (subject_name, cache_name, model_name, feature_name, 'partition-max')\n",
    "                    print(*keys)\n",
    "\n",
    "                    save_file_name = f'{\"__\".join(keys)}.nii.gz'\n",
    "\n",
    "                    data = feature_correlation_map[:]\n",
    "                    data = np.mean(data, axis=-1)\n",
    "                    sorted_indices_flat = np.argsort(data, axis=None)\n",
    "\n",
    "                    T, H, W, D = data.shape\n",
    "                    grid = np.zeros(shape=(4, T, H, W, D), dtype=int)\n",
    "                    grid[0] = np.arange(T)[:, None, None, None]\n",
    "                    grid[1] = np.arange(H)[None, :, None, None]\n",
    "                    grid[2] = np.arange(W)[None, None, :, None]\n",
    "                    grid[3] = np.arange(D)[None, None, None, :]\n",
    "                    grid_flat = grid.reshape(4, T * H * W * D)\n",
    "                    sorted_indices = grid_flat[:, sorted_indices_flat]\n",
    "\n",
    "                    image = nib.Nifti1Image(torch.tensor(data).permute(1, 2, 3, 0).numpy(), affine)\n",
    "                    nib.save(image, subject_out_path / save_file_name)\n",
    "                    #image = tio.ScalarImage(tensor=torch.tensor(data), affine=affine)\n",
    "                    #image.save(subject_out_path / save_file_name)\n",
    "\n",
    "                    group = f.require_group('/'.join(keys))\n",
    "                    require_dataset(group, 'scores', data)\n",
    "                    require_dataset(group, 'sorted_indices_flat', sorted_indices_flat)\n",
    "                    require_dataset(group, 'sorted_indices', sorted_indices)\n",
    "\n",
    "correlation_maps.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c0e80-abae-4877-86ab-e9726cda10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7434f2-2180-42eb-849c-82fe4b4168ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166a5d7-c23a-4517-8286-c36e4954d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 100, 100)\n",
    "max_ids = x.argmax(axis=-1)\n",
    "torch.gather(x, -1, max_ids[..., None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bff7e-7939-4034-a3b6-8d144fa7e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tio.ScalarImage(out_path / 'sub-01'/ 'sub-01__large-window__ViT-B=32__embedding__max.nii.gz')\n",
    "image.load()\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62535bd8-8d8a-498e-8130-a2d01ead021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "array_img = nib.Nifti1Image(image['data'].numpy(), image['affine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18456d3c-12f9-4dc6-97f6-2857fd9af4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_maps = h5py.File(ssd_derivatives_path / 'feature-correlation-maps.hdf5', 'r')\n",
    "#print(list(correlation_maps['sub-01/large-window/ViT-B=32/embedding'].keys()))\n",
    "\n",
    "data = correlation_maps['sub-01/large-window/ViT-B=32/embedding'][:]\n",
    "@interact(t=(0, data.shape[0]-1), d=(0, data.shape[3]-1), f=(0, data.shape[4]-1))\n",
    "def show(t, d, f):\n",
    "    plt.imshow(data[t, :, :, d, f], vmin=0, vmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18569cfd-ac0b-4f88-813a-8dfd9e7a829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_data = h5py.File(ssd_derivatives_path / 'TC2See2021-cached.hdf5', 'r')\n",
    "\n",
    "data = cached_data['sub-01/large-window/data'][:10]\n",
    "@interact(b=(0, data.shape[0]-1), t=(0, data.shape[1]-1), d=(0, data.shape[4]-1))\n",
    "def show(b, t, d):\n",
    "    plt.imshow(data[b, t, :, :, d], vmin=-3, vmax=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe828a-723f-4387-a2c7-164714bca649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897b484-15dc-4ab6-a203-5cc002b1b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vedo import Volume, dataurl\n",
    "from vedo.applications import RayCastPlotter\n",
    "\n",
    "embryo = Volume(dataurl+\"embryo.slc\").mode(1).c('jet') # change properties\n",
    "\n",
    "plt = RayCastPlotter(embryo) # Plotter instance\n",
    "plt.show(viewup=\"z\", bg='black', bg2='blackboard', axes=7).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08befd02-732d-49f7-b0ee-b3f80dcb4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(ssd_derivatives_path / 'feature-selection-maps.hdf5', 'r')\n",
    "\n",
    "subject = 'sub-02'\n",
    "cache_name = 'window-4-6'\n",
    "model_name = 'ViT-B=32'\n",
    "feature_name = 'transformer.resblocks.0'\n",
    "selection_mode = 'mean-top-5'\n",
    "\n",
    "key = '/'.join([subject, cache_name, model_name, feature_name, selection_mode, 'scores'])\n",
    "\n",
    "data = f[key][0]\n",
    "(~np.isnan(data)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1fa6b-0368-46dc-ba4a-cde40f99bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stuff(k, v):\n",
    "    if not isinstance(v, h5py.Dataset):\n",
    "        return\n",
    "    if not v.name.endswith('scores'):\n",
    "        return\n",
    "    \n",
    "    data = v[:]\n",
    "    #data[np.isnan(data)] = 0.\n",
    "    #v[:] = data\n",
    "    print(v.name, v, np.isnan(v[:]).sum())\n",
    "    \n",
    "with h5py.File(ssd_derivatives_path / 'feature-selection-maps.hdf5', 'a') as f:\n",
    "    f.visititems(stuff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
