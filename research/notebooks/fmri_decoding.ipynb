{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfc4b29-e1d9-49fb-8536-03d8a9cd6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43afeb4a-2dd3-4441-a25a-f61cc1b5a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kamitani2019 dataset (author format and preprocessing)\n",
    "\n",
    "from research.data.kamitani_2019 import Kamitani2019H5Preprocessed\n",
    "from research.data.kamitani_2017 import Kamitani2017H5Preprocessed\n",
    "\n",
    "#features_model = 'biggan-128'\n",
    "#features_model = 'bigbigan-resnet50'\n",
    "#features_model = 'RN50'\n",
    "features_model = 'ViT-B=32'\n",
    "#features_model = 'vqgan'\n",
    "\n",
    "config = '2019'\n",
    "\n",
    "if config == '2019':\n",
    "    root = \"X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\kamitani-preprocessed\"\n",
    "    subject = 'sub-03'\n",
    "    index_name = 'image_index'\n",
    "    index_type = int\n",
    "    training_repetitions = 5\n",
    "    test_repetitions = 24\n",
    "    dataset_class = Kamitani2019H5Preprocessed\n",
    "elif config == '2017':\n",
    "    root = \"X:\\\\Datasets\\\\Generic-Object-Decoding\\\\\"\n",
    "    subject = 'Subject1'\n",
    "    index_name = 'stimulus_name'\n",
    "    index_type = float\n",
    "    training_repetitions = 1\n",
    "    test_repetitions = 35\n",
    "    dataset_class = Kamitani2017H5Preprocessed\n",
    "\n",
    "dataset = dataset_class(\n",
    "    root, \n",
    "    subjects=[subject,], \n",
    "    func_sessions=['natural_training', 'natural_test'],\n",
    "    features_path=f'X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\{features_model}-features.hdf5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42f1283-caee-416e-b872-5c069f751ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '1443537.022563', 2: '1621127.019020', 3: '1677366.018182', 4: '1846331.017038', 5: '1858441.011077', 6: '1943899.024131', 7: '1976957.013223', 8: '2071294.046212', 9: '2128385.020264', 10: '2139199.010398', 11: '2190790.015121', 12: '2274259.024319', 13: '2416519.012793', 14: '2437136.012836', 15: '2437971.005013', 16: '2690373.007713', 17: '2797295.015411', 18: '2824058.018729', 19: '2882301.014188', 20: '2916179.024850', 21: '2950256.022949', 22: '2951358.023759', 23: '3064758.038750', 24: '3122295.031279', 25: '3124170.013920', 26: '3237416.058334', 27: '3272010.011001', 28: '3345837.012501', 29: '3379051.008496', 30: '3452741.024622', 31: '3455488.028622', 32: '3482252.022530', 33: '3495258.009895', 34: '3584254.005040', 35: '3626115.019498', 36: '3710193.022225', 37: '3716966.028524', 38: '3761084.043533', 39: '3767745.000109', 40: '3941684.021672', 41: '3954393.010038', 42: '4210120.009062', 43: '4252077.010859', 44: '4254777.016338', 45: '4297750.025624', 46: '4387400.016693', 47: '4507155.021299', 48: '4533802.019479', 49: '4554684.053399', 50: '4572121.003262'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.stimulus_info['natural_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac189a9-8ab1-4d8c-9fbe-2da29728c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ROI and feature layers\n",
    "\n",
    "X_key = 'ROI_VC'\n",
    "\n",
    "#Y_key = 'visual.layer4.7.bn3'\n",
    "#Y_key = 'visual'\n",
    "#Y_key = 'layer4.2.add'\n",
    "#Y_key = 'z_mean'\n",
    "#Y_key = 'attnpool.getitem_8'\n",
    "Y_key = 'embedding'\n",
    "#Y_key = 'vqgan-f16-1024-latent'\n",
    "#Y_key = 'z'\n",
    "#Y_key = 'y_embedding'\n",
    "\n",
    "classification = False\n",
    "data = dataset.get_data(brain_keys=[X_key, index_name], feature_keys=[Y_key])\n",
    "#Y_shape = data[subject]['natural_training'][Y_key].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063307c8-465b-40bc-8075-48a02be49fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    session_data, \n",
    "    X_key, \n",
    "    Y_key, \n",
    "    max_features=None, \n",
    "    average_repetitions=False, \n",
    "    num_repetitions=None, \n",
    "    split=None,\n",
    "    seed=0\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if len(session_data[Y_key].shape) > 2:\n",
    "        session_data[Y_key] = torch.flatten(torch.from_numpy(session_data[Y_key]), start_dim=1).numpy()\n",
    "    \n",
    "    if max_features is not None:\n",
    "        if session_data[Y_key].shape[1] > max_features:\n",
    "            choice = np.random.choice(max_features, size=max_features)\n",
    "            session_data[Y_key] = session_data[Y_key][:, choice]\n",
    "        \n",
    "    image_index = session_data[index_name].astype(index_type)[:, 0]\n",
    "    X = session_data[X_key]\n",
    "    Y = session_data[Y_key]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    if average_repetitions:\n",
    "        assert num_repetitions is not None\n",
    "        \n",
    "        sorted_indices = np.argsort(image_index)\n",
    "        X = X[sorted_indices]\n",
    "        Y = Y[sorted_indices]\n",
    "        \n",
    "        N = N // num_repetitions\n",
    "        X = np.stack([x.mean(axis=0) for x in np.split(X, N)])\n",
    "        Y = np.stack([y[0] for y in np.split(Y, N)])\n",
    "        image_index = np.arange(1, N + 1)\n",
    "        \n",
    "    if split:\n",
    "        unique_indices, inverse_indices = np.unique(image_index, return_inverse=True)\n",
    "\n",
    "        train_indices, _ = train_test_split(unique_indices, train_size=split)\n",
    "\n",
    "        train_indices = set(train_indices)\n",
    "        train_mask = np.array([i in train_indices for i in image_index])\n",
    "        test_mask = ~train_mask\n",
    "\n",
    "        N_train = train_mask.sum()\n",
    "        N_test = N - N_train\n",
    "        \n",
    "        X_train, Y_train = X[train_mask], Y[train_mask]\n",
    "        X_test, Y_test = X[test_mask], Y[test_mask]\n",
    "        \n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#max_features = None\n",
    "max_features = 1024\n",
    "X_train, Y_train, X_val, Y_val = preprocess(data[subject]['natural_training'], \n",
    "                                            max_features=max_features, X_key=X_key, Y_key=Y_key, \n",
    "                                            average_repetitions=False, num_repetitions=training_repetitions,\n",
    "                                            split=0.8)\n",
    "\n",
    "X_test, Y_test = preprocess(data[subject]['natural_test'], \n",
    "                            max_features=max_features, X_key=X_key, Y_key=Y_key, \n",
    "                            average_repetitions=True, num_repetitions=test_repetitions,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8463de-3a7c-445b-99ec-9882252156ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4800, 13099), (4800, 512), (1200, 13099), (1200, 512), (50, 13099), (50, 512)]\n"
     ]
    }
   ],
   "source": [
    "print([item.shape for item in (X_train, Y_train, X_val, Y_val, X_test, Y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba473a8a-e9c5-4d8c-a15d-29ad96363ede",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (Temp/ipykernel_14228/2025871939.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Cefir\\AppData\\Local\\Temp/ipykernel_14228/2025871939.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_val = X_scaler.transform(X_val)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "break\n",
    "if classification:\n",
    "    pass\n",
    "else:\n",
    "    Y_scaler = StandardScaler()\n",
    "    Y_train = Y_scaler.fit_transform(Y_train)\n",
    "    Y_val = Y_scaler.transform(Y_val)\n",
    "    Y_test = Y_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2695cb90-9092-4eec-bf02-63f3e9513b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def pearsonr(Y, Y_pred, axis=0):\n",
    "    Y = Y.astype(np.double)\n",
    "    Y_pred = Y_pred.astype(np.double)\n",
    "    \n",
    "    Y = Y - Y.mean(axis=axis, keepdims=True)\n",
    "    Y_pred = Y_pred - Y_pred.mean(axis=axis, keepdims=True)\n",
    "    \n",
    "    Y = Y / scipy.linalg.norm(Y, axis=axis,  keepdims=True)\n",
    "    Y_pred = Y_pred / scipy.linalg.norm(Y_pred, axis=axis,  keepdims=True)\n",
    "    \n",
    "    return (Y * Y_pred).sum(axis=axis).mean()\n",
    "\n",
    "def cosine_similarity(Y, Y_pred, axis=0):\n",
    "    Y = Y.astype(np.double)\n",
    "    Y_pred = Y_pred.astype(np.double)\n",
    "    \n",
    "    Y = Y / scipy.linalg.norm(Y, axis=axis, keepdims=True)\n",
    "    Y_pred = Y_pred / scipy.linalg.norm(Y_pred, axis=axis,  keepdims=True)\n",
    "    \n",
    "    return (Y * Y_pred).sum(axis=axis).mean()\n",
    "\n",
    "def pearsonr_scipy(Y, Y_pred):\n",
    "    r = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        r.append(scipy.stats.pearsonr(Y[:, i], Y_pred[:, i])[0])\n",
    "    return np.mean(r)\n",
    "    \n",
    "def r2_score(Y, Y_pred):\n",
    "    ssr = ((Y - Y_pred) ** 2).sum(axis=0)\n",
    "    sst = ((Y - Y.mean(axis=0, keepdims=True)) ** 2).sum(axis=0)\n",
    "    return (1 - ssr / sst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf71cb4b-1294-4438-942b-2a21fb424e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 47.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-batch 0.44890961979031957 0.1674159272244446 0.2572398542703962\n",
      "r-row 0.7875615338259986 0.7143328736393799 0.7445208071891645\n",
      "cs-batch 0.5349494151022156 0.3170199659361004 0.38590073881393633\n",
      "cs-row 0.7873143325248458 0.7139014140473315 0.7441747255531432\n"
     ]
    }
   ],
   "source": [
    "# Fit ridge regression with the FastL2LiR from the Kamitani group.\n",
    "from sklearn.metrics import r2_score\n",
    "from fastl2lir import FastL2LiR\n",
    "\n",
    "model = FastL2LiR()\n",
    "model.fit(X_train, Y_train, alpha=100.0, n_feat=500)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_val_pred = model.predict(X_val)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('r-batch', pearsonr(Y_train, Y_train_pred), pearsonr(Y_val, Y_val_pred), pearsonr(Y_test, Y_test_pred))\n",
    "print('r-row', pearsonr(Y_train, Y_train_pred, axis=1), pearsonr(Y_val, Y_val_pred, axis=1), pearsonr(Y_test, Y_test_pred, axis=1))\n",
    "print('cs-batch', cosine_similarity(Y_train, Y_train_pred), cosine_similarity(Y_val, Y_val_pred), cosine_similarity(Y_test, Y_test_pred))\n",
    "print('cs-row', cosine_similarity(Y_train, Y_train_pred, axis=1), cosine_similarity(Y_val, Y_val_pred, axis=1), cosine_similarity(Y_test, Y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77bae5-4dda-48a2-8b58-710971dc2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pipelines = []\n",
    "for i in tqdm(range(Y_train.shape[1])):\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(f_regression, k=500)),\n",
    "        ('model', Ridge(alpha=100.))\n",
    "        #('model', SVR())\n",
    "    ])\n",
    "    pipeline.fit(X_train, Y_train[:, i])\n",
    "    pipelines.append(pipeline)\n",
    "\n",
    "Y_train_pred = np.stack([pipeline.predict(X_train) for pipeline in pipelines], axis=1)\n",
    "Y_val_pred = np.stack([pipeline.predict(X_val) for pipeline in pipelines], axis=1)\n",
    "Y_test_pred = np.stack([pipeline.predict(X_test) for pipeline in pipelines], axis=1)\n",
    "\n",
    "print('r', pearsonr(Y_train, Y_train_pred), pearsonr(Y_val, Y_val_pred), pearsonr(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079fb199-98fd-4388-92c2-94cafcaf0339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cefir\\anaconda3\\envs\\Neurophysiological-Data-Decoding\\lib\\site-packages\\numpy\\linalg\\linalg.py:2561: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n"
     ]
    }
   ],
   "source": [
    "def get_top_k_correlation_indices(X, Y, k):\n",
    "    X = X / np.linalg.norm(X, axis=0, keepdims=True)\n",
    "    Y = Y / np.linalg.norm(Y, axis=0, keepdims=True)\n",
    "    \n",
    "    return np.stack([\n",
    "        np.abs(X.T @ Y[:, i]).argsort(axis=0)[-k:]\n",
    "        for i in range(Y.shape[1])\n",
    "    ])\n",
    "\n",
    "top_k_correlation_indices = get_top_k_correlation_indices(X_train, Y_train, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883b92-2ad9-4c3a-b9fb-3b105a3c3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in tqdm(range(Y_train.shape[1])):\n",
    "    model = Ridge(alpha=100.)\n",
    "    model.fit(X_train[:, top_k_correlation_indices[i]], Y_train[:, i])\n",
    "    models.append(model)\n",
    "\n",
    "Y_train_pred = np.stack([model.predict(X_train[:, top_k_correlation_indices[i]]) for i, model in enumerate(models)], axis=1)\n",
    "Y_val_pred = np.stack([model.predict(X_val[:, top_k_correlation_indices[i]]) for i, model in enumerate(models)], axis=1)\n",
    "Y_test_pred = np.stack([model.predict(X_test[:, top_k_correlation_indices[i]]) for i, model in enumerate(models)], axis=1)\n",
    "\n",
    "print('r', pearsonr(Y_train, Y_train_pred), pearsonr(Y_val, Y_val_pred), pearsonr(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3bd458-5e5a-463d-afc0-2d70a252194b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SparseLayer(in_features=500, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep learning \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SparseLayer(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        selection_indices: torch.Tensor,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, **kwargs)\n",
    "        assert out_features == selection_indices.shape[0]\n",
    "        self.register_buffer('selection_indices', selection_indices)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x[:, self.selection_indices]\n",
    "        x = x * self.weight\n",
    "        x = x.sum(dim=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return 1. - self.cosine_similarity(x, y).mean()\n",
    "\n",
    "\n",
    "def pearsonr_torch(Y, Y_pred, dim=0):\n",
    "    Y = Y.to(torch.float64)\n",
    "    Y_pred = Y_pred.to(torch.float64)\n",
    "    \n",
    "    Y = Y - Y.mean(dim=dim, keepdim=True)\n",
    "    Y_pred = Y_pred - Y_pred.mean(dim=dim, keepdim=True)\n",
    "    \n",
    "    Y = Y / torch.norm(Y, dim=dim, keepdim=True)\n",
    "    Y_pred = Y_pred / torch.norm(Y_pred, dim=dim, keepdim=True)\n",
    "    \n",
    "    return (Y * Y_pred).sum(dim=dim).mean().item()\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    SparseLayer(in_features=500, out_features=512, selection_indices=torch.from_numpy(top_k_correlation_indices)),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(512, 512),\n",
    ")\n",
    "\n",
    "#criterion = CosineSimilarityLoss()\n",
    "#riterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=0.1)\n",
    "dataset_training = TensorDataset(torch.from_numpy(X_train).float(), \n",
    "                                 torch.from_numpy(Y_train).float())\n",
    "dataset_validation = TensorDataset(torch.from_numpy(X_val).float(), \n",
    "                                   torch.from_numpy(Y_val).float())\n",
    "dataset_test = TensorDataset(torch.from_numpy(X_test).float(), \n",
    "                             torch.from_numpy(Y_test).float())\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6d468dc-1761-4ad5-94e0-d6c1e3c5e16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-batch 0.5547963927015711 0.24145324461490764 0.3080806872471269\n",
      "r-row 0.8348170976119066 0.7404708566743083 0.7520718797145054\n",
      "r-batch 0.5652137497440708 0.23906500646119147 0.3110986830628377\n",
      "r-row 0.8402669858749717 0.7400986929005368 0.7513049790806676\n",
      "r-batch 0.5754995827219875 0.23889647244966924 0.31220508174371564\n",
      "r-row 0.8439707166373858 0.7392252709719693 0.751670731993461\n",
      "r-batch 0.5834982405064885 0.23727718407131712 0.3106389147329606\n",
      "r-row 0.8480051379346375 0.7386184315242165 0.7503096790812159\n",
      "r-batch 0.5895570707535469 0.23670333718632613 0.31070251080419564\n",
      "r-row 0.8507802937507634 0.737970206581787 0.7498160205458423\n",
      "r-batch 0.5936144926383666 0.23524993645301634 0.3073936595096415\n",
      "r-row 0.8527225712091138 0.7379314531259841 0.7504854012051848\n"
     ]
    }
   ],
   "source": [
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=128)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None]) for x, _ in dataset]).cpu()\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "max_iterations = 1500\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y, y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_train_pred = run_all(dataset_training)\n",
    "            Y_val_pred = run_all(dataset_validation)\n",
    "            Y_test_pred = run_all(dataset_test)\n",
    "        \n",
    "        print('r-batch', \n",
    "              pearsonr_torch(dataset_training.tensors[1], Y_train_pred), \n",
    "              pearsonr_torch(dataset_validation.tensors[1], Y_val_pred), \n",
    "              pearsonr_torch(dataset_test.tensors[1], Y_test_pred))\n",
    "        print('r-row', \n",
    "              pearsonr_torch(dataset_training.tensors[1], Y_train_pred, dim=1), \n",
    "              pearsonr_torch(dataset_validation.tensors[1], Y_val_pred, dim=1), \n",
    "              pearsonr_torch(dataset_test.tensors[1], Y_test_pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3fc84-1461-4f80-a1ae-076a0d7f174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2e1f4-0922-4e93-9403-11315919288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\')\n",
    "\n",
    "features_model\n",
    "\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test-prediction__v2.npy', Y_scaler.inverse_transform(Y_test_pred).reshape(50, 128))#\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test__v2.npy', Y_scaler.inverse_transform(Y_test).reshape(50, 128))#*Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e110a942-a624-4891-82c0-e58006f63123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\')\n",
    "\n",
    "features_model\n",
    "\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test-prediction__v2.npy', Y_test_pred.reshape(50, 512))#\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test__v2.npy', Y_test.reshape(50, 512))#*Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d5e35-90e5-4bee-a04f-678cd2aeca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pipelines = []\n",
    "for i in tqdm(range(Y_train.shape[1])):\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(f_classif, k=500)),\n",
    "        ('model', LogisticRegression(solver='liblinear'))\n",
    "    ])\n",
    "    pipeline.fit(X_train, Y_train[:, i])\n",
    "    pipelines.append(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9f7a9-3728-49cd-90d0-40756f31a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "select_k_best = SelectKBest(f_classif, k=5)\n",
    "X_train_selection = select_k_best.fit_transform(X_train, Y_train[:, 0])\n",
    "\n",
    "model = LogisticRegression(solver='saga', C=1000.0)\n",
    "model.fit(X_train_selection, Y_train[:, 0])\n",
    "\n",
    "X_test_selection = select_k_best.transform(X_test,)\n",
    "print(model.score(X_train_selection, Y_train[:, 0]), model.score(X_test_selection, Y_test[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513357f-c059-42da-85a0-110f3aa4ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_pickle('G:\\\\Github Repositories\\\\GenericObjectDecoding\\\\code\\\\python\\\\results\\\\GenericObjectDecoding.pkl')\n",
    "#print(df)\n",
    "df = df[df['feature'] == 'cnn8']\n",
    "\n",
    "true_feature_averaged_percept = df['true_feature_averaged_percept'].array[0]\n",
    "predicted_feature_averaged_percept = df['predicted_feature_averaged_percept'].array[0]\n",
    "print(predicted_feature_averaged_percept.shape)\n",
    "\n",
    "print(r2_score(true_feature_averaged_percept, predicted_feature_averaged_percept))\n",
    "print(pearsonr(true_feature_averaged_percept, predicted_feature_averaged_percept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82627f4d-d528-4439-a8cc-0402780d3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_stuff = Y_train.reshape(4800, *Y_shape)\n",
    "Y_stuff.std(axis=(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neurophysiological-Data-Decoding",
   "language": "python",
   "name": "neurophysiological-data-decoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
