{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226c5508-b00b-43e1-83ae-87b4f6ff51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a687a-96ff-4fec-9b31-a8b44b17c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import clip\n",
    "from collections import OrderedDict\n",
    "\n",
    "clip_model, _ = clip.load('RN50x16', device='cpu')\n",
    "clip_model = nn.Sequential(OrderedDict([\n",
    "    ('wrapped', clip_model.visual),\n",
    "    ('logits', nn.Identity())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917675ae-0d73-48c6-ab88-3303174725a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_module.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346e7678-2ee7-401f-bc1e-f24fc61cacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from model_tools.check_submission import check_models\n",
    "import torch\n",
    "from torch import nn\n",
    "import functools\n",
    "from model_tools.activations.pytorch import PytorchWrapper\n",
    "from model_tools.brain_transformation import ModelCommitment\n",
    "from model_tools.activations.pytorch import load_preprocess_images\n",
    "from brainscore import score_model\n",
    "\n",
    "import clip\n",
    "\n",
    "model_definitions = {\n",
    "    'RN50': {'layers': ['layer2.3.bn3', 'layer3.5.bn3', 'layer4.2.bn3', 'attnpool']},\n",
    "    'RN101': {'layers': ['layer2.3.bn3', 'layer3.22.bn3', 'layer4.2.bn3', 'attnpool']},\n",
    "    'RN50x4': {'layers': ['layer2.5.bn3', 'layer3.9.bn3', 'layer4.5.bn3', 'attnpool']},\n",
    "    'RN50x16': {'layers': ['layer2.7.bn3', 'layer3.17.bn3', 'layer4.7.bn3', 'attnpool']},\n",
    "    'ViT-B/32': {'layers': [*[f'transformer.resblocks.{i}' for i in range(12)], 'ln_post']},\n",
    "    'ViT-B/16': {'layers': [*[f'transformer.resblocks.{i}' for i in range(12)], 'ln_post']},\n",
    "}\n",
    "\n",
    "available_models = clip.available_models()\n",
    "for model_name in available_models:\n",
    "    if model_name not in model_definitions:\n",
    "        print(f\"Available clip model available which is not defined: {model_name}\")\n",
    "\n",
    "\n",
    "for model_name, model_definition in model_definitions.items():\n",
    "    if model_name not in available_models:\n",
    "        print(f\"Clip model {model_name} was defined but is not available, skipping it\")\n",
    "        continue\n",
    "\n",
    "    clip_model, preprocess = clip.load(model_name, device='cpu')\n",
    "\n",
    "    file_path = Path('.') / 'zeroshot-weights' / f'{model_name.replace(\"/\", \"=\")}-zeroshot-weights.pt'\n",
    "    if not file_path.exists():\n",
    "        print(f\"Zeroshot weights not found for {model_name}, generating them.\")\n",
    "        make_clip_zeroshot_weights(model_name)\n",
    "    zeroshot_weights = torch.load(file_path)\n",
    "\n",
    "    class ZeroShotWeights(nn.Module):\n",
    "        def __init__(self, zeroshot_weights: torch.Tensor):\n",
    "            super().__init__()\n",
    "            self.zeroshot_weights = zeroshot_weights\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x / x.norm(dim=-1, keepdim=True)\n",
    "            logits = 100. * x @ self.zeroshot_weights.to(x.dtype).to(x.device)\n",
    "            return logits\n",
    "\n",
    "    clip_model = nn.Sequential(OrderedDict([\n",
    "        ('wrapped', clip_model.visual),\n",
    "        ('logits', ZeroShotWeights(zeroshot_weights))\n",
    "    ]))\n",
    "\n",
    "    resize = preprocess.transforms[0]\n",
    "    normalize = preprocess.transforms[4]\n",
    "    preprocessing = functools.partial(load_preprocess_images,\n",
    "                                      image_size=resize.size,\n",
    "                                      normalize_mean=normalize.mean,\n",
    "                                      normalize_std=normalize.std)\n",
    "\n",
    "    model_definition['layers'] = [f'wrapped.{layer}' for layer in model_definition['layers']]\n",
    "    model_definition['activations'] = PytorchWrapper(identifier=model_name, model=clip_model,\n",
    "                                                     preprocessing=preprocessing, batch_size=8)\n",
    "    model_definition['model'] = ModelCommitment(identifier=model_name,\n",
    "                                                activations_model=model_definition['activations'],\n",
    "                                                layers=model_definition['layers'],)\n",
    "                                                #behavioral_readout_layer=model_definition['layers'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baabb2d-2f56-4b36-87e5-8c3acbfd58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from entrypoints\n",
      "Loading lookup from C:\\Users\\Cefir\\AppData\\Roaming\\Python\\Python38\\site-packages\\brainscore\\lookup.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'movshon.FreemanZiemba2013public.V1-pls': <brainscore.benchmarks._neural_common.NeuralBenchmark at 0x1a7fdbdea00>,\n",
       " 'movshon.FreemanZiemba2013public.V2-pls': <brainscore.benchmarks._neural_common.NeuralBenchmark at 0x1a7fdbdea90>,\n",
       " 'dicarlo.MajajHong2015public.V4-pls': <brainscore.benchmarks._neural_common.NeuralBenchmark at 0x1a7fdbded00>,\n",
       " 'dicarlo.MajajHong2015public.IT-pls': <brainscore.benchmarks._neural_common.NeuralBenchmark at 0x1a7fdbe7250>,\n",
       " 'dicarlo.Rajalingham2018public-i2n': <brainscore.benchmarks.public_benchmarks.RajalinghamMatchtosamplePublicBenchmark at 0x1a7fdbe7460>,\n",
       " 'fei-fei.Deng2009-top1': <brainscore.utils.LazyLoad at 0x1a7fdb65d30>,\n",
       " 'dietterich.Hendrycks2019-noise-top1': <brainscore.benchmarks.imagenet_c.Imagenet_C_Category at 0x1a7fdb65fd0>,\n",
       " 'dietterich.Hendrycks2019-blur-top1': <brainscore.benchmarks.imagenet_c.Imagenet_C_Category at 0x1a7fdb77190>,\n",
       " 'dietterich.Hendrycks2019-weather-top1': <brainscore.benchmarks.imagenet_c.Imagenet_C_Category at 0x1a7fdbb1df0>,\n",
       " 'dietterich.Hendrycks2019-digital-top1': <brainscore.benchmarks.imagenet_c.Imagenet_C_Category at 0x1a7fdbbc490>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brainscore.benchmarks import public_benchmark_pool\n",
    "\n",
    "public_benchmark_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f31068-edef-442e-85c4-0515e011e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross-validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [1:18:51<00:00, 473.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from brainscore.benchmarks import public_benchmark_pool\n",
    "\n",
    "benchmark = public_benchmark_pool['dicarlo.MajajHong2015public.IT-pls']\n",
    "\n",
    "score = benchmark(model_definitions['RN50']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107d035c-78c1-4f38-a50d-d53959fe5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test.pkl', 'wb') as f:\n",
    "    pickle.dump(score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43af81f7-6376-4573-8a4e-8af49cb47e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score (aggregation: 2)>\n",
      "array([0.53534334, 0.00350558])\n",
      "Coordinates:\n",
      "  * aggregation  (aggregation) <U6 'center' 'error'\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (aggregation: 2)>\\narray([0.59689724, 0.00350558]...\n",
      "    ceiling:  <xarray.Score (aggregation: 2)>\\narray([0.81579938, 0.00144955]...\n"
     ]
    }
   ],
   "source": [
    "with open('test.pkl', 'rb') as f:\n",
    "    score_loaded = pickle.load(f)\n",
    "print(score_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neurophysiological-Data-Decoding",
   "language": "python",
   "name": "neurophysiological-data-decoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
