{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc4b29-e1d9-49fb-8536-03d8a9cd6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchio as tio\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afeb4a-2dd3-4441-a25a-f61cc1b5a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kamitani2019 dataset (new preprocessing)\n",
    "\n",
    "from research.data.kamitani_2019 import Kamitani2019H5\n",
    "\n",
    "#features_model = 'biggan-128'\n",
    "#features_model = 'bigbigan-resnet50'\n",
    "#features_model = 'RN50'\n",
    "features_model = 'ViT-B=32'\n",
    "#features_model = 'vqgan'\n",
    "\n",
    "feature_key = 'embedding'\n",
    "#feature_key = 'z_mean'\n",
    "#feature_key = 'vqgan-f16-1024-pre_quant'\n",
    "#feature_key = 'vqgan-f16-1024-indices'\n",
    "\n",
    "feature_selection_key = feature_key\n",
    "#feature_selection_key = 'vqgan-f16-1024-pre_quant'\n",
    "\n",
    "root = \"X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\"\n",
    "ssd_root = \"C:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\"\n",
    "h5_path = Path(ssd_root) / \"kamitani2019-cached.hdf5\"\n",
    "features_path = Path(root) / \"features\" / f\"{features_model}-features.hdf5\"\n",
    "feature_selection_path = Path(root) / 'feature-selection-maps.hdf5'\n",
    "subject = 'sub-03'\n",
    "\n",
    "num_features = 2500\n",
    "dataset_params = dict(\n",
    "    h5_path=h5_path,\n",
    "    subjects=[subject,],\n",
    "    features_path=features_path,\n",
    "    feature_keys=[feature_key],\n",
    "    feature_selection_path=feature_selection_path,\n",
    "    feature_selection_key=f'natural_training/average-4/{features_model}/{feature_selection_key}/mean-top-5',\n",
    "    feature_selection_top_k=num_features,\n",
    ")\n",
    "\n",
    "dataset_training = Kamitani2019H5(\n",
    "    **dataset_params,\n",
    "    cached_preprocessing_name='average-4',\n",
    "    func_sessions=['natural_training'],\n",
    "    average_stimulus_repetitions=False,\n",
    ")\n",
    "\n",
    "dataset_test = Kamitani2019H5(\n",
    "    **dataset_params,\n",
    "    cached_preprocessing_name='average-4',\n",
    "    func_sessions=['natural_test'],\n",
    "    average_stimulus_repetitions=True,\n",
    ")\n",
    "\n",
    "dataset_imagery = Kamitani2019H5(\n",
    "    **dataset_params,\n",
    "    cached_preprocessing_name='average-8',\n",
    "    func_sessions=['imagery'],\n",
    "    average_stimulus_repetitions=True,\n",
    ")\n",
    "\n",
    "len(dataset_training), len(dataset_test), len(dataset_imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5099c-1de5-4cb1-ab14-100caac2f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([s[0]['stimulus_id'] for s in dataset_imagery.stimulus_grouped_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc9932-20b8-4dc6-af9f-b931eeb07db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = dataset_training.get_data()\n",
    "data_test = dataset_test.get_data()\n",
    "data_imagery = dataset_imagery.get_data()\n",
    "X_train = data_training['data'].float()\n",
    "Y_train = data_training['features'][feature_key]\n",
    "X_test = data_test['data'].float()\n",
    "Y_test = data_test['features'][feature_key]\n",
    "X_imagery = data_imagery['data'].float()\n",
    "Y_imagery = data_imagery['features'][feature_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e870a-2b7c-43ca-b576-187d7bbda337",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape indices\n",
    "Y_train, Y_test, Y_imagery = [Y.reshape(-1, 14, 14) for Y in (Y_train, Y_test, Y_imagery)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8463de-3a7c-445b-99ec-9882252156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(item.shape, item.dtype) for item in (X_train, Y_train, X_test, Y_test, X_imagery, Y_imagery)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831023e-1019-4f7c-b48d-e713a6e2bed6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "embedding_weight = np.load('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-model1024-embedding-weight.npy')\n",
    "embedding_weight = torch.from_numpy(embedding_weight).cuda()\n",
    "#Y = Y_test[:2].cuda()\n",
    "\n",
    "#print(Y.shape, embedding_weight.shape)\n",
    "\n",
    "#sqr_dist = ((Y[:, None] - embedding_weight[None, :, :, None, None]) ** 2).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09efa4-8513-4510-bbc0-0b52d194efba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sqr_dist2 = (Y ** 2).sum(dim=1)[:, None] + \\\n",
    "            (embedding_weight ** 2).sum(dim=1)[None, :, None, None] - \\\n",
    "            2. * torch.einsum('bchw, ec -> behw', Y, embedding_weight)\n",
    "(sqr_dist - sqr_dist2).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f7f67-eebb-4833-8d8e-753ad0cb408b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing VQGAN embedding\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "embedding_weight = np.load('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-model1024-embedding-weight.npy')\n",
    "embedding_weight = torch.from_numpy(embedding_weight).cuda()\n",
    "\n",
    "def preprocess_embedding(Y):\n",
    "    N, C, H, W = Y.shape\n",
    "    E = embedding_weight.shape[0]\n",
    "    \n",
    "    Y = Y.cuda()\n",
    "    Y_out = torch.zeros(N, E, H, W)\n",
    "\n",
    "    for i in tqdm(range(Y.shape[0])):\n",
    "        y = Y[i]\n",
    "        y = embedding_weight[:, :, None, None] - y[None]\n",
    "        y = y * y\n",
    "        y = y.sum(dim=1)\n",
    "        y = F.softmin(0.5 * y, dim=0)\n",
    "        Y_out[i] = y.cpu()\n",
    "            \n",
    "    return Y_out\n",
    "\n",
    "Y_train = preprocess_embedding(Y_train)\n",
    "Y_test = preprocess_embedding(Y_test)\n",
    "Y_imagery = preprocess_embedding(Y_imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63246077-a93a-410c-92ce-c143fbd2a824",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "Y = Y.sort(dim=1).values\n",
    "Y = Y[:, -5:]\n",
    "\n",
    "Y[:, -1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2f2c2-a9da-44a4-8f7b-aea1c3dda120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_shape = Y_train.shape[1:]\n",
    "C, H, W = feature_shape\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], C * H * W)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], C * H * W)\n",
    "Y_imagery = Y_imagery.reshape(Y_imagery.shape[0], C * H * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695cb90-9092-4eec-bf02-63f3e9513b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def pearsonr(Y, Y_pred, axis=0):\n",
    "    Y = Y.astype(np.double)\n",
    "    Y_pred = Y_pred.astype(np.double)\n",
    "    \n",
    "    Y = Y - Y.mean(axis=axis, keepdims=True)\n",
    "    Y_pred = Y_pred - Y_pred.mean(axis=axis, keepdims=True)\n",
    "    \n",
    "    Y = Y / scipy.linalg.norm(Y, axis=axis,  keepdims=True)\n",
    "    Y_pred = Y_pred / scipy.linalg.norm(Y_pred, axis=axis,  keepdims=True)\n",
    "    \n",
    "    return (Y * Y_pred).sum(axis=axis).mean()\n",
    "\n",
    "def cosine_similarity(Y, Y_pred, axis=0):\n",
    "    Y = Y.astype(np.double)\n",
    "    Y_pred = Y_pred.astype(np.double)\n",
    "    \n",
    "    Y = Y / scipy.linalg.norm(Y, axis=axis, keepdims=True)\n",
    "    Y_pred = Y_pred / scipy.linalg.norm(Y_pred, axis=axis,  keepdims=True)\n",
    "    \n",
    "    return (Y * Y_pred).sum(axis=axis).mean()\n",
    "\n",
    "def pearsonr_scipy(Y, Y_pred):\n",
    "    r = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        r.append(scipy.stats.pearsonr(Y[:, i], Y_pred[:, i])[0])\n",
    "    return np.mean(r)\n",
    "    \n",
    "def r2_score(Y, Y_pred):\n",
    "    ssr = ((Y - Y_pred) ** 2).sum(axis=0)\n",
    "    sst = ((Y - Y.mean(axis=0, keepdims=True)) ** 2).sum(axis=0)\n",
    "    return (1 - ssr / sst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71cb4b-1294-4438-942b-2a21fb424e57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit ridge regression with the FastL2LiR from the Kamitani group.\n",
    "from sklearn.metrics import r2_score\n",
    "from fastl2lir import FastL2LiR\n",
    "\n",
    "model = FastL2LiR()\n",
    "model.fit(X_train, Y_train, alpha=100.0, n_feat=500)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('r-batch', pearsonr(Y_train, Y_train_pred), pearsonr(Y_test, Y_test_pred))\n",
    "print('r-row', pearsonr(Y_train, Y_train_pred, axis=1), pearsonr(Y_test, Y_test_pred, axis=1))\n",
    "print('cs-batch', cosine_similarity(Y_train, Y_train_pred), cosine_similarity(Y_test, Y_test_pred))\n",
    "print('cs-row', cosine_similarity(Y_train, Y_train_pred, axis=1), cosine_similarity(Y_test, Y_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6f616-f932-4db8-b32c-7270bb39fde8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load vqgan embedding\n",
    "\n",
    "embedding_root = 'X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-embeddings\\\\'\n",
    "embedding_name = 'isomap'\n",
    "ndim = 8\n",
    "\n",
    "embedding = torch.from_numpy(np.load(Path(embedding_root) / f'ndim-{ndim}' / f'{embedding_name}-embedding.npy')).float()\n",
    "E = embedding.shape[1]\n",
    "print(embedding.mean(), embedding.std())\n",
    "embedding = (embedding - embedding.mean()) / embedding.std()\n",
    "print(embedding.mean(), embedding.std())\n",
    "print(embedding.shape)\n",
    "\n",
    "Y_train, Y_train_indices = embedding[Y_train.long()], Y_train\n",
    "Y_test, Y_test_indices = embedding[Y_test.long()], Y_test\n",
    "Y_imagery, Y_imagery_indices = embedding[Y_imagery.long()], Y_imagery\n",
    "\n",
    "Y_train, Y_test, Y_imagery = (Y.reshape(-1, E, 14, 14) for Y in (Y_train, Y_test, Y_imagery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b0b52-b384-450b-9abd-cc43a3d95b5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load vggan pca\n",
    "\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from research.metrics.metrics import embedding_distance\n",
    "\n",
    "vqgan_pca_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-pca\\\\')\n",
    "n_components = 4\n",
    "\n",
    "embedding_weight = torch.from_numpy(np.load('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-model1024-embedding-weight.npy'))\n",
    "\n",
    "pca = load(vqgan_pca_path / f'pca-{n_components}dim-sklearn.pkl')\n",
    "def pca_transform(Y):\n",
    "    N, C, H, W = Y.shape\n",
    "    Y = rearrange(Y, 'n c h w -> (n h w) c')\n",
    "    Y = torch.from_numpy(pca.transform(Y)).float()\n",
    "    Y = rearrange(Y, '(n h w) c -> n c h w', h=H, w=W)\n",
    "    return Y\n",
    "\n",
    "def pca_inverse_transform(Y):\n",
    "    N, C, H, W = Y.shape\n",
    "    Y = rearrange(Y, 'n c h w -> (n h w) c')\n",
    "    Y = torch.from_numpy(pca.inverse_transform(Y)).float()\n",
    "    Y = rearrange(Y, '(n h w) c -> n c h w', h=H, w=W)\n",
    "    return Y\n",
    "\n",
    "#Y_test_pca = pca_transform(Y_test)\n",
    "#Y_test_pca_inverse = pca_inverse_transform(Y_test_pca)\n",
    "\n",
    "#Y_test_embedding_distance = embedding_distance(Y_test, embedding_weight)\n",
    "#Y_test_pca_inverse_emebdding_distance = embedding_distance(Y_test_pca_inverse.float(), embedding_weight)\n",
    "\n",
    "Y_train = pca_transform(Y_train).float()\n",
    "Y_test = pca_transform(Y_test).float()\n",
    "Y_imagery = pca_transform(Y_imagery).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0976d9-0a6c-42cf-881c-f39d7befc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset_training = TensorDataset(X_train, Y_train)\n",
    "dataset_test = TensorDataset(X_test, Y_test)\n",
    "dataset_imagery = TensorDataset(X_imagery, Y_imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bd458-5e5a-463d-afc0-2d70a252194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from research.models.components_2d import BlurConvTranspose2d\n",
    "from research.models.fmri_decoders import VariationalDecoder, SpatialDecoder, SpatialDiscriminator\n",
    "from research.metrics.loss_functions import (\n",
    "    EuclideanLoss, \n",
    "    EmbeddingClassifierLoss,\n",
    "    ProbabalisticCrossEntropyLoss,\n",
    "    VariationalLoss,\n",
    "    CosineSimilarityLoss,\n",
    "    EmbeddingDistributionLoss,\n",
    "    ContrastiveCosineSimilarityLoss,\n",
    ")\n",
    "from research.metrics.metrics import cosine_similarity, pearsonr, embedding_distance\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "#out_size = 1024\n",
    "out_size = Y_train.shape[1]\n",
    "\n",
    "class SpatialDecoder2(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = nn.ConvTranspose2d(in_features, out_features, kernel_size=14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None, None]\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "#decoder_class=SpatialDecoder,\n",
    "#decoder_params={\n",
    "#   'in_features': num_features, \n",
    "#   'out_features': out_size * 2, \n",
    "#   'hidden_size': hidden_size, \n",
    "#   'kernel_size': 7,\n",
    "#}\n",
    "\n",
    "#decoder_class = SpatialDecoder2\n",
    "#decoder_params = {\n",
    "#   'in_features': num_features, \n",
    "#   'out_features': out_size * 2, \n",
    "#}\n",
    "\n",
    "#model = VariationalDecoder(\n",
    "#    num_features, out_size, hidden_size, \n",
    "#    decoder_class=decoder_class,\n",
    "#    decoder_params=decoder_params,\n",
    "#)\n",
    "\n",
    "#model = SpatialDecoder(num_features, out_size, hidden_size, kernel_size=7)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=num_features, out_features=hidden_size),\n",
    "    nn.LeakyReLU(),\n",
    "    torch.nn.Dropout(p=0.9, inplace=False),\n",
    "    nn.Linear(hidden_size, out_size),\n",
    ")\n",
    "\n",
    "criterion = ContrastiveCosineSimilarityLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#riterion = nn.L1Loss()\n",
    "#criterion = VariationalLoss(distance_loss=criterion, kld_weight=1e-2,)\n",
    "#criterion = ProbabalisticCrossEntropyLoss(smoothness=0.0,)\n",
    "#criterion = EuclideanLoss(dim=1)\n",
    "\n",
    "#embedding_weight = np.load('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\vqgan-model1024-embedding-weight.npy')\n",
    "#embedding_weight = torch.from_numpy(embedding_weight).cuda()\n",
    "#criterion = EmbeddingDistributionLoss(embedding_weight, distance_criterion=criterion, kl_weight=.1)\n",
    "#criterion = EmbeddingClassifierLoss(embedding_weight, kl_weight=1.)\n",
    "\n",
    "optimizer = Adam(\n",
    "    params=[*model.parameters(), *criterion.parameters()],\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9921b-8657-4d2e-a3e8-974d8cbcc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(X_train[:3].cuda())\n",
    "    print([e.shape for e in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50a681-aa77-431e-866d-ba4833e690be",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = torch.unique(Y_train, dim=0, return_inverse=True)\n",
    "_, indices = torch.unique(indices[:60], return_inverse=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48328571-f487-4b07-978e-36c69ee95a80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "discriminator = SpatialDiscriminator(in_features=out_size, hidden_size=512, kernel_size=7)\n",
    "discriminator_optimizer = Adam(params=discriminator.parameters(), lr=1e-5,)\n",
    "discriminator.to(device)\n",
    "\n",
    "real = 1.\n",
    "fake = -1.\n",
    "\n",
    "real_target = torch.tensor([real, fake]).cuda()[None]\n",
    "fake_target = torch.tensor([fake, real]).cuda()[None]\n",
    "\n",
    "class SoftplusLoss(nn.Module):\n",
    "    def forward(self, prediction, target):\n",
    "        return F.softplus(target * prediction).mean()\n",
    "    \n",
    "discriminator_criterion = SoftplusLoss()\n",
    "\n",
    "discriminator_weight = 0.01\n",
    "\n",
    "classifier = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532f759-5c0b-4df3-9ea6-b331f575dfa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 2500).cuda()\n",
    "with torch.no_grad():\n",
    "    x = model(x)\n",
    "    print(x.shape)\n",
    "    x = discriminator(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d468dc-1761-4ad5-94e0-d6c1e3c5e16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard training\n",
    "\n",
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=1200)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None]).cpu() for x, _ in dataset])\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "max_iterations = 251\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred = model(x)\n",
    "    #y = y.reshape(y_pred.shape)\n",
    "    loss = criterion(y, y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print(round(loss.detach().cpu().item(), 3))\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_train_pred = run_all(dataset_training)\n",
    "            Y_test_pred = run_all(dataset_test)\n",
    "            Y_train = dataset_training.tensors[1]\n",
    "            Y_test = dataset_test.tensors[1]\n",
    "        \n",
    "        print('r-batch', \n",
    "              pearsonr(Y_train, Y_train_pred), \n",
    "              pearsonr(Y_test, Y_test_pred))\n",
    "        print('cs-row', \n",
    "              cosine_similarity(Y_train, Y_train_pred, dim=1), \n",
    "              cosine_similarity(Y_test, Y_test_pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5df21-42d4-4992-b5f0-e81f6c695e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similiarity2(Y, Y_pred):\n",
    "    Y = Y.to(torch.float64)\n",
    "    Y_pred = Y_pred.to(torch.float64)\n",
    "\n",
    "    Y = Y / torch.norm(Y, dim=-1, keepdim=True)\n",
    "    Y_pred = Y_pred / torch.norm(Y_pred, dim=-1, keepdim=True)\n",
    "    \n",
    "    return Y @ Y_pred.T\n",
    "\n",
    "cs = cosine_similiarity2(Y_test, Y_test_pred)\n",
    "print(((cs.argsort(dim=0).diag() / cs.shape[0]).mean() + (cs.argsort(dim=1).diag() / cs.shape[0]).mean()) * 0.5)\n",
    "plt.imshow(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a4db5-81c1-485a-ae9c-d785ece97c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.random.randn(8, 8)\n",
    "plt.axis('off')\n",
    "plt.imshow(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001bc17-ab49-4658-bfae-9ac509cf6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8bca9-abcf-4ac7-854b-5da03bd18b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0eec5-a351-49c2-abdd-06d65ac81acb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier training\n",
    "\n",
    "Y_train = dataset_training.tensors[1]\n",
    "Y_test = dataset_test.tensors[1]\n",
    "\n",
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=24)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None]).cpu() for x, _ in dataset])\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "max_iterations = 251\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if isinstance(loss, tuple):\n",
    "        loss, loss_dict = loss\n",
    "    else:\n",
    "        loss_dict = loss.detach().cpu().item()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('train loss', loss_dict)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            #Y_train_pred = run_all(dataset_training)\n",
    "            Y_test_pred = run_all(dataset_test)\n",
    "            \n",
    "            print(f'test loss {criterion(Y_test_pred, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac874a-e828-4393-8466-e287176d4638",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adversarial training\n",
    "\n",
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=32)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None]).cpu() for x, _ in dataset])\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "def fmt_loss(loss):\n",
    "    return round(loss.detach().cpu().item(), 3)\n",
    "\n",
    "max_iterations = 251\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    if classifier:\n",
    "        y = embedding_distance(y, embedding_weight.cuda())\n",
    "        y = y ** 0.5\n",
    "        y = F.softmax(-y, dim=1)\n",
    "    \n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    discriminator.train()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    d_real_pred = discriminator(y)\n",
    "    d_fake_pred = discriminator(model(x))\n",
    "    d_real_loss = discriminator_criterion(d_real_pred, real_target)\n",
    "    d_fake_loss = discriminator_criterion(d_fake_pred, fake_target)\n",
    "    d_loss = (d_real_loss + d_fake_loss) * 0.5\n",
    "    \n",
    "    discriminator_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    discriminator.eval()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y, y_pred)\n",
    "    \n",
    "    g_fake_pred = discriminator(y_pred)\n",
    "    g_fake_loss = discriminator_criterion(g_fake_pred, real_target)\n",
    "    if i % 10 == 0:\n",
    "        print(f'iter={i}, structural_loss={fmt_loss(loss)}, g_fake_loss={fmt_loss(g_fake_loss)}, d_real_loss={fmt_loss(d_real_loss)}, d_fake_loss={fmt_loss(d_fake_loss)}')\n",
    "    loss = loss + g_fake_loss * discriminator_weight\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_train_pred = run_all(dataset_training)\n",
    "            Y_test_pred = run_all(dataset_test)\n",
    "            Y_train = dataset_training.tensors[1]\n",
    "            Y_test = dataset_test.tensors[1]\n",
    "        \n",
    "        print('r-batch', \n",
    "              pearsonr_torch(Y_train, Y_train_pred), \n",
    "              pearsonr_torch(Y_test, Y_test_pred))\n",
    "        print('cs-row', \n",
    "              cosine_similarity_torch(Y_train, Y_train_pred, dim=1), \n",
    "              cosine_similarity_torch(Y_test, Y_test_pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3fc84-1461-4f80-a1ae-076a0d7f174d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variational training\n",
    "\n",
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=128)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None])[0] for x, _ in dataset]).cpu()\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "max_iterations = 1500\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred, mu, log_var = model(x)\n",
    "    loss, loss_dict = criterion(y, y_pred, mu, log_var)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print({k: round(v.detach().cpu().item(), 3) for k, v in loss_dict.items()})\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_train_pred = run_all(dataset_training)\n",
    "            Y_test_pred = run_all(dataset_test)\n",
    "        \n",
    "        print('r-batch', \n",
    "              pearsonr(dataset_training.tensors[1], Y_train_pred), \n",
    "              pearsonr(dataset_test.tensors[1], Y_test_pred))\n",
    "        print('cs-row', \n",
    "              cosine_similarity(dataset_training.tensors[1], Y_train_pred, dim=1), \n",
    "              cosine_similarity(dataset_test.tensors[1], Y_test_pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2e1f4-0922-4e93-9403-11315919288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\')\n",
    "\n",
    "features_model\n",
    "\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test-prediction__v2.npy', Y_scaler.inverse_transform(Y_test_pred).reshape(50, 128))#\n",
    "np.save(out_path / f'{features_model}__{Y_key}__{subject}__test__v2.npy', Y_scaler.inverse_transform(Y_test).reshape(50, 128))#*Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbde133-8e92-4cb5-a566-4c68fb4e81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110a942-a624-4891-82c0-e58006f63123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\decoded_features')\n",
    "\n",
    "np.save(out_path / f'{features_model}__{feature_key}__{subject}__test-prediction__v4.npy', Y_test_pred)#\n",
    "np.save(out_path / f'{features_model}__{feature_key}__{subject}__test__v4.npy', Y_test)#*Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2e9e5-8d1f-490e-9a01-b7d4f3c4dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da24b6-7cae-40a7-a23c-8a7e753fd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\decoded_features')\n",
    "\n",
    "def run_all_variational(dataset):\n",
    "    mean = []\n",
    "    std = []\n",
    "    for x, _ in dataset:\n",
    "        _, mu, log_var = model(x.to(device)[None])\n",
    "        mean.append(mu)\n",
    "        std.append(torch.exp(0.5 * log_var))\n",
    "    mean = torch.cat(mean)\n",
    "    std = torch.cat(std)\n",
    "    out = torch.stack([mean, std], dim=1)\n",
    "    return out.detach().cpu().numpy()\n",
    "        \n",
    "Y_test_pred = run_all_variational(dataset_test)\n",
    "Y_imagery_pred = run_all_variational(dataset_imagery)\n",
    "\n",
    "version = '4-0'\n",
    "code_name = 'embedding'\n",
    "out_dir = out_path / features_model / feature_key / subject\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.save(out_dir / f'natural_test__{code_name}__v{version}.npy', Y_test_pred)#.reshape(Y_test_pred.shape[0], *feature_shape))\n",
    "np.save(out_dir / f'imagery__{code_name}__v{version}.npy', Y_imagery_pred)#.reshape(Y_imagery_pred.shape[0], *feature_shape))\n",
    "\n",
    "if code_name == 'embedding':\n",
    "    np.save(out_dir / f'{code_name}__v{version}.npy', embedding.cpu().numpy())#.reshape(Y_imagery_pred.shape[0], *feature_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7910f-4926-4b98-8716-5f43110cea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8229da-f9d5-41be-b7bd-f2738104634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\decoded_features')\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_test_pred = run_all(dataset_test).cpu().numpy()\n",
    "    Y_imagery_pred = run_all(dataset_imagery).cpu().numpy()\n",
    "\n",
    "version = '1-0'\n",
    "code_name = 'contrastive'\n",
    "out_dir = out_path / features_model / feature_key / subject\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.save(out_dir / f'natural_test__{code_name}__v{version}.npy', Y_test_pred)#.reshape(Y_test_pred.shape[0], *feature_shape))\n",
    "np.save(out_dir / f'imagery__{code_name}__v{version}.npy', Y_imagery_pred)#.reshape(Y_imagery_pred.shape[0], *feature_shape))\n",
    "\n",
    "if code_name == 'embedding':\n",
    "    np.save(out_dir / f'{code_name}__v{version}.npy', embedding.cpu().numpy())#.reshape(Y_imagery_pred.shape[0], *feature_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786a545-a541-48af-97b0-34fc8bad7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\decoded_features')\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_imagery_pred = run_all(dataset_imagery).cpu().numpy()\n",
    "\n",
    "\n",
    "version = '1-0'\n",
    "code_name = 'pca'\n",
    "out_dir = out_path / features_model / feature_key / subject\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.save(out_dir / f'natural_test__{code_name}__v{version}.npy', pca_inverse_transform(Y_test_pred))\n",
    "np.save(out_dir / f'imagery__{code_name}__v{version}.npy', pca_inverse_transform(Y_imagery_pred))#.reshape(Y_imagery_pred.shape[0], *feature_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d5e35-90e5-4bee-a04f-678cd2aeca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pipelines = []\n",
    "for i in tqdm(range(Y_train.shape[1])):\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(f_classif, k=500)),\n",
    "        ('model', LogisticRegression(solver='liblinear'))\n",
    "    ])\n",
    "    pipeline.fit(X_train, Y_train[:, i])\n",
    "    pipelines.append(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9f7a9-3728-49cd-90d0-40756f31a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "select_k_best = SelectKBest(f_classif, k=5)\n",
    "X_train_selection = select_k_best.fit_transform(X_train, Y_train[:, 0])\n",
    "\n",
    "model = LogisticRegression(solver='saga', C=1000.0)\n",
    "model.fit(X_train_selection, Y_train[:, 0])\n",
    "\n",
    "X_test_selection = select_k_best.transform(X_test,)\n",
    "print(model.score(X_train_selection, Y_train[:, 0]), model.score(X_test_selection, Y_test[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513357f-c059-42da-85a0-110f3aa4ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_pickle('G:\\\\Github Repositories\\\\GenericObjectDecoding\\\\code\\\\python\\\\results\\\\GenericObjectDecoding.pkl')\n",
    "#print(df)\n",
    "df = df[df['feature'] == 'cnn8']\n",
    "\n",
    "true_feature_averaged_percept = df['true_feature_averaged_percept'].array[0]\n",
    "predicted_feature_averaged_percept = df['predicted_feature_averaged_percept'].array[0]\n",
    "print(predicted_feature_averaged_percept.shape)\n",
    "\n",
    "print(r2_score(true_feature_averaged_percept, predicted_feature_averaged_percept))\n",
    "print(pearsonr(true_feature_averaged_percept, predicted_feature_averaged_percept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82627f4d-d528-4439-a8cc-0402780d3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_stuff = Y_train.reshape(4800, *Y_shape)\n",
    "Y_stuff.std(axis=(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neurophysiological-Data-Decoding",
   "language": "python",
   "name": "neurophysiological-data-decoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
