{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966b46e7-8338-4383-99c5-7bd69ca0ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchio as tio\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "from scipy import ndimage\n",
    "from fracridge import FracRidgeRegressorCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "dir2 = os.path.abspath('../..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e372a6b0-e692-4547-a423-376c07acba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('D:\\\\Datasets\\\\NSD\\\\')\n",
    "derivatives_path = dataset_path / 'derivatives'\n",
    "betas_path = dataset_path / 'nsddata_betas' / 'ppdata'\n",
    "ppdata_path = dataset_path / 'nsddata' / 'ppdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a439c-cd84-4058-bb38-afe34754bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = {f'subj0{i}': {} for i in range(1, 9)}\n",
    "\n",
    "for subject_name, subject_data in subjects.items():\n",
    "    responses_file_path = ppdata_path / subject_name / 'behav' / 'responses.tsv'\n",
    "    subject_data['responses'] = pd.read_csv(responses_file_path, sep='\\t',)\n",
    "    \n",
    "    # The last 3 sessions are currently held-out for the algonauts challenge\n",
    "    # remove them for now.\n",
    "    session_ids = subject_data['responses']['SESSION']\n",
    "    held_out_mask = session_ids > (np.max(session_ids) - 3)\n",
    "    subject_data['responses'] = subject_data['responses'][~held_out_mask]\n",
    "    \n",
    "    subject_betas_path = derivatives_path / 'betas' / subject_name / 'func1pt8mm' / 'betas_fithrf_GLMdenoise_RR'\n",
    "    num_sessions = np.max(subject_data['responses']['SESSION'])\n",
    "    \n",
    "    subject_data['betas'] = h5py.File(subject_betas_path / f'betas_sessions.hdf5', 'r')['betas']\n",
    "    \n",
    "    #subject_data['brainmask'] = nib.load(ppdata_path / subject_name / 'func1pt8mm' / 'brainmask.nii.gz')\n",
    "    #subject_data['t1_path'] = ppdata_path / subject_name / 'func1pt8mm' / 'T1_to_func1pt8mm.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c895f6be-f036-4db9-bcdd-feecb87e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_model = 'ViT-B=32'\n",
    "embedding_model = 'bigbigan-resnet50'\n",
    "\n",
    "#embedding_key = 'embedding'\n",
    "embedding_key = 'z_mean'\n",
    "\n",
    "subject_name = 'subj01'\n",
    "\n",
    "num_voxels = 2500\n",
    "\n",
    "split_name = 'split-01'\n",
    "split = h5py.File(derivatives_path / 'data_splits' / f'{split_name}.hdf5')\n",
    "subject_split = split[subject_name]\n",
    "test_mask = subject_split['test_response_mask'][:].astype(bool)\n",
    "validation_mask = subject_split['validation_response_mask'][:].astype(bool)\n",
    "validation_indices = np.where(validation_mask)[0]\n",
    "training_mask = ~(test_mask | validation_mask)\n",
    "training_indices = np.where(training_mask)[0]\n",
    "responses = subjects[subject_name]['responses']\n",
    "training_stimulus_ids = responses['73KID'].to_numpy()[training_indices] - 1\n",
    "validation_stimulus_ids = responses['73KID'].to_numpy()[validation_indices] - 1\n",
    "\n",
    "model_embeddings = h5py.File(derivatives_path / 'stimulus_embeddings' / f'{embedding_model}-embeddings.hdf5', 'r')\n",
    "stimulus_embeddings = model_embeddings[embedding_key]\n",
    "\n",
    "encoder_name = 'fracridge'\n",
    "encoder_parameters = h5py.File(derivatives_path / f'{encoder_name}-parameters.hdf5', 'r')\n",
    "sorted_indices_flat = encoder_parameters[subject_name][embedding_model][embedding_key]['sorted_indices_flat']\n",
    "\n",
    "X = np.stack([\n",
    "    subjects[subject_name]['betas'][:, i] \n",
    "    for i in sorted_indices_flat[:num_voxels]\n",
    "], axis=1)\n",
    "X = torch.from_numpy(X).float() / 300\n",
    "\n",
    "X_train = X[training_indices]\n",
    "X_val = X[validation_mask]\n",
    "\n",
    "X_train_mean = X_train.mean(dim=0, keepdims=True)\n",
    "X_train_std = X_train.std(dim=0, keepdims=True)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_val = (X_val - X_train_mean) / X_train_std\n",
    "\n",
    "Y = stimulus_embeddings[:]\n",
    "Y = torch.from_numpy(Y).float()\n",
    "Y_train = Y[training_stimulus_ids]\n",
    "Y_val = Y[validation_stimulus_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1d56aa-7845-48f7-afe8-e460f26e0b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21750, 2500]),\n",
       " torch.Size([21750, 120]),\n",
       " torch.Size([3000, 2500]),\n",
       " torch.Size([3000, 120]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15521a5-e6dc-43c2-be24-0dc83c21a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset_training = TensorDataset(X_train, Y_train)\n",
    "dataset_val = TensorDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81551d1e-a9c4-4adc-9f75-00658c51c1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalDecoder(\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=2500, out_features=5000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.9, inplace=False)\n",
       "    (3): Linear(in_features=5000, out_features=240, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from research.models.components_2d import BlurConvTranspose2d\n",
    "from research.models.fmri_decoders import VariationalDecoder, SpatialDecoder, SpatialDiscriminator\n",
    "from research.metrics.loss_functions import (\n",
    "    EuclideanLoss, \n",
    "    EmbeddingClassifierLoss,\n",
    "    ProbabalisticCrossEntropyLoss,\n",
    "    VariationalLoss,\n",
    "    CosineSimilarityLoss,\n",
    "    EmbeddingDistributionLoss,\n",
    "    ContrastiveCosineSimilarityLoss,\n",
    ")\n",
    "from research.metrics.metrics import cosine_similarity, pearsonr, embedding_distance\n",
    "\n",
    "hidden_size = 5000\n",
    "out_size = Y_train.shape[1]\n",
    "\n",
    "model = VariationalDecoder(\n",
    "    num_voxels, out_size, hidden_size, \n",
    "    #decoder_class=decoder_class,\n",
    "    #decoder_params=decoder_params,\n",
    ")\n",
    "\n",
    "#model = nn.Sequential(\n",
    "#    nn.Linear(in_features=num_features, out_features=hidden_size),\n",
    "#    nn.LeakyReLU(),\n",
    "#    torch.nn.Dropout(p=0.9, inplace=False),\n",
    "#    nn.Linear(hidden_size, out_size),\n",
    "#)\n",
    "\n",
    "#criterion = ContrastiveCosineSimilarityLoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = CosineSimilarityLoss()\n",
    "#riterion = nn.L1Loss()\n",
    "criterion = VariationalLoss(distance_loss=criterion, kld_weight=1e-4,)\n",
    "\n",
    "optimizer = Adam(\n",
    "    params=[*model.parameters(), *criterion.parameters()],\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc972968-dff7-44aa-97d6-c9263a14eb52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.663, 'distance_loss': 0.636, 'kld_loss': 0.027, 'kld': 271.457}\n",
      "2v2-cosine-dist tensor(0.9972) tensor(0.9600)\n",
      "2v2-euclidean-dist tensor(0.9972) tensor(0.9605)\n",
      "{'loss': 0.651, 'distance_loss': 0.624, 'kld_loss': 0.027, 'kld': 271.768}\n",
      "{'loss': 0.638, 'distance_loss': 0.612, 'kld_loss': 0.026, 'kld': 259.16}\n",
      "{'loss': 0.644, 'distance_loss': 0.617, 'kld_loss': 0.027, 'kld': 269.696}\n",
      "{'loss': 0.662, 'distance_loss': 0.636, 'kld_loss': 0.026, 'kld': 264.942}\n",
      "{'loss': 0.657, 'distance_loss': 0.629, 'kld_loss': 0.028, 'kld': 276.638}\n",
      "{'loss': 0.672, 'distance_loss': 0.645, 'kld_loss': 0.027, 'kld': 271.491}\n",
      "{'loss': 0.639, 'distance_loss': 0.612, 'kld_loss': 0.027, 'kld': 270.181}\n",
      "{'loss': 0.655, 'distance_loss': 0.628, 'kld_loss': 0.027, 'kld': 268.653}\n",
      "{'loss': 0.645, 'distance_loss': 0.618, 'kld_loss': 0.027, 'kld': 270.583}\n",
      "{'loss': 0.64, 'distance_loss': 0.614, 'kld_loss': 0.026, 'kld': 263.112}\n",
      "2v2-cosine-dist tensor(0.9977) tensor(0.9609)\n",
      "2v2-euclidean-dist tensor(0.9977) tensor(0.9614)\n",
      "{'loss': 0.665, 'distance_loss': 0.638, 'kld_loss': 0.027, 'kld': 274.909}\n",
      "{'loss': 0.655, 'distance_loss': 0.628, 'kld_loss': 0.027, 'kld': 265.939}\n",
      "{'loss': 0.648, 'distance_loss': 0.62, 'kld_loss': 0.027, 'kld': 271.995}\n",
      "{'loss': 0.647, 'distance_loss': 0.62, 'kld_loss': 0.027, 'kld': 270.841}\n",
      "{'loss': 0.63, 'distance_loss': 0.604, 'kld_loss': 0.027, 'kld': 265.415}\n",
      "{'loss': 0.653, 'distance_loss': 0.626, 'kld_loss': 0.026, 'kld': 263.474}\n",
      "{'loss': 0.666, 'distance_loss': 0.639, 'kld_loss': 0.027, 'kld': 269.585}\n",
      "{'loss': 0.654, 'distance_loss': 0.627, 'kld_loss': 0.027, 'kld': 272.479}\n",
      "{'loss': 0.658, 'distance_loss': 0.631, 'kld_loss': 0.027, 'kld': 271.881}\n",
      "{'loss': 0.641, 'distance_loss': 0.614, 'kld_loss': 0.027, 'kld': 272.335}\n",
      "2v2-cosine-dist tensor(0.9980) tensor(0.9616)\n",
      "2v2-euclidean-dist tensor(0.9980) tensor(0.9621)\n",
      "{'loss': 0.636, 'distance_loss': 0.609, 'kld_loss': 0.027, 'kld': 271.806}\n",
      "{'loss': 0.646, 'distance_loss': 0.619, 'kld_loss': 0.027, 'kld': 272.794}\n",
      "{'loss': 0.655, 'distance_loss': 0.629, 'kld_loss': 0.026, 'kld': 262.086}\n",
      "{'loss': 0.639, 'distance_loss': 0.613, 'kld_loss': 0.026, 'kld': 261.938}\n",
      "{'loss': 0.641, 'distance_loss': 0.614, 'kld_loss': 0.027, 'kld': 270.372}\n",
      "{'loss': 0.648, 'distance_loss': 0.622, 'kld_loss': 0.026, 'kld': 263.56}\n",
      "{'loss': 0.637, 'distance_loss': 0.61, 'kld_loss': 0.027, 'kld': 270.407}\n",
      "{'loss': 0.638, 'distance_loss': 0.611, 'kld_loss': 0.027, 'kld': 272.903}\n",
      "{'loss': 0.644, 'distance_loss': 0.617, 'kld_loss': 0.027, 'kld': 271.037}\n",
      "{'loss': 0.644, 'distance_loss': 0.617, 'kld_loss': 0.027, 'kld': 271.455}\n",
      "2v2-cosine-dist tensor(0.9983) tensor(0.9648)\n",
      "2v2-euclidean-dist tensor(0.9983) tensor(0.9652)\n",
      "{'loss': 0.655, 'distance_loss': 0.628, 'kld_loss': 0.027, 'kld': 269.571}\n",
      "{'loss': 0.646, 'distance_loss': 0.619, 'kld_loss': 0.027, 'kld': 271.297}\n",
      "{'loss': 0.646, 'distance_loss': 0.619, 'kld_loss': 0.027, 'kld': 274.151}\n",
      "{'loss': 0.643, 'distance_loss': 0.616, 'kld_loss': 0.027, 'kld': 271.749}\n",
      "{'loss': 0.641, 'distance_loss': 0.614, 'kld_loss': 0.028, 'kld': 276.328}\n",
      "{'loss': 0.639, 'distance_loss': 0.613, 'kld_loss': 0.027, 'kld': 265.481}\n",
      "{'loss': 0.638, 'distance_loss': 0.611, 'kld_loss': 0.027, 'kld': 270.173}\n",
      "{'loss': 0.64, 'distance_loss': 0.613, 'kld_loss': 0.026, 'kld': 264.639}\n",
      "{'loss': 0.645, 'distance_loss': 0.618, 'kld_loss': 0.027, 'kld': 274.708}\n",
      "{'loss': 0.649, 'distance_loss': 0.622, 'kld_loss': 0.027, 'kld': 272.979}\n",
      "2v2-cosine-dist tensor(0.9985) tensor(0.9649)\n",
      "2v2-euclidean-dist tensor(0.9985) tensor(0.9654)\n",
      "{'loss': 0.622, 'distance_loss': 0.594, 'kld_loss': 0.028, 'kld': 278.966}\n",
      "{'loss': 0.651, 'distance_loss': 0.624, 'kld_loss': 0.027, 'kld': 268.251}\n",
      "{'loss': 0.633, 'distance_loss': 0.606, 'kld_loss': 0.027, 'kld': 266.475}\n",
      "{'loss': 0.642, 'distance_loss': 0.616, 'kld_loss': 0.027, 'kld': 267.383}\n",
      "{'loss': 0.628, 'distance_loss': 0.601, 'kld_loss': 0.027, 'kld': 270.539}\n",
      "{'loss': 0.636, 'distance_loss': 0.609, 'kld_loss': 0.027, 'kld': 270.755}\n",
      "{'loss': 0.657, 'distance_loss': 0.629, 'kld_loss': 0.027, 'kld': 274.07}\n",
      "{'loss': 0.624, 'distance_loss': 0.596, 'kld_loss': 0.028, 'kld': 275.142}\n",
      "{'loss': 0.646, 'distance_loss': 0.619, 'kld_loss': 0.027, 'kld': 272.815}\n",
      "{'loss': 0.643, 'distance_loss': 0.616, 'kld_loss': 0.027, 'kld': 268.78}\n",
      "2v2-cosine-dist tensor(0.9988) tensor(0.9650)\n",
      "2v2-euclidean-dist tensor(0.9988) tensor(0.9654)\n",
      "{'loss': 0.637, 'distance_loss': 0.611, 'kld_loss': 0.027, 'kld': 267.884}\n",
      "{'loss': 0.638, 'distance_loss': 0.611, 'kld_loss': 0.027, 'kld': 269.337}\n",
      "{'loss': 0.641, 'distance_loss': 0.614, 'kld_loss': 0.027, 'kld': 273.582}\n",
      "{'loss': 0.64, 'distance_loss': 0.613, 'kld_loss': 0.027, 'kld': 272.866}\n",
      "{'loss': 0.637, 'distance_loss': 0.609, 'kld_loss': 0.028, 'kld': 280.832}\n",
      "{'loss': 0.645, 'distance_loss': 0.618, 'kld_loss': 0.027, 'kld': 265.726}\n",
      "{'loss': 0.646, 'distance_loss': 0.62, 'kld_loss': 0.027, 'kld': 266.961}\n",
      "{'loss': 0.625, 'distance_loss': 0.598, 'kld_loss': 0.027, 'kld': 268.878}\n",
      "{'loss': 0.659, 'distance_loss': 0.632, 'kld_loss': 0.027, 'kld': 268.362}\n"
     ]
    }
   ],
   "source": [
    "# Variational training\n",
    "\n",
    "training_dataloader = DataLoader(dataset_training, shuffle=True, batch_size=128)\n",
    "\n",
    "def get_data_iterator(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "            \n",
    "def run_all(dataset):\n",
    "    return torch.cat([model(x.to(device)[None])[0] for x, _ in dataset]).cpu()\n",
    "\n",
    "training_data_iterator = get_data_iterator(training_dataloader)\n",
    "\n",
    "max_iterations = 1500\n",
    "for i in range(max_iterations):\n",
    "    x, y = next(training_data_iterator)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    model.train()\n",
    "    y_pred, mu, log_var = model(x)\n",
    "    loss, loss_dict = criterion(y, y_pred, mu, log_var)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if i % 25 == 0:\n",
    "        print({k: round(v.detach().cpu().item(), 3) for k, v in loss_dict.items()})\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Y_train_pred = run_all(dataset_training)\n",
    "            Y_val_pred = run_all(dataset_val)\n",
    "        \n",
    "        print('2v2-cosine-dist', \n",
    "              two_versus_two(dataset_training.tensors[1], Y_train_pred, cosine_distance),\n",
    "              two_versus_two(dataset_val.tensors[1], Y_val_pred, cosine_distance),)\n",
    "        print('2v2-euclidean-dist', \n",
    "              two_versus_two(dataset_training.tensors[1], Y_train_pred, euclidean_distance),\n",
    "              two_versus_two(dataset_val.tensors[1], Y_val_pred, euclidean_distance),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2f050-59af-4681-8124-aa4f80589b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = derivatives_path / 'decoded_features'\n",
    "\n",
    "def run_all_variational(dataset):\n",
    "    mean = []\n",
    "    std = []\n",
    "    for x, _ in dataset:\n",
    "        _, mu, log_var = model(x.to(device)[None])\n",
    "        mean.append(mu)\n",
    "        std.append(torch.exp(0.5 * log_var))\n",
    "    mean = torch.cat(mean)\n",
    "    std = torch.cat(std)\n",
    "    out = torch.stack([mean, std], dim=1)\n",
    "    return out.detach().cpu().numpy()\n",
    "        \n",
    "Y_val_pred = run_all_variational(dataset_val)\n",
    "\n",
    "version = '4-0'\n",
    "code_name = 'sanitycheck'\n",
    "out_dir = out_path / embedding_model / embedding_key / subject_name\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "np.save(out_dir / f'Y_val_pred__{code_name}__v{version}.npy', Y_val_pred)\n",
    "np.save(out_dir / f'Y_val__{code_name}__v{version}.npy', Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa23b86e-07b3-45ae-9d38-5e07f9d19c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9775), tensor(0.9769))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def euclidean_distance(Y1, Y2):\n",
    "    Y1_squared = (Y1 ** 2).sum(dim=-1)\n",
    "    Y2_squared = (Y2 ** 2).sum(dim=-1)\n",
    "    Y1_dot_Y2 = torch.einsum('... i, ... i -> ...', Y1, Y2)\n",
    "    \n",
    "    # recall (y1 - y2)^2 = y1^2 + y2^2 - 2y1*y2\n",
    "    squared_distance = Y1_squared + Y2_squared - 2 * Y1_dot_Y2\n",
    "    return torch.sqrt(squared_distance)\n",
    "\n",
    "def cosine_distance(Y1, Y2):\n",
    "    return 1. - torch.einsum('... i, ... i -> ...', Y1, Y2)\n",
    "\n",
    "def two_versus_two(Y1, Y2, distance_measure):\n",
    "    distances = distance_measure(Y1[None, :], Y2[:, None])\n",
    "    different = distances + distances.T\n",
    "    \n",
    "    distances_diag = torch.diag(distances)\n",
    "    same = distances_diag[None, :] + distances_diag[:, None]\n",
    "    \n",
    "    comparison = same < different\n",
    "    \n",
    "    return comparison[np.triu_indices(Y1.shape[0], k=1)].float().mean()\n",
    "\n",
    "cosine_2v2 = two_versus_two(dataset_val.tensors[1], Y_val_pred, cosine_distance)\n",
    "euclidean_2v2 = two_versus_two(dataset_val.tensors[1], Y_val_pred, euclidean_distance)\n",
    "cosine_2v2, euclidean_2v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "530bcdd2-1c08-4bc6-a770-94c226405ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18f97ed1490>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 50\n",
    "distances = euclidean_distance(Y_val[None, :N], Y_val_pred[:N, None])\n",
    "different = distances + distances.T\n",
    "\n",
    "distances_diag = torch.diag(distances)\n",
    "same = distances_diag[None, :] + distances_diag[:, None]\n",
    "\n",
    "comparison = same < different\n",
    "\n",
    "plt.imshow(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neurophysiological-Data-Decoding",
   "language": "python",
   "name": "neurophysiological-data-decoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
