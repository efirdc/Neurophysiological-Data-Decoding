{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f38025-c211-495d-9b5e-87134834f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchio as tio\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "from scipy import ndimage\n",
    "from fracridge import FracRidgeRegressorCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dir2 = os.path.abspath('../..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "    \n",
    "from research.data.natural_scenes import (\n",
    "    NaturalScenesDataset,\n",
    "    StimulusDataset,\n",
    "    KeyDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8e1f1b-f56c-447e-ba90-3949e2033c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('D:\\\\Datasets\\\\NSD\\\\')\n",
    "nsd = NaturalScenesDataset(dataset_path)\n",
    "\n",
    "derivatives_path = dataset_path / 'derivatives'\n",
    "betas_path = dataset_path / 'nsddata_betas' / 'ppdata'\n",
    "ppdata_path = dataset_path / 'nsddata' / 'ppdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01775854-0e7a-45ca-97f4-54c11789f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image ids of the shared 1000 images across all participants\n",
    "shared_1000_path = dataset_path / 'nsddata' / 'stimuli' / 'nsd' / 'shared1000.tsv'\n",
    "shared_1000 = pd.read_csv(shared_1000_path, sep='\\t', header=None)\n",
    "shared_1000 = set(shared_1000[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ec8364-7ada-4441-b5ad-2457ca1162d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01 image_ids.shape=(27750,), len(three_repetition_ids)=8420\n",
      "subj02 image_ids.shape=(27750,), len(three_repetition_ids)=8420\n",
      "subj03 image_ids.shape=(21750,), len(three_repetition_ids)=5081\n",
      "subj04 image_ids.shape=(20250,), len(three_repetition_ids)=4424\n",
      "subj05 image_ids.shape=(27750,), len(three_repetition_ids)=8420\n",
      "subj06 image_ids.shape=(21750,), len(three_repetition_ids)=5081\n",
      "subj07 image_ids.shape=(27750,), len(three_repetition_ids)=8420\n",
      "subj08 image_ids.shape=(20250,), len(three_repetition_ids)=4424\n",
      "len(shared_1000_three_repetitions)=413\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14924/1242883957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mN_non_shared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshared_1000_three_repetitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msubject_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'asdf' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a train-test-validation split\n",
    "split_name = 'split-01'\n",
    "N_test = 1000\n",
    "N_validation = 1000\n",
    "\n",
    "seed = 0\n",
    "\n",
    "for subject_name, subject_data in nsd.subjects.items():\n",
    "    responses = subject_data['responses']\n",
    "    \n",
    "    image_ids = responses['73KID'].to_numpy()\n",
    "    unique_image_ids, unique_counts = np.unique(image_ids, return_counts=True)\n",
    "    three_repetition_ids = unique_image_ids[unique_counts == 3]\n",
    "    subject_data['three_repetition_ids'] = set(three_repetition_ids)\n",
    "    print(f'{subject_name} {image_ids.shape=}, {len(three_repetition_ids)=}')\n",
    "    \n",
    "shared_1000_three_repetitions = set.intersection(\n",
    "    shared_1000,\n",
    "    *[subject_data['three_repetition_ids']\n",
    "    for subject_data in nsd.subjects.values()]\n",
    ")\n",
    "print(f'{len(shared_1000_three_repetitions)=}')\n",
    "N_non_shared = N_test - len(shared_1000_three_repetitions)\n",
    "\n",
    "print(asdf)\n",
    "\n",
    "for subject_name, subject_data in subjects.items():\n",
    "    three_repetition_ids = subject_data['three_repetition_ids']\n",
    "    non_shared_three_repetition_ids = list(three_repetition_ids - shared_1000_three_repetitions)\n",
    "    random.Random(seed).shuffle(non_shared_three_repetition_ids)\n",
    "    \n",
    "    test_image_ids = list(shared_1000_three_repetitions) + non_shared_three_repetition_ids[:N_non_shared]\n",
    "    validation_image_ids = non_shared_three_repetition_ids[N_non_shared:(N_non_shared + N_validation)]\n",
    "    subject_data['test_image_ids'] = np.array(test_image_ids)\n",
    "    subject_data['validation_image_ids'] = np.array(test_image_ids)\n",
    "    \n",
    "    test_image_ids = set(test_image_ids)\n",
    "    validation_image_ids = set(validation_image_ids)\n",
    "    image_ids = subject_data['responses']['73KID'].to_numpy()\n",
    "    subject_data['test_response_ids'] = np.argwhere([image_id in test_image_ids for image_id in image_ids])[:, 0]\n",
    "    subject_data['validation_response_ids'] = np.argwhere([image_id in validation_image_ids for image_id in image_ids])[:, 0]\n",
    "    \n",
    "with h5py.File(derivatives_path / 'data_splits' / f'{split_name}.hdf5', 'w') as f:\n",
    "    for subject_name, subject_data in subjects.items():\n",
    "        subject = f.require_group(subject_name)\n",
    "        \n",
    "        three_repetition_ids = subject_data['three_repetition_ids']\n",
    "        non_shared_three_repetition_ids = list(three_repetition_ids - shared_1000_three_repetitions)\n",
    "        random.Random(seed).shuffle(non_shared_three_repetition_ids)\n",
    "\n",
    "        test_image_ids = list(shared_1000_three_repetitions) + non_shared_three_repetition_ids[:N_non_shared]\n",
    "        validation_image_ids = non_shared_three_repetition_ids[N_non_shared:(N_non_shared + N_validation)]\n",
    "        subject['test_image_ids'] = np.array(test_image_ids)\n",
    "        subject['validation_image_ids'] = np.array(test_image_ids)\n",
    "\n",
    "        test_image_ids = set(test_image_ids)\n",
    "        validation_image_ids = set(validation_image_ids)\n",
    "        image_ids = subject_data['responses']['73KID'].to_numpy()\n",
    "        subject['test_response_mask'] = np.array([image_id in test_image_ids for image_id in image_ids], dtype=bool)\n",
    "        subject['validation_response_mask'] = np.array([image_id in validation_image_ids for image_id in image_ids], dtype=bool)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
