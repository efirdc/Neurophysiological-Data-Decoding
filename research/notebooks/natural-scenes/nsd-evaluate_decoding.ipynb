{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd1a94-9bf4-4e14-a233-58df70c94891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pylustrator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "from scipy import ndimage, stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "dir2 = os.path.abspath('../..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "    \n",
    "from research.data.natural_scenes import NaturalScenesDataset\n",
    "from research.experiments.nsd.nsd_access import NSDAccess\n",
    "from research.metrics.metrics import cosine_distance, top_knn_test, r2_score, pearsonr\n",
    "from pipeline.utils import get_data_iterator, DisablePrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20988b01-7610-445a-8283-a7e8b51c24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_path = Path('D:\\\\Datasets\\\\NSD\\\\')\n",
    "nsd = NaturalScenesDataset(nsd_path, coco_path='X:\\\\Datasets\\\\COCO')\n",
    "stimuli_path = nsd_path / 'nsddata_stimuli' / 'stimuli' / 'nsd' / 'nsd_stimuli.hdf5'\n",
    "stimulus_images = h5py.File(stimuli_path, 'r')['imgBrick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa741d7-5d49-48b4-bca0-ef4170d5ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT-B=32' #'clip-vit-large-patch14'\n",
    "group_name = 'group-10'\n",
    "\n",
    "subjects = [f'subj0{i}' for i in range(1, 9)]\n",
    "embedding_name = 'embedding'\n",
    "fold_name = 'val'\n",
    "\n",
    "embeddings = h5py.File(nsd_path / f'derivatives/decoded_features/{model_name}/{group_name}.hdf5', 'r')\n",
    "\n",
    "results_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold_name}/{embedding_name}/'\n",
    "results_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "Y_full = h5py.File(nsd_path / f'derivatives/stimulus_embeddings/{model_name}.hdf5', 'r')[embedding_name][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdf8ea-dc2a-442f-94f1-0866eee4ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a clip model\n",
    "import clip\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(clip.available_models())\n",
    "clip_model_name = 'ViT-B/32'\n",
    "full_model, preprocess = clip.load(clip_model_name, device=device)\n",
    "perceptor = full_model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95015065-8ca5-449f-8c54-0d82a097cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine evaluation summaries into one folder\n",
    "\n",
    "import shutil\n",
    "\n",
    "groups = ['group-4', 'group-10', 'group-11', 'group-12', 'group-13', 'group-14']\n",
    "\n",
    "for group_name in groups:\n",
    "    results_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold_name}/{embedding_name}/'\n",
    "    shutil.copyfile(\n",
    "        results_path / 'decoding_evaluation/evaluation_summary.png', \n",
    "        nsd_path / f'derivatives/figures/decoding/{model_name}/decoding_evaluation_summary/{group_name}.png'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41094aa-561a-4718-94ff-cfb07886d202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load encoder data\n",
    "\n",
    "from research.models.fmri_decoders import Decoder\n",
    "\n",
    "num_voxels = None\n",
    "\n",
    "results = {}\n",
    "folds = {}\n",
    "for fold in ('val', 'test'):\n",
    "    fold_data = {\n",
    "        'X_all': [],\n",
    "        'Y_all': [],\n",
    "        'Y_pred_all': [],\n",
    "        'stimulus_ids_all': [],\n",
    "    }\n",
    "    folds[fold] = fold_data\n",
    "models_all = []\n",
    "state_dicts_all = []\n",
    "indices_all = []\n",
    "\n",
    "load_X = True\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "    \n",
    "    train_mask, val_mask, test_mask = nsd.get_split(subject, 'split-01')\n",
    "    \n",
    "    if load_X:\n",
    "        subject_embeddings = embeddings[f'{subject}/{embedding_name}']\n",
    "        config = dict(subject_embeddings.attrs)\n",
    "\n",
    "        model_params = {k: config[k] for k in ('layer_sizes', 'dropout_p')}\n",
    "        model = Decoder(**model_params)\n",
    "        model = model.eval()\n",
    "        state_dict = {k: torch.from_numpy(v[:]) for k, v in subject_embeddings['model'].items()}\n",
    "        state_dicts_all.append(state_dict)\n",
    "        model.load_state_dict({k: v.clone() for k, v in state_dict.items()})\n",
    "        models_all.append(model)\n",
    "        \n",
    "        betas_params = {\n",
    "            k: config[k] \n",
    "            for k in (\n",
    "                'subject_name', 'voxel_selection_path', \n",
    "                'voxel_selection_key', 'num_voxels', 'return_volume_indices', 'threshold'\n",
    "            )\n",
    "        }\n",
    "        if betas_params['threshold'] is not None:\n",
    "            betas_params['num_voxels'] = None\n",
    "            betas_params['return_tensor_dataset'] = False\n",
    "        betas, betas_indices = nsd.load_betas(**betas_params)\n",
    "        folds['val']['X_all'].append(betas[val_mask])\n",
    "        folds['test']['X_all'].append(betas[test_mask])\n",
    "        indices_all.append(betas_indices)\n",
    "\n",
    "    stimulus_params = dict(\n",
    "        subject_name=subject,\n",
    "        stimulus_path=f'derivatives/stimulus_embeddings/{model_name}.hdf5',\n",
    "        stimulus_key=embedding_name,\n",
    "        delay_loading=False,\n",
    "        return_tensor_dataset=False,\n",
    "        return_stimulus_ids=True,\n",
    "    )\n",
    "    stimulus, stimulus_ids = nsd.load_stimulus(**stimulus_params)\n",
    "    for fold, mask in [('val', val_mask), ('test', test_mask)]:\n",
    "        \n",
    "        folds[fold]['stimulus_ids_all'].append(stimulus_ids[mask])\n",
    "    \n",
    "        Y = stimulus[mask].astype(np.float32)\n",
    "        Y = Y.reshape(Y.shape[0], -1)\n",
    "        folds[fold]['Y_all'].append(Y)\n",
    "    \n",
    "        Y_pred = subject_embeddings[f'{fold}/Y_pred'][:]\n",
    "        Y_pred = Y_pred / np.linalg.norm(Y_pred, axis=1)[:, None]\n",
    "        folds[fold][\"Y_pred_all\"].append(Y_pred)\n",
    "        \n",
    "locals().update(folds[fold_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa93124-104a-4cd8-af1f-dce67f24ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(subject_embeddings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89379228-b3db-470d-90a2-94371f287050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e332f2-afac-4107-9466-135667f35fae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3944ed-6c8d-477f-9c6b-1cc272e696cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top k accuracy figure\n",
    "\n",
    "N = 1000\n",
    "top_k_values = [1, 5, 10, 50, 100, 500]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "fold = 'test'\n",
    "metric = 'cosine'\n",
    "\n",
    "for fold in ('val', 'test'):\n",
    "    top_knn_accuracy = {}\n",
    "    for subject_id, subject in enumerate(subjects):\n",
    "        \n",
    "        stimulus_ids = folds[fold]['stimulus_ids_all'][subject_id]\n",
    "        Y = folds[fold]['Y_all'][subject_id]\n",
    "        Y_pred = folds[fold]['Y_pred_all'][subject_id]\n",
    "        \n",
    "        unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "            stimulus_ids, return_index=True, return_inverse=True)\n",
    "        \n",
    "        top_knn_accuracy[subject] = top_knn_test(\n",
    "            Y[unique_index], Y_pred, unique_inverse, k=top_k_values, metric=metric)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.xticks(ticks=range(len(top_k_values)), labels=top_k_values)\n",
    "    plt.title(f'Top knn accuracy (n={N})\\n{model_name=}, {embedding_name=}, {group_name=}')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(len(top_k_values)), chance_accuracy, label='chance (k/n)', color='gray')\n",
    "    for subject, subject_results in top_knn_accuracy.items():\n",
    "        plt.plot(range(len(top_k_values)), subject_results, label=subject)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    file_name = 'top_k_accuracy.png'\n",
    "    out_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold}/{embedding_name}/decoding_evaluation'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    plt.savefig(out_path / file_name, pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19998f7f-241b-40f5-a4d1-52795a1dad62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print standard deviation of each dimension\n",
    "Y_std = Y_full.std(axis=0)\n",
    "Y_std_argsort_ids = Y_std.argsort()\n",
    "print('dim, std')\n",
    "for std, dim in zip(Y_std[Y_std.argsort()], Y_std.argsort()):\n",
    "    print(f'{dim}, {std:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be81054-5165-47a5-a243-b5b1c0887705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 histograms\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 3), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9, left=0.07)\n",
    "\n",
    "out_path = results_path / 'decoding_evaluation'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "r2_all = []\n",
    "r_all = []\n",
    "cosine_dist_all = []\n",
    "distance_rank_all = []\n",
    "top_knn_all = []\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "    print('dim, r2')\n",
    "    r2 = r2_score(torch.from_numpy(Y_all[i]), torch.from_numpy(Y_pred_all[i]), reduction=None)\n",
    "    r2_argsort_ids = np.argsort(r2)\n",
    "    for dim, dim_r2 in zip(r2_argsort_ids[:10], r2[r2_argsort_ids[:10]]):\n",
    "        print(f'{round(dim.item(), 3)}, {round(dim_r2.item(), 3)}')\n",
    "    r2_all.append(r2)\n",
    "    r2 = torch.clone(r2)\n",
    "    r2[r2 < -0.25] = 0\n",
    "    ax[i].hist(r2)\n",
    "    #ax[i].set_ylim(0, 225)\n",
    "    ax[i].set_xlim(-0.1, 0.7) \n",
    "    ax[i].set_xticks([0.0, 0.2, 0.4, 0.6])\n",
    "    #if i > 0:\n",
    "        #ax[i].set_yticks([])\n",
    "    #ax[i].set_xlabel(subject)\n",
    "\n",
    "fig.suptitle('Histogram of R^2 for Brain-Decoded Embeddings, 8 Participants')\n",
    "fig.supxlabel('R^2')\n",
    "fig.supylabel('Number of Dimensions')\n",
    "file_name = 'variance_explained_histogram.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "# r histograms\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 3), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9, left=0.07)\n",
    "for i, subject in enumerate(subjects):\n",
    "    r = pearsonr(torch.from_numpy(Y_all[i]), torch.from_numpy(Y_pred_all[i]), reduction=None)\n",
    "    r_all.append(r)\n",
    "    ax[i].hist(r)\n",
    "\n",
    "fig.suptitle('Histogram of pearsonr for Brain-Decoded Embeddings, 8 Participants')\n",
    "fig.supxlabel('pearsonr')\n",
    "fig.supylabel('# Dimensions')\n",
    "file_name = 'pearsonr_histogram.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "# cosine histogram\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 3), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9, left=0.07)\n",
    "for i, subject in enumerate(subjects):\n",
    "    cosine_dist = 1. - (Y_all[i] * Y_pred_all[i]).sum(axis=1)\n",
    "    cosine_dist_all.append(cosine_dist)\n",
    "    ax[i].hist(cosine_dist)\n",
    "\n",
    "fig.suptitle('Histogram of Cosine Distance for Brain-Decoded Embeddings, 8 Participants')\n",
    "fig.supxlabel('Cosine Distance')\n",
    "fig.supylabel('# Stimuli')\n",
    "file_name = 'cosine_distance_histogram.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "# distance classification histogram\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 3), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9, left=0.07)\n",
    "for i, subject in enumerate(subjects):\n",
    "    Y = Y_all[i]\n",
    "    Y_pred = Y_pred_all[i]\n",
    "    stimulus_ids = stimulus_ids_all[i]\n",
    "    unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "        stimulus_ids, return_index=True, return_inverse=True\n",
    "    )\n",
    "    Y_dists = 1. - Y[unique_index] @ Y_pred.T\n",
    "    Y_dist_ids = Y_dists.argsort(axis=0)\n",
    "    Y_ks = np.argwhere((Y_dist_ids == unique_inverse[None, :]).T)\n",
    "    \n",
    "    distance_rank_all.append(Y_ks)\n",
    "    top_knn_all.append(top_knn_test(\n",
    "            Y[unique_index], Y_pred, unique_inverse, k=top_k_values, metric=metric))\n",
    "    ax[i].hist(Y_ks[:, 1])\n",
    "\n",
    "fig.suptitle('Histogram of Distance Classifcation Rankings for Brain-Decoded Embeddings, 8 Participants')\n",
    "fig.supxlabel('Classification Rank')\n",
    "fig.supylabel('# Stimuli')\n",
    "file_name = 'distance_ranking_histogram.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820023f-e647-421c-b7d9-ec439e4099a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lines\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    subject_path = out_path / f'dimension_plots/{subject}'\n",
    "    subject_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    r = r_all[subject_id]\n",
    "    r2 = r2_all[subject_id]\n",
    "    r2_argsort = np.argsort(r2)\n",
    "    num_dims = r.shape[0]\n",
    "    dim_ids = np.arange(num_dims)\n",
    "    \n",
    "    dim_ids = np.concatenate([dim_ids[:5], dim_ids[num_dims // 2 - 2: num_dims // 2 + 2], dim_ids[-5:]])\n",
    "    dims = r2_argsort[dim_ids]\n",
    "    \n",
    "    dim_start = 0\n",
    "    dim_end = 100\n",
    "    \n",
    "    for i, dim in enumerate(dims):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(dim_start, dim_end), Y_all[subject_id][dim_start:dim_end, dim], label='target')\n",
    "        plt.plot(np.arange(dim_start, dim_end), Y_pred_all[subject_id][dim_start:dim_end, dim], label='prediction')\n",
    "        plt.title(f'{subject=}, r2_rank={dim_ids[i]}, dim={int(dim)}, r2={r2[dim]:.2f}, r={r[dim]:.2f}')\n",
    "        plt.legend()\n",
    "        file_name = f'rank-{dim_ids[i]}_dim-{dim}.png'\n",
    "        plt.savefig(subject_path / file_name, pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802c2f-9ca2-4fab-baff-174204c7daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "num_checks = 50\n",
    "\n",
    "font = ImageFont.truetype('arial.ttf', 64)\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    Y = Y_all[subject_id]\n",
    "    Y_pred = Y_pred_all[subject_id]\n",
    "    stimulus_ids = stimulus_ids_all[subject_id]\n",
    "    unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "        stimulus_ids, return_index=True, return_inverse=True\n",
    "    )\n",
    "    Y_dists = 1. - Y[unique_index] @ Y_pred.T\n",
    "    Y_dist_ids = Y_dists.argsort(axis=0)\n",
    "    Y_ks = np.argwhere((Y_dist_ids == unique_inverse[None, :]).T)\n",
    "    Y_dist_stimulus_ids = unique_stimulus_ids[Y_dist_ids]\n",
    "    \n",
    "    for i, (y_ks, y_dist_stimulus_ids) in enumerate(zip(Y_ks, Y_dist_stimulus_ids.T)):\n",
    "        y_k = y_ks[1]\n",
    "        y_dists = Y_dists[:, i]\n",
    "        y_dists.sort()\n",
    "        \n",
    "        show_ids = np.array([0, 1, 2, *[i for i in range(np.clip(y_k, 6, 993) - 3, np.clip(y_k, 6, 993) + 4)]])\n",
    "\n",
    "        show_stimulus_ids = y_dist_stimulus_ids[show_ids]\n",
    "\n",
    "        images = np.concatenate([stimulus_images[stimulus_id] for stimulus_id in show_stimulus_ids], axis=1)\n",
    "\n",
    "        text_images = []\n",
    "        for j in show_ids:\n",
    "            img = Image.new('RGB', (425, (64 + 25) * 2))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            fill = 'green'\n",
    "            if j < y_k:\n",
    "                fill = 'red'\n",
    "            elif j > y_k:\n",
    "                fill = 'blue'\n",
    "            draw.multiline_text((212, 50), str(j), fill=fill, anchor='mm', font=font, align='center')\n",
    "            draw.multiline_text((212, 64+50), f'd={y_dists[j]:.2f}', anchor='mm', font=font, align='center')\n",
    "            text_images.append(img)\n",
    "        text_images = np.concatenate(text_images, axis=1)\n",
    "        images = np.concatenate([images, text_images], axis=0)\n",
    "    \n",
    "        out_path = results_path / 'distance_ranking' / subject\n",
    "        out_path.mkdir(exist_ok=True, parents=True)\n",
    "        file_name = f'stim-{y_dist_stimulus_ids[y_k]}_image-{i}.png'\n",
    "        Image.fromarray(images).save(out_path / file_name)\n",
    "        if i == num_checks:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b1835-45a3-459c-8786-938be9e0fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dist_stimulus_ids = unique_stimulus_ids[Y_dist_ids]\n",
    "Y_dist_stimulus_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5653a87-d037-49c8-9833-757a03676f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8, 8), sharex=True)\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.95, wspace=0.25)\n",
    "plt.suptitle(f'Brain-Decoder Evaluation, {group_name=}')\n",
    "\n",
    "\n",
    "ax[0, 0].bar(np.arange(8), [r2.mean() for r2 in r2_all])\n",
    "ax[0, 0].set_ylabel('R^2')\n",
    "ax[0, 1].bar(np.arange(8), [r.mean() for r in r_all])\n",
    "ax[0, 1].set_ylabel('Pearson Correlation')\n",
    "ax[1, 0].bar(np.arange(8), [cosine_dist.mean() for cosine_dist in cosine_dist_all])\n",
    "ax[1, 0].set_ylabel('Cosine Distance')\n",
    "ax[1, 0].set_xlabel('Subject')\n",
    "ax[1, 1].bar(np.arange(8), [top_knn[0] for top_knn in top_knn_all])\n",
    "ax[1, 1].set_ylabel('Top 1 Accuracy')\n",
    "ax[1, 1].set_xlabel('Subject')\n",
    "\n",
    "for x in ax.flat:\n",
    "    x.set_xticks(np.arange(8), np.arange(1, 9))\n",
    "    \n",
    "file_name = 'evaluation_summary.png'\n",
    "plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcdcf8-9ae0-4f92-b5b6-9947d682f4ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768fd48-d4b9-407e-b742-2b480704af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit individual and group PCAs\n",
    "\n",
    "Y_pca = PCA()\n",
    "Y_pca.fit(Y_full)\n",
    "\n",
    "Y_pred_group_pca = PCA()\n",
    "Y_pred_group_pca.fit(np.concatenate(Y_pred_all))\n",
    "\n",
    "Y_pred_pcas = []\n",
    "for i, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "    \n",
    "    Y_pred_pca = PCA()\n",
    "    Y_pred_pcas.append(Y_pred_pca)\n",
    "    Y_pred_pca.fit(Y_pred_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c13f80-43d7-4f60-bb59-c811cfb7f123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute variance explained of different PCAs\n",
    "\n",
    "subject_id = 0\n",
    "\n",
    "r2_results = {}\n",
    "\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    Y_pred = Y_pred_all[subject_id]\n",
    "    Y_pred_pca = Y_pred_pcas[subject_id]\n",
    "    r2_results[subject] = subject_results = {}\n",
    "    \n",
    "    pcas = [('subject', Y_pred_pca), ('group', Y_pred_group_pca), ('full', Y_pca)]\n",
    "    for pca_name, pca in pcas:\n",
    "        print(subject, pca_name)\n",
    "        subject_results[pca_name] = []\n",
    "        Y_pred_transformed = pca.transform(Y_pred)\n",
    "        for j in range(Y_pred.shape[1]):\n",
    "            Y_pred_inv = Y_pred_transformed[:, :j+1] @ pca.components_[:j+1] + pca.mean_\n",
    "            r2 = r2_score(torch.from_numpy(Y_pred), torch.from_numpy(Y_pred_inv))\n",
    "            subject_results[pca_name].append(r2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a883fe-693b-4882-abf7-49eec9318dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance explained of PCAs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 3), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9, left=0.05)\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    subject_results = r2_results[subject]\n",
    "    ax[i].set_xscale('log', base=2)\n",
    "    ticks = [1, 2, 8, 32, 128, 512]\n",
    "    ax[i].set_xticks(ticks=ticks, labels=ticks)\n",
    "    ax[i].grid(visible=True)\n",
    "    #ax[i].set_xlim(1, 512)\n",
    "    #ax[i].set_ylim(0., 1.)\n",
    "    #ax[i].axhline(0.95, color='black', linestyle='dashed')\n",
    "    #for t in ticks[1:-1]:\n",
    "        #ax[i].axvline(t, color='gray', linestyle='dashed')\n",
    "    for pca_name, r2 in subject_results.items():\n",
    "        #r2_sum = [sum(r2[:i+1]) for i in range(Y_full.shape[1])]\n",
    "        x = np.arange(Y_full.shape[1]) + 1\n",
    "        ax[i].plot(x, r2, label=pca_name)\n",
    "\n",
    "ax[0].legend()\n",
    "fig.suptitle('Variance Explained in Brain-Decoded Embeddings by PCA Components, 8 Participants')\n",
    "fig.supxlabel('Number of PCA Components')\n",
    "fig.supylabel('Variance Explained')\n",
    "file_name = 'pca_variance_explained.png'\n",
    "plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe87e3-9f7a-4622-bf43-e55b36f457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of PCA components (group vs individual)\n",
    "\n",
    "num_components = 32\n",
    "\n",
    "group_components = Y_pred_group_pca.components_[:num_components]\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(12, 6), )\n",
    "ax = ax.flatten()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    subject_components = Y_pred_pcas[i].components_[:num_components]\n",
    "    ticks = [2,  8, 16, 24, 32]\n",
    "    ax[i].set_xticks(np.array(ticks)-1, ticks)\n",
    "    ax[i].imshow(subject_components @ group_components.T, vmin=-1, vmax=1, cmap='bwr')\n",
    "#ax[0].legend()\n",
    "fig.suptitle('RSM Between Subject and Group PCA Components')\n",
    "#fig.supxlabel('Number of PCA Components')\n",
    "#fig.supylabel('Variance Explained')\n",
    "file_name = 'pca_component_rsm.png'\n",
    "plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fce5b-8cac-4b43-9920-426da87bd0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similarity of PCA components (group vs full)\n",
    "\n",
    "num_components = 32\n",
    "group_components = Y_pred_group_pca.components_\n",
    "full_components = Y_pca.components_\n",
    "\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "fig.tight_layout()\n",
    "plt.imshow(group_components @ full_components.T, vmin=-1, vmax=1, cmap='bwr')\n",
    "\n",
    "file_name = 'pca_component_rsm_full_vs_group.png'\n",
    "plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0d15-499d-4b62-a5db-9d1ff15289db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained of the decoding model in the PCA space\n",
    "\n",
    "# Plot variance explained of PCAs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=8, figsize=(2 * len(subjects), 8), \n",
    "                      sharex=True, sharey=True,)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.075, top=0.9,)\n",
    "\n",
    "num_components = 32\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "    Y = Y_all[subject_id]\n",
    "    Y_pred = Y_pred_all[subject_id]\n",
    "    Y_pca_tranformed = Y_pred_group_pca.transform(Y)[:, :num_components]\n",
    "    Y_pred_pca_transformed = Y_pred_group_pca.transform(Y_pred)[:, :num_components]\n",
    "    \n",
    "    #Y_pca_tranformed = np.maximum(-Y_pca_tranformed, 0)\n",
    "    #Y_pred_pca_transformed = np.maximum(-Y_pred_pca_transformed, 0)\n",
    "    \n",
    "    r2_whole = r2_score(\n",
    "        torch.from_numpy(Y_pca_tranformed), \n",
    "        torch.from_numpy(Y_pred_pca_transformed),\n",
    "    reduction=None)\n",
    "    \n",
    "    r2_positive = r2_score(\n",
    "        torch.from_numpy(np.maximum(Y_pca_tranformed, 0)), \n",
    "        torch.from_numpy(np.maximum(Y_pred_pca_transformed, 0)),\n",
    "    reduction=None)\n",
    "    \n",
    "    r2_negative = r2_score(\n",
    "        torch.from_numpy(np.maximum(-Y_pca_tranformed, 0)), \n",
    "        torch.from_numpy(np.maximum(-Y_pred_pca_transformed, 0)),\n",
    "    reduction=None)\n",
    "    \n",
    "    #print(r2[:num_components].tolist())\n",
    "    for i in range(3):\n",
    "        ax[i, subject_id].set_ylim(-0.5, 0.8)\n",
    "        ax[i, subject_id].set_xticks([1, 8, 16, 24, 32])\n",
    "    ax[0, subject_id].bar(x=np.arange(num_components), height=r2_whole[:num_components], bottom=0, width=1.)\n",
    "    ax[1, subject_id].bar(x=np.arange(num_components), height=r2_positive[:num_components], bottom=0, width=1.)\n",
    "    ax[2, subject_id].bar(x=np.arange(num_components), height=r2_negative[:num_components], bottom=0, width=1.)\n",
    "    \n",
    "    ax[0, subject_id].set_title(subject)\n",
    "\n",
    "#ax[0].legend()\n",
    "ax[0, 0].set_ylabel('Whole PCA')\n",
    "ax[1, 0].set_ylabel('Positive PCA')\n",
    "ax[2, 0].set_ylabel('Negative PCA')\n",
    "fig.suptitle('Variance Explained by Brain-Decoding Model in Group PCA Space')\n",
    "fig.supxlabel('PCA Dimension')\n",
    "#fig.supylabel('Variance Explained')\n",
    "file_name = 'decoding_variance_explained_group_pca.png'\n",
    "plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c930ebd-5386-4911-a0b6-104b9566ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K accuracy in PCA space\n",
    "\n",
    "top_k_values = [1]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "metric = 'cosine'\n",
    "\n",
    "for fold in ('val', 'test'):\n",
    "    results = {}\n",
    "    for subject_id, subject in enumerate(subjects):\n",
    "        print(subject)\n",
    "        \n",
    "        Y = folds[fold]['Y_all'][subject_id]\n",
    "        Y_pred = folds[fold]['Y_pred_all'][subject_id]\n",
    "        Y_pca_tranformed = Y_pred_group_pca.transform(Y)\n",
    "        Y_pred_pca_transformed = Y_pred_group_pca.transform(Y_pred)\n",
    "\n",
    "        stimulus_ids = folds[fold]['stimulus_ids_all'][subject_id]\n",
    "        \n",
    "        unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "            stimulus_ids, return_index=True, return_inverse=True)\n",
    "        \n",
    "        results[subject] = top_1_accuracy = []\n",
    "        for num_components in range(1, Y.shape[1]):\n",
    "            top_1_accuracy.append(top_knn_test(\n",
    "                Y_pca_tranformed[unique_index][:, :num_components], \n",
    "                Y_pred_pca_transformed[:, :num_components], \n",
    "                unique_inverse, k=top_k_values, metric=metric\n",
    "            )[0])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, (subject_name, top_1_accuracy) in enumerate(results.items()):\n",
    "        x = np.arange(Y_full.shape[1] - 1) + 1\n",
    "        plt.plot(x, top_1_accuracy, label=subject_name)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xticks(ticks=[i * 32 for i in range(17)])\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel('num PCA components')\n",
    "    plt.ylabel('top 1 accuracy')\n",
    "    out_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold}/{embedding_name}'\n",
    "    file_name = 'pca_vs_top1.png'\n",
    "    plt.savefig(out_path / file_name, pad_inches=0)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fd5c1-2e34-4764-bba1-2e7e95781666",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1370ccc-c53b-4b29-8704-2ba74b80a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold}/{embedding_name}'\n",
    "out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052654f-c4ec-4478-8cdd-05a19fb37529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(2 * len(subjects), 4), \n",
    "#                      sharex=True, sharey=True,)\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(bottom=0.075, top=0.9,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878ce5f-7841-4ac3-a4ea-1aa71f8054c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find PCA component images with CLIP retrieval\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, OPTICS, KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "from torchvision import transforms\n",
    "from urllib.request import urlopen, HTTPError, URLError\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "indice_name = 'laion_400m'\n",
    "client = ClipClient(\n",
    "    url=\"https://knn5.laion.ai/knn-service\", \n",
    "    indice_name=indice_name,\n",
    "    aesthetic_score=9,\n",
    "    aesthetic_weight=0.0,\n",
    "    use_safety_model=False,\n",
    "    use_violence_detector=True,\n",
    "    num_images=500,\n",
    ")\n",
    "dims = 512\n",
    "\n",
    "num_components = 32\n",
    "group_components = Y_pred_group_pca.components_\n",
    "\n",
    "trans = torch.nn.Sequential(\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    ")\n",
    "\n",
    "for sign, sign_name in [(1, 'positive'), (-1, 'negative')]:\n",
    "    num_images = 10\n",
    "    images = []\n",
    "    for c in range(num_components):\n",
    "        print(c)\n",
    "        response = client.query(embedding_input=(group_components[c] * sign).tolist())\n",
    "        component_images = []\n",
    "        for r in response:\n",
    "            try:\n",
    "                component_images.append(trans(Image.open(urlopen(r['url'], timeout=10))).convert('RGB'))\n",
    "            except UnidentifiedImageError:\n",
    "                print(\"UnidentifiedImageError\")\n",
    "            except HTTPError:\n",
    "                print(\"HTTPError\")\n",
    "            except URLError:\n",
    "                print(\"URLError\")\n",
    "            except ConnectionResetError:\n",
    "                print(\"ConnectionResetError\")\n",
    "            except:\n",
    "                print(\"timeout?\")\n",
    "\n",
    "            if len(component_images) >= num_images:\n",
    "                break\n",
    "        component_images = np.concatenate(component_images, axis=1)\n",
    "        images.append(component_images)\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    file_name = f'pca_component_images_{sign_name}.png'\n",
    "    Image.fromarray(images).save(results_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628b8a5-d64a-4705-8cab-e0a0a207d83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA brain decoded image retreival\n",
    "\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "from torchvision import transforms\n",
    "from urllib.request import urlopen, HTTPError, URLError\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "run_name = 'run-001'\n",
    "average_repetitions = False\n",
    "num_images = 3\n",
    "\n",
    "indice_name = 'laion_400m'\n",
    "clip_retreival_params = dict(\n",
    "    url=\"https://knn5.laion.ai/knn-service\", \n",
    "    indice_name=indice_name,\n",
    "    aesthetic_score=9,\n",
    "    aesthetic_weight=0.0,\n",
    "    use_safety_model=False,\n",
    "    use_violence_detector=False,\n",
    "    num_images=500,\n",
    ")\n",
    "\n",
    "client = ClipClient(\n",
    "    **clip_retreival_params\n",
    ")\n",
    "\n",
    "trans = torch.nn.Sequential(\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    ")\n",
    "\n",
    "out_path = results_path / run_name\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(out_path / 'params.json', 'w') as f:\n",
    "    f.write(json.dumps({\n",
    "        'average_repetitions': average_repetitions, \n",
    "        'run_name': run_name, \n",
    "        **clip_retreival_params\n",
    "    }))\n",
    "\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    subject_path = out_path / subject / 'images'\n",
    "    subject_path.mkdir(exist_ok=True, parents=True)\n",
    "    print(subject_path)\n",
    "    \n",
    "    Y_pred = Y_pred_all[subject_id]\n",
    "    stimulus_ids = stimulus_ids_all[subject_id]\n",
    "    unique_stimulus_ids = np.unique(stimulus_ids)\n",
    "    \n",
    "    if average_repetitions:\n",
    "        Y_pred = np.stack([Y_pred[i == stimulus_ids].mean(axis=0) for i in unique_stimulus_ids])\n",
    "        stimulus_ids = unique_stimulus_ids\n",
    "    \n",
    "    for stimulus_id in unique_stimulus_ids:\n",
    "        stimulus_image = trans(Image.fromarray(stimulus_images[stimulus_id]))\n",
    "        \n",
    "        for image_version, image_id in enumerate(np.where(stimulus_ids == stimulus_id)[0]):\n",
    "            y_pred = Y_pred[image_id]\n",
    "            images = [stimulus_image]\n",
    "            response = client.query(embedding_input=y_pred.tolist())\n",
    "            for r in response:\n",
    "                try:\n",
    "                    images.append(trans(Image.open(urlopen(r['url'], timeout=10))).convert('RGB'))\n",
    "                except UnidentifiedImageError:\n",
    "                    print(\"UnidentifiedImageError\")\n",
    "                except HTTPError:\n",
    "                    print(\"HTTPError\")\n",
    "                except URLError:\n",
    "                    print(\"URLError\")\n",
    "                except ConnectionResetError:\n",
    "                    print(\"ConnectionResetError\")\n",
    "                except:\n",
    "                    print(\"timeout?\")\n",
    "\n",
    "                if len(images) > num_images:\n",
    "                    break\n",
    "\n",
    "            images = np.concatenate(images, axis=1)\n",
    "            file_name = f'stim-{stimulus_id}_image-{image_id}_v-{image_version}.png'\n",
    "            Image.fromarray(images).save(subject_path / file_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac08dc6-048a-4683-b468-83cffb75b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA component distributions\n",
    "\n",
    "Y_pred_all_transformed = Y_pred_group_pca.transform(np.concatenate(Y_pred_all))\n",
    "\n",
    "size = 2\n",
    "fig, ax = plt.subplots(nrows=4, ncols=8, figsize=(8 * size, 4 * size), \n",
    "                      sharex=False, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.1, top=0.9, left=0.07)\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    ax[i].hist(Y_pred_all_transformed[:, i])\n",
    "    #ax[i].set_ylim(0, 225)\n",
    "    #ax[i].set_xlim(-0.1, 0.7) \n",
    "    #ax[i].set_xticks([0.0, 0.2, 0.4, 0.6])\n",
    "    #if i > 0:\n",
    "        #ax[i].set_yticks([])\n",
    "    #ax[i].set_xlabel(subject)\n",
    "\n",
    "fig.suptitle('Histogram of Group PCA Dimensions')\n",
    "fig.supxlabel('Dimension Values')\n",
    "fig.supylabel('Number of Stimuli')\n",
    "file_name = 'group_pca_histogram.png'\n",
    "plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f661a86-58df-427b-8f56-efaa9fab6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_all_transformed = Y_pred_group_pca.transform(np.concatenate(Y_pred_all))\n",
    "\n",
    "for component_id in range(32):\n",
    "    y = Y_pred_all_transformed[:, component_id]\n",
    "    sample_points = np.linspace(y.min(), y.max(), 90)\n",
    "    sample_ids = [np.argmin(np.abs(y - s)) for s in sample_points]\n",
    "    sample_stimulus_ids = np.concatenate(stimulus_ids_all)[sample_ids]\n",
    "    sample_images = np.array([stimulus_images[i] for i in sample_stimulus_ids])\n",
    "    sample_images = rearrange(sample_images, '(n1 n2) h w c -> (n2 h) (n1 w) c', n2=3)\n",
    "    out_path = results_path / 'pca_component_images'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    file_name = f'component-{component_id}.png'\n",
    "    Image.fromarray(sample_images).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc461d93-04f7-48bb-95db-3c17cff1a3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decoded tsne\n",
    "Y_pred_tsne = []\n",
    "tsne_params = dict(\n",
    "    n_components=2, \n",
    "    metric='cosine',\n",
    "    init=\"pca\", \n",
    "    #learning_rate=\"auto\", \n",
    "    random_state=2,\n",
    "    verbose=1, \n",
    ")\n",
    "for subject_id in range(8):\n",
    "    print(subject_id)\n",
    "    top_n_voxels = None\n",
    "    tsne = TSNE(**tsne_params)\n",
    "    Y_pred_tsne.append(tsne.fit_transform(Y_pred_all[subject_id]))\n",
    "Y_pred_group_tsne = TSNE(**tsne_params).fit_transform(np.concatenate(Y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261fc7e-e54a-4fab-b427-94818287378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_image_plot(y, stimulus_ids, stimulus_images, image_size, num_images, extent):\n",
    "    S = image_size * num_images\n",
    "    full_image = np.zeros(shape=(S, S, 3), dtype=np.ubyte)\n",
    "\n",
    "    coords = np.linspace(-extent, extent, num_images)\n",
    "    grid = np.stack(np.meshgrid(coords, coords))\n",
    "    grid = rearrange(grid, 'd h w -> (h w) d')\n",
    "    grid.shape\n",
    "\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    neighbors = NearestNeighbors(metric='chebyshev')\n",
    "    neighbors.fit(y)\n",
    "\n",
    "    distances, ids = neighbors.kneighbors(grid, n_neighbors=1,)\n",
    "    distances = rearrange(distances, '(h w) d -> h w d', h=num_images)\n",
    "    ids = rearrange(ids, '(h w) d -> h w d', h=num_images)\n",
    "\n",
    "    distance_threshold = extent / num_images\n",
    "    for i in range(num_images):\n",
    "        for j in range(num_images):\n",
    "            if distances[i, j] > distance_threshold:\n",
    "                continue\n",
    "            stimulus_id = stimulus_ids[ids[i, j, 0]]\n",
    "            stim_image = stimulus_images[stimulus_id]\n",
    "            stim_image = Image.fromarray(stim_image)\n",
    "            stim_image = stim_image.resize(size=(image_size, image_size), resample=PIL.Image.LANCZOS)\n",
    "            stim_image = np.array(stim_image)\n",
    "            full_image[i * image_size:(i + 1) * image_size, j * image_size:(j + 1) * image_size] = stim_image\n",
    "    return full_image\n",
    "\n",
    "\n",
    "out_path = results_path / 'tsne'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    full_image = tsne_image_plot(\n",
    "        y=Y_pred_tsne[subject_id], \n",
    "        stimulus_ids=stimulus_ids_all[subject_id], \n",
    "        stimulus_images=stimulus_images,\n",
    "        image_size=64, num_images=120, extent=60\n",
    "    )\n",
    "\n",
    "    file_name = f'tsne_{subject}.png'\n",
    "    Image.fromarray(full_image).save(out_path / file_name)\n",
    "    \n",
    "full_image = tsne_image_plot(\n",
    "    y=Y_pred_group_tsne, \n",
    "    stimulus_ids=np.concatenate(stimulus_ids_all), \n",
    "    stimulus_images=stimulus_images,\n",
    "    image_size=64, num_images=200, extent=60\n",
    ")\n",
    "\n",
    "file_name = f'tsne_group.png'\n",
    "Image.fromarray(full_image).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04c3bf-e273-4d60-9584-72b723746f3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4b831-9849-4849-b1f1-dc4366979a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-hot clip retreival\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, OPTICS, KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "from torchvision import transforms\n",
    "from urllib.request import urlopen, HTTPError, URLError\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "indice_name = 'laion_400m'\n",
    "client = ClipClient(\n",
    "    url=\"https://knn5.laion.ai/knn-service\", \n",
    "    indice_name=indice_name,\n",
    "    aesthetic_score=9,\n",
    "    aesthetic_weight=0.0,\n",
    "    use_safety_model=False,\n",
    "    use_violence_detector=False,\n",
    "    num_images=500,\n",
    ")\n",
    "dims = 512\n",
    "\n",
    "trans = torch.nn.Sequential(\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    ")\n",
    "\n",
    "num_images = 9\n",
    "for dim in range(dims):\n",
    "    print(dim)\n",
    "    embedding_input = np.zeros(dims)\n",
    "    embedding_input[dim] = 1.\n",
    "    response = client.query(embedding_input=embedding_input.tolist())\n",
    "    images = []\n",
    "    for r in response:\n",
    "        try:\n",
    "            images.append(trans(Image.open(urlopen(r['url'], timeout=10))).convert('RGB'))\n",
    "        except UnidentifiedImageError:\n",
    "            print(\"UnidentifiedImageError\")\n",
    "        except HTTPError:\n",
    "            print(\"HTTPError\")\n",
    "        except URLError:\n",
    "            print(\"URLError\")\n",
    "        except ConnectionResetError:\n",
    "            print(\"ConnectionResetError\")\n",
    "        except:\n",
    "            print(\"timeout?\")\n",
    "\n",
    "        if len(images) >= num_images:\n",
    "            break\n",
    "    \n",
    "    image = np.stack(images)\n",
    "    image = rearrange(image, '(n1 n2) h w c -> (n1 h) (n2 w) c', n1=3)\n",
    "    out_path = nsd_path / f'derivatives/clip_retreival/{indice_name}/'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    file_name = f'dim_{dim:04}.png'\n",
    "    Image.fromarray(image).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f66a29-473a-4fe5-aeb1-d916699241f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-hot stimulus set retreival\n",
    "\n",
    "trans = torch.nn.Sequential(\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    ")\n",
    "\n",
    "num_images = 9\n",
    "for dim in range(Y_full.shape[1]):\n",
    "    y = Y_full[:, dim]\n",
    "    argsort_ids = np.argsort(y)\n",
    "    \n",
    "    positive_images = []\n",
    "    negative_images = []\n",
    "    \n",
    "    for stimulus_id in argsort_ids[:num_images]:\n",
    "        positive_images.append(trans(Image.fromarray(stimulus_images[stimulus_id])))\n",
    "    for stimulus_id in argsort_ids[::-1][:num_images]:\n",
    "        negative_images.append(trans(Image.fromarray(stimulus_images[stimulus_id])))\n",
    "        \n",
    "    positive_image = rearrange(np.stack(positive_images), '(n1 n2) h w c -> (n1 h) (n2 w) c', n1=3)\n",
    "    negative_image = rearrange(np.stack(negative_images), '(n1 n2) h w c -> (n1 h) (n2 w) c', n1=3)\n",
    "    \n",
    "    out_path = nsd_path / f'derivatives/clip_retreival/mscoco_73k/'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    Image.fromarray(positive_image).save(out_path /  f'dim_{dim:04}_positive.png')\n",
    "    Image.fromarray(negative_image).save(out_path /  f'dim_{dim:04}_negative.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e8a7e-3eb7-439a-9917-d00309ea0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_images_73k(Y_dims, num_images=10, labels=None, font_size=32, concatenate=True):\n",
    "    trans = torch.nn.Sequential(\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "    )\n",
    "    Y_full_activations = Y_full @ Y_dims.T\n",
    "    \n",
    "    font = ImageFont.truetype('arial.ttf', font_size)\n",
    "    images_all = []\n",
    "    for i in range(Y_dims.shape[0]):\n",
    "        y = Y_full_activations[:, i]\n",
    "        stimulus_ids = np.argsort(y)[::-1]\n",
    "        images = [\n",
    "            trans(Image.fromarray(stimulus_images[stimulus_id]))\n",
    "            for stimulus_id in stimulus_ids[:num_images]\n",
    "        ]\n",
    "        if labels is not None:\n",
    "            img = Image.new('RGB', (256, 256))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.multiline_text((128, 128),labels[i], anchor='mm', font=font, align='center')\n",
    "            images = [img] + images\n",
    "        if concatenate:\n",
    "            images = np.concatenate(images, axis=1)\n",
    "        images_all.append(images)\n",
    "    if concatenate:\n",
    "        images_all = np.concatenate(images_all)\n",
    "    \n",
    "    return images_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9decc-e9c2-44d1-a382-dd764ea11658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.experiments.nsd.nsd_clip_reconstruction import (\n",
    "    reconstruct,\n",
    "    load_vqgan_model\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "model_name = \"vqgan_imagenet_f16_16384\"\n",
    "vqgan_checkpoint = f\"{model_name}.ckpt\"\n",
    "vqgan_config = f\"{model_name}.yaml\"\n",
    "vqgan_model = load_vqgan_model(vqgan_config, vqgan_checkpoint).cuda()\n",
    "\n",
    "def top_images_clip_recon(Y_dims, num_images=10, labels=None):\n",
    "    font = ImageFont.truetype('arial.ttf', 32)\n",
    "    \n",
    "    images_all = []\n",
    "    for i in range(Y_dims.shape[0]):\n",
    "        images = []\n",
    "        for j in range(num_images):\n",
    "            recon_image = reconstruct(\n",
    "                stimulus_embeddings={'embedding': Y_dims[i:i+1].cuda()},\n",
    "                hook_modules={'': 'embedding'},\n",
    "                model=vqgan_model,\n",
    "                vqgan_checkpoint=vqgan_checkpoint,\n",
    "                perceptor=full_model.visual.eval().requires_grad_(False).to(device),\n",
    "                device=torch.device('cuda'),\n",
    "                max_iterations=500,\n",
    "                embedding_iterations=500,\n",
    "            )\n",
    "            images.append(recon_image[-1][0])\n",
    "            \n",
    "        if labels is not None:\n",
    "            img = Image.new('RGB', (224, 224))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.multiline_text((112, 112),labels[i], anchor='mm', font=font, align='center')\n",
    "            images = [img] + images\n",
    "        images = np.concatenate(images, axis=1)\n",
    "        images_all.append(images)\n",
    "    images_all = np.concatenate(images_all)\n",
    "    \n",
    "    return images_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a5b06-bd91-4f96-b433-6105e40c51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "from sklearn.cluster import AgglomerativeClustering, OPTICS, KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from torchvision import transforms\n",
    "from urllib.request import urlopen, HTTPError, URLError\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "\n",
    "def top_images_clip_retreival(Y_dims, num_images=10, use_safety_model=False, use_violence_detector=False,):\n",
    "    indice_name = 'laion_400m'\n",
    "    client = ClipClient(\n",
    "        url=\"https://knn5.laion.ai/knn-service\", \n",
    "        indice_name=indice_name,\n",
    "        aesthetic_score=9,\n",
    "        aesthetic_weight=0.0,\n",
    "        use_safety_model=use_safety_model,\n",
    "        use_violence_detector=use_violence_detector,\n",
    "        num_images=500,\n",
    "    )\n",
    "\n",
    "    trans = torch.nn.Sequential(\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "    )\n",
    "    images_all = []\n",
    "    for i in range(Y_dims.shape[0]):\n",
    "        response = client.query(embedding_input=Y_dims[i].tolist())\n",
    "        images = []\n",
    "        for r in response:\n",
    "            try:\n",
    "                images.append(trans(Image.open(urlopen(r['url'], timeout=10))).convert('RGB'))\n",
    "            except UnidentifiedImageError:\n",
    "                print(\"UnidentifiedImageError\")\n",
    "            except HTTPError:\n",
    "                print(\"HTTPError\")\n",
    "            except URLError:\n",
    "                print(\"URLError\")\n",
    "            except ConnectionResetError:\n",
    "                print(\"ConnectionResetError\")\n",
    "            except:\n",
    "                print(\"timeout?\")\n",
    "\n",
    "            if len(images) >= num_images:\n",
    "                break\n",
    "\n",
    "        image = np.concatenate(images, axis=1)\n",
    "        images_all.append(image)\n",
    "    images_all = np.concatenate(images_all)\n",
    "    return images_all\n",
    "\n",
    "#image = rearrange(image, '(n1 n2) h w c -> (n1 h) (n2 w) c', n1=3)\n",
    "#out_path = nsd_path / f'derivatives/clip_retreival/{indice_name}/'\n",
    "#out_path.mkdir(exist_ok=True, parents=True)\n",
    "#file_name = f'dim_{dim:04}.png'\n",
    "#Image.fromarray(image).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72439b9e-b6d4-4dd5-9837-eb0475019fe0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e03750-77e9-41c9-a2a0-cffc965c0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hidden layer activations\n",
    "\n",
    "Y_hidden_all = []\n",
    "for subject_id in range(len(subjects)):\n",
    "    model = models_all[subject_id]\n",
    "    #for k, v in model.state_dict().items():\n",
    "    #    print(k, v.shape)\n",
    "\n",
    "    modules = dict(model.named_modules())\n",
    "    #print(modules)\n",
    "    hook_layer = modules['layers.1']\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        Y_hidden_all.append(output)\n",
    "\n",
    "    hook_handle = hook_layer.register_forward_hook(forward_hook)\n",
    "\n",
    "    X = X_all[subject_id][0]\n",
    "    with torch.no_grad():\n",
    "        model(X)\n",
    "\n",
    "    hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2defc-e1ae-412b-8ccb-d0bc78dbff86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Histogram of hidden layer activations\n",
    "\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    size = 2\n",
    "    fig, ax = plt.subplots(nrows=8, ncols=8, figsize=(8 * size, 8 * size), \n",
    "                          sharex=False, sharey=True,)\n",
    "    ax = ax.flatten()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.07)\n",
    "\n",
    "    for i in range(ax.shape[0]):\n",
    "        ax[i].hist(Y_hidden_all[subject_id][:, i])\n",
    "        #ax[i].set_ylim(0, 225)\n",
    "        #ax[i].set_xlim(-0.1, 0.7) \n",
    "        #ax[i].set_xticks([0.0, 0.2, 0.4, 0.6])\n",
    "        #if i > 0:\n",
    "            #ax[i].set_yticks([])\n",
    "        #ax[i].set_xlabel(subject)\n",
    "\n",
    "    fig.suptitle('Histogram of Hidden Layer Activations')\n",
    "    fig.supxlabel('Dimension Values')\n",
    "    fig.supylabel('Number of Stimuli')\n",
    "    file_name = f'hidden_activation_histogram_{subject}.png'\n",
    "    out_path = results_path / 'hidden_layer_activations'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    plt.savefig(out_path / file_name, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab979126-a2cd-4f6d-9ffd-fe0dc88ecd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "    for component_id in range(32):\n",
    "        y = Y_hidden_all[subject_id][:, component_id]\n",
    "        sample_points = np.linspace(y.min(), y.max(), 90)\n",
    "        sample_ids = [np.argmin(np.abs(y - s)) for s in sample_points]\n",
    "        sample_stimulus_ids = stimulus_ids_all[subject_id][sample_ids]\n",
    "        sample_images = np.array([stimulus_images[i] for i in sample_stimulus_ids])\n",
    "        sample_images = rearrange(sample_images, '(n1 n2) h w c -> (n2 h) (n1 w) c', n2=3)\n",
    "        out_path = results_path / 'hidden_layer_activation_images'\n",
    "        out_path.mkdir(exist_ok=True, parents=True)\n",
    "        file_name = f'subject-{subject}_component-{component_id}.png'\n",
    "        Image.fromarray(sample_images).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e1223-241b-4059-b718-b9e36b13d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "Y_pred_all_transformed = Y_pred_group_pca.transform()\n",
    "\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    components = models_all[0].state_dict()['layers.3.weight']\n",
    "    for component_id in range(32):\n",
    "        y = Y_hidden_all[subject_id][:, component_id]\n",
    "        sample_points = np.linspace(y.min(), y.max(), 90)\n",
    "        sample_ids = [np.argmin(np.abs(y - s)) for s in sample_points]\n",
    "        sample_stimulus_ids = np.concatenate(stimulus_ids_all)[sample_ids]\n",
    "        sample_images = np.array([stimulus_images[i] for i in sample_stimulus_ids])\n",
    "        sample_images = rearrange(sample_images, '(n1 n2) h w c -> (n2 h) (n1 w) c', n2=3)\n",
    "        out_path = results_path / 'hidden_layer_activation_images'\n",
    "        out_path.mkdir(exist_ok=True, parents=True)\n",
    "        file_name = f'subject-{subject}_component-{component_id}.png'\n",
    "        Image.fromarray(sample_images).save(out_path / file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239281d-bc32-4d4a-9633-816e4cc92795",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torch.nn.Sequential(\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    ")\n",
    "\n",
    "num_components = 32\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    \n",
    "    components = models_all[subject_id].state_dict()['layers.3.weight'].numpy()\n",
    "    Y_full_activations = Y_full @ components\n",
    "    \n",
    "    images_all = []\n",
    "    for i in range(num_components):\n",
    "        y = Y_full_activations[:, i]\n",
    "        stimulus_ids = np.argsort(y)[::-1]\n",
    "        images = np.concatenate([\n",
    "            trans(Image.fromarray(stimulus_images[stimulus_id]))\n",
    "            for stimulus_id in stimulus_ids[:num_images]\n",
    "        ], axis=1)\n",
    "        images_all.append(images)\n",
    "    images_all = np.concatenate(images_all)\n",
    "    \n",
    "    out_path = results_path / 'hidden_layer_activation_images_73k'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    file_name = f'subject-{subject}.png'\n",
    "    Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75897f65-5ab0-4b19-ba60-01e1e4a7983a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e9e2d-eb16-4e16-839d-be35a480bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlations in stimulus set embeddings\n",
    "from research.metrics.metrics import pearsonr\n",
    "\n",
    "r = pearsonr(torch.from_numpy(Y_full[:, None]), torch.from_numpy(Y_full[:, :, None]), reduction=None).numpy()\n",
    "\n",
    "r_ids = np.stack(np.meshgrid(*[np.arange(s) for s in r.shape]))\n",
    "triu_ids = np.triu_indices_from(r, k=1)\n",
    "\n",
    "r_ids_triu = r_ids[:, triu_ids[0], triu_ids[1]]\n",
    "r_triu = r[triu_ids[0], triu_ids[1]]\n",
    "\n",
    "r_argsort_ids = np.argsort(np.abs(r_triu))[::-1]\n",
    "print('dimensions, r')\n",
    "for dim_r, dims in list(zip(r_triu[r_argsort_ids], r_ids_triu[:, r_argsort_ids].T))[:50]:\n",
    "    print(tuple(dims), round(dim_r, 7))\n",
    "    \n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "fig.tight_layout()\n",
    "plt.imshow(r, vmin=-1, vmax=1, cmap='bwr')\n",
    "\n",
    "#file_name = 'pca_component_rsm_full_vs_group.png'\n",
    "#plt.savefig(results_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e2eb3-cb78-47be-9677-c14c557e24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "img = Image.new('RGB', (256, 256))\n",
    "draw = ImageDraw.Draw(img)\n",
    "# font = ImageFont.truetype(<font-file>, <font-size>)\n",
    "font = ImageFont.truetype('arial.ttf', 32)\n",
    "# draw.text((x, y),\"Sample Text\",(r,g,b))\n",
    "draw.multiline_text((128, 128),\"Sample Text\\nLine2\", anchor='mm', font=font, align='center')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ee422-8036-46ba-baf2-f543d6812082",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\n",
    "    'trees', 'dog', 'cat', 'crowded', 'foggy', 'sunny', 'nighttime', 'sad', 'happy', 'angry', 'pets',\n",
    "    'animals', 'children', 'event', 'activity'\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    Y_dims = full_model.encode_text(clip.tokenize(dimensions).to(device))\n",
    "Y_dims = Y_dims.cpu().numpy()\n",
    "#Y_dims[:, [133, 312, 92]] = 0.\n",
    "Y_dims = Y_dims / np.linalg.norm(Y_dims, axis=1)[:, None]\n",
    "\n",
    "images_all = top_images_73k(Y_dims, num_images=10, labels=dimensions)\n",
    "\n",
    "out_path = results_path / 'handpicked_clip_dimensions'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "file_name = f'top_images_73k.png'\n",
    "Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a67585-a1cb-4be6-ae0d-30a6db9962df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = torch.randn(5000, 512).numpy()\n",
    "r_random_all = []\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    r_random = pearsonr(\n",
    "        torch.from_numpy(Y_all[subject_id] @ W.T), \n",
    "        torch.from_numpy(Y_pred_all[subject_id] @ W.T), \n",
    "    reduction=None)\n",
    "    print(r_random.mean(), r_random.std())\n",
    "    r_random_all.append(r_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b88c29-88bf-4df5-a9f4-c8aff29f8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_random_all[0].sort().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bee023-7406-4b51-99de-ba6f59e57904",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r_random_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5e760-e78f-4804-8029-2f3d9431ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id, subject in enumerate(subjects):\n",
    "    Y, Y_pred = Y_all[subject_id], Y_pred_all[subject_id]\n",
    "    Y_dims_proj = Y @ Y_dims.T\n",
    "    Y_pred_dims_proj = Y_pred @ Y_dims.T\n",
    "    Y_dims_proj_r = pearsonr(\n",
    "        torch.from_numpy(Y_dims_proj), torch.from_numpy(Y_pred_dims_proj), \n",
    "    reduction=None)\n",
    "    print(Y_dims_proj_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fec4b-19fa-498d-9a44-c8ad2113ed16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list((Path(os.environ['WINDIR']) / 'fonts').iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b17ffc-66a3-40db-8721-5754f87becda",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Path(os.environ['WINDIR']) / 'fonts/DejaVuSans.ttf').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460850d-15a9-42cd-84e2-a1ba7cf7b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "img = Image.new('RGB', (128, 128))\n",
    "draw = ImageDraw.Draw(img)\n",
    "# font = ImageFont.truetype(<font-file>, <font-size>)\n",
    "#font = ImageFont.truetype('DejaVuSans.ttf', 16)\n",
    "# draw.text((x, y),\"Sample Text\",(r,g,b))\n",
    "draw.text((0, 0),\"Sample Text\",(255,255,255),)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f244b8-9194-48b0-84e0-2b69b4b4c82c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dimension Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b5d1c-749c-47f4-9da5-0133d9dc3047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "def brain_decodability_evaluation(\n",
    "    Y, \n",
    "    Y_pred, \n",
    "    Y_full,\n",
    "    stimulus_ids,\n",
    "    W, \n",
    "    out_path,\n",
    "    labels=None,\n",
    "    image_labels=None\n",
    "):\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    num_components = W.shape[0]\n",
    "    if labels is None:\n",
    "        labels = [str(i) for i in range(num_components)]\n",
    "    if image_labels is None:\n",
    "        image_labels = labels\n",
    "    \n",
    "    W_triu_indices = torch.triu_indices(num_components, num_components, offset=1)\n",
    "    W_rsm = (W @ W.T).cpu().numpy()\n",
    "    W_rsm_triu = W_rsm[W_triu_indices[0], W_triu_indices[1]]\n",
    "    \n",
    "    r2_scores = r2_score(F.relu(Y @ W.T), F.relu(Y_pred @ W.T), reduction=None).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(num_components * 0.2, 3))\n",
    "    plt.title('Brain-Decoding Variance Explained of CLIP Dimensions')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('R^2')\n",
    "    plt.bar(np.arange(num_components), r2_scores)\n",
    "    if labels is not None:\n",
    "        plt.xticks(rotation=45, ha='right', labels=labels, ticks=np.arange(num_components))\n",
    "    file_name = 'variance_explained.png'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    \n",
    "    r_scores = pearsonr(F.relu(Y @ W.T), F.relu(Y_pred @ W.T), reduction=None).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(num_components * 0.2, 3))\n",
    "    plt.title('Brain-Decoding Pearson Correlation of CLIP Dimensions')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('pearsonr')\n",
    "    plt.bar(np.arange(num_components), r_scores)\n",
    "    if labels is not None:\n",
    "        plt.xticks(rotation=45, ha='right', labels=labels, ticks=np.arange(num_components))\n",
    "    file_name = 'pearsonr.png'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(num_components * 0.2,) * 2) \n",
    "    plt.imshow(W_rsm, cmap='bwr', vmin=-1, vmax=1)\n",
    "    if labels is not None:\n",
    "        plt.xticks(rotation=45, ha='right', labels=labels, ticks=np.arange(num_components))\n",
    "        plt.yticks(labels=labels, ticks=np.arange(num_components))\n",
    "    file_name = 'cosine_similarity_rsm.png'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    \n",
    "    plt.figure() \n",
    "    plt.hist(W_rsm[W_triu_indices[0], W_triu_indices[1]])\n",
    "    file_name = 'cosine_similairty_histogram.png'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight', ticks=np.arange(num_components))\n",
    "    # plt.show()\n",
    "\n",
    "    Y_full_transformed = Y_full @ W.cpu().numpy().T\n",
    "\n",
    "    size = 2\n",
    "    num_cols = 8\n",
    "    num_rows = int(num_components / num_cols)\n",
    "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols * size, num_rows * size), \n",
    "                          sharex=False, sharey=True,)\n",
    "    ax = ax.flatten()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.95, left=0.07)\n",
    "\n",
    "    for i in range(ax.shape[0]):\n",
    "        y = Y_full_transformed[:, i].copy()\n",
    "        ax[i].hist(y[y > 0])\n",
    "\n",
    "    fig.suptitle('Histogram of 73k Stimulus Set Projected Onto Optimized Components')\n",
    "    fig.supxlabel('Dimension Values')\n",
    "    fig.supylabel('Number of Stimuli')\n",
    "    file_name = 'component_histogram.png'\n",
    "    plt.savefig(out_path / file_name, pad_inches=0)\n",
    "    # plt.show()\n",
    "    \n",
    "    N = 1000\n",
    "    top_k_values = [1, 5, 10, 50, 100, 500]\n",
    "    chance_accuracy = [k / N for k in top_k_values]\n",
    "\n",
    "    metric = 'euclidean'\n",
    "    \n",
    "    Y_W, Y_pred_W = F.relu(Y @ W.T), F.relu(Y_pred @ W.T)\n",
    "    \n",
    "    top_knn_accuracy = {}\n",
    "    unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "        stimulus_ids, return_index=True, return_inverse=True)\n",
    "    top_knn_accuracy = top_knn_test(\n",
    "        Y_W[unique_index].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse, k=top_k_values, metric=metric)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.xticks(ticks=range(len(top_k_values)), labels=top_k_values)\n",
    "    plt.title(f'Top knn accuracy')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(len(top_k_values)), chance_accuracy, label='chance (k/n)', color='gray')\n",
    "    plt.plot(range(len(top_k_values)), top_knn_accuracy, label='actual')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    file_name = 'top_k_accuracy.png'\n",
    "    plt.savefig(out_path / file_name, pad_inches=0)\n",
    "\n",
    "    images_all = top_images_73k(W.cpu().numpy(), num_images=10, labels=image_labels)\n",
    "    file_name = f'top_images_73k.png'\n",
    "    Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9f991-10bb-4128-8957-c7c7b7c02fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_criterion = ContrastiveDistanceLoss(squared_euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67122459-1646-4884-b5e2-160a1ac90518",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb505d2-3892-49de-b03e-92027a97efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape, Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72adb7c-03ac-467e-9ced-86e53f41ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(Y, Y_pred)\n",
    "dataloader = DataLoader(dataset, batch_size=3000)\n",
    "data_iterator = get_data_iterator(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87829a2-c965-4190-9bd0-5e82e1d58043",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_batch, Y_pred_batch = next(data_iterator)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96555edf-3a17-4715-9b9d-fffa954d7b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.cluster import hierarchy\n",
    "from research.metrics.loss_functions import ContrastiveDistanceLoss\n",
    "from research.metrics.metrics import squared_euclidean_distance\n",
    "\n",
    "optimized_dims_path = nsd_path / f'derivatives/decoded_features/{model_name}/optimzied_dims/'\n",
    "optimized_dims_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "run_name = 'run36'\n",
    "\n",
    "metric = 'euclidean'\n",
    "top_k_values = [1]\n",
    "\n",
    "contrastive_criterion = ContrastiveDistanceLoss(squared_euclidean_distance)\n",
    "\n",
    "with h5py.File(optimized_dims_path / f'{group_name}.hdf5', 'w') as f:\n",
    "    for subject_id in ['all']:\n",
    "        params = {\n",
    "            'subject_id': subject_id,\n",
    "            'lr': 0.0002, \n",
    "            'r2_weight': 0.,\n",
    "            'r_weight': 0.,\n",
    "            'contrastive_weight': 1.,\n",
    "            'cossim_weight': 0.,\n",
    "            'correlation_weight': 0.0,\n",
    "            'l1_weight': 0.,\n",
    "            'num_components': 128,\n",
    "            'num_iterations': 15000,\n",
    "            'negative_slope': 0.05\n",
    "        }\n",
    "        locals().update(params)\n",
    "        \n",
    "        if subject_id == 'all':\n",
    "            subject_name = subject_id\n",
    "            Y_val_all = torch.cat([torch.from_numpy(Y_all[subject_id]).cuda() for subject_id in range(8)])\n",
    "            Y_val_all_pred = torch.cat([torch.from_numpy(Y_pred_all[subject_id]).cuda() for subject_id in range(8)])\n",
    "\n",
    "            Y_val = torch.from_numpy(Y_all[0]).cuda()\n",
    "            Y_val_pred = torch.from_numpy(Y_pred_all[0]).cuda()\n",
    "            stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'][0])\n",
    "\n",
    "            Y_test = torch.from_numpy(folds['test']['Y_all'][0]).cuda()\n",
    "            Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][0]).cuda()\n",
    "            stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][0])\n",
    "            \n",
    "            dataset = TensorDataset(Y_val_all, Y_val_all_pred)\n",
    "            dataloader = DataLoader(dataset, batch_size=3000)\n",
    "            data_iterator = get_data_iterator(dataloader)\n",
    "            \n",
    "        else:\n",
    "            subject_name = f'subj0{subject_id + 1}'\n",
    "            Y_val = torch.from_numpy(Y_all[subject_id]).cuda()\n",
    "            Y_val_pred = torch.from_numpy(Y_pred_all[subject_id]).cuda()\n",
    "            stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'][subject_id])\n",
    "\n",
    "            Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "            Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "            stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])\n",
    "            \n",
    "            dataset = TensorDataset(Y_val, Y_val_pred)\n",
    "            dataloader = DataLoader(dataset, batch_size=3000)\n",
    "            data_iterator = get_data_iterator(dataloader)\n",
    "            \n",
    "            '''\n",
    "            train_mask, val_mask, test_mask = nsd.get_split(subject_name, 'split-01')\n",
    "            \n",
    "            subject_embeddings = embeddings[f'{subject_name}/{embedding_name}']\n",
    "            config = dict(subject_embeddings.attrs)\n",
    "            \n",
    "            betas_params = {\n",
    "                k: config[k] \n",
    "                for k in (\n",
    "                    'subject_name', 'voxel_selection_path', \n",
    "                    'voxel_selection_key', 'num_voxels', 'return_volume_indices', 'threshold'\n",
    "                )\n",
    "            }\n",
    "            if betas_params['threshold'] is not None:\n",
    "                betas_params['num_voxels'] = None\n",
    "                betas_params['return_tensor_dataset'] = False\n",
    "            print('1')\n",
    "            betas, betas_indices = nsd.load_betas(**betas_params)\n",
    "            X_train = torch.from_numpy(betas[train_mask])\n",
    "            model = models_all[subject_id]\n",
    "\n",
    "            print('2')\n",
    "            with torch.no_grad():\n",
    "                Y_train_pred = model(X_train).cuda()\n",
    "                Y_train_pred = Y_train_pred / torch.linalg.norm(Y_train_pred, axis=1)[:, None]\n",
    "            print('3')\n",
    "            \n",
    "            stimulus_params = dict(\n",
    "                subject_name=subject_name,\n",
    "                stimulus_path=f'derivatives/stimulus_embeddings/{model_name}.hdf5',\n",
    "                stimulus_key=embedding_name,\n",
    "                delay_loading=False,\n",
    "                return_tensor_dataset=False,\n",
    "                return_stimulus_ids=True,\n",
    "            )\n",
    "            stimulus, stimulus_ids = nsd.load_stimulus(**stimulus_params)\n",
    "            stimulus_ids_train = stimulus_ids[train_mask]\n",
    "            Y_train = stimulus[train_mask].astype(np.float32)\n",
    "            Y_train = Y_train.reshape(Y_train.shape[0], -1)\n",
    "            Y_train = torch.from_numpy(Y_train).cuda()\n",
    "            print('4')'''\n",
    "            \n",
    "        unique_stimulus_ids_val, unique_index_val, unique_inverse_val = np.unique(\n",
    "            stimulus_ids_val, return_index=True, return_inverse=True)\n",
    "        unique_stimulus_ids_test, unique_index_test, unique_inverse_test = np.unique(\n",
    "            stimulus_ids_test, return_index=True, return_inverse=True)\n",
    "\n",
    "        dataset = TensorDataset(Y_val, Y_val_pred)\n",
    "        dataloader = DataLoader(dataset, batch_size=3000)\n",
    "        data_iterator = get_data_iterator(dataloader)\n",
    "\n",
    "        W = torch.randn(num_components, Y_train.shape[1]).cuda()\n",
    "        W.requires_grad = True\n",
    "\n",
    "        W_triu_indices = torch.triu_indices(num_components, num_components, offset=1)\n",
    "\n",
    "        optim = torch.optim.Adam([W], lr=lr)\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            Y_batch, Y_pred_batch = next(data_iterator)\n",
    "            Y_W = F.leaky_relu(Y_batch @ W.T, negative_slope=negative_slope)\n",
    "            Y_pred_W = F.leaky_relu(Y_pred_batch @ W.T, negative_slope=negative_slope)\n",
    "            r2_loss = -r2_score(Y_W, Y_pred_W)\n",
    "            r_loss = -pearsonr(Y_W, Y_pred_W)\n",
    "            contrastive_loss = contrastive_criterion(Y_W, Y_pred_W)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    Y_val_W, Y_val_pred_W = F.relu(Y_val @ W.T), F.relu(Y_val_pred @ W.T)\n",
    "                    Y_test_W, Y_test_pred_W = F.relu(Y_test @ W.T), F.relu(Y_test_pred @ W.T)\n",
    "                    \n",
    "                    top_knn_accuracy_val = top_knn_test(\n",
    "                        Y_val_W[unique_index_val].cpu().numpy(), Y_val_pred_W.cpu().numpy(), unique_inverse_val, k=top_k_values, metric=metric)\n",
    "                    top_knn_accuracy_test = top_knn_test(\n",
    "                        Y_test_W[unique_index_test].cpu().numpy(), Y_test_pred_W.cpu().numpy(), unique_inverse_test, k=top_k_values, metric=metric)\n",
    "            \n",
    "                print(f'i={i}, r_loss={-r_loss.detach().cpu().item()}', top_knn_accuracy_val, top_knn_accuracy_test)\n",
    "            \n",
    "            l1_loss = Y_pred_W.mean()\n",
    "            \n",
    "            W_norm = W / torch.linalg.norm(W, dim=1)[:, None]\n",
    "            W_cossim = W_norm @ W_norm.T\n",
    "            cossim_loss = F.relu(W_cossim[W_triu_indices[0], W_triu_indices[1]]).mean()\n",
    "            \n",
    "            Y_pred_W_centered = (Y_pred_W - Y_pred_W.mean(axis=0))\n",
    "            Y_pred_W_norm = Y_pred_W_centered / torch.norm(Y_pred_W_centered, dim=0)\n",
    "            Y_pred_W_r = Y_pred_W_norm.T @ Y_pred_W_norm\n",
    "            correlation_loss = F.relu(Y_pred_W_r[W_triu_indices[0], W_triu_indices[1]]).mean()\n",
    "\n",
    "            loss = r2_loss * r2_weight + r_loss * r_weight + contrastive_loss * contrastive_weight + l1_loss * l1_weight + cossim_loss * cossim_weight + correlation_loss * correlation_weight\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        W.requires_grad = False\n",
    "        W = W / torch.linalg.norm(W, dim=1)[:, None]\n",
    "        \n",
    "        W_triu_indices = torch.triu_indices(num_components, num_components, offset=1)\n",
    "        W_rsm = (W @ W.T).cpu().numpy()\n",
    "        W_rsm_triu = W_rsm[W_triu_indices[0], W_triu_indices[1]]\n",
    "        y = 1 - W_rsm_triu\n",
    "        Z = hierarchy.linkage(y, optimal_ordering=True)\n",
    "        leaves = hierarchy.leaves_list(Z)\n",
    "        W = W[leaves]\n",
    "\n",
    "        out_path = results_path / 'optimized_clip_dimensions' / run_name / subject_name\n",
    "        out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        with open(out_path / 'params.json', 'w') as f:\n",
    "            f.write(json.dumps(params))\n",
    "            \n",
    "        np.save(out_path / 'W.npy', W.cpu().numpy())\n",
    "        \n",
    "        brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W, out_path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211abb5-570b-496a-9c3e-e69068683c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'run36'\n",
    "subject_name = 'all'\n",
    "\n",
    "W = np.load(results_path / 'optimized_clip_dimensions' / run_name / subject_name / \"W.npy\")\n",
    "\n",
    "Y_full.shape\n",
    "\n",
    "Y_full_W = Y_full @ W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd467ab1-2e9e-449d-8c99-f33af75e0aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for subject_id in range(8):\n",
    "    subject_name = f'subj0{subject_id + 1}'\n",
    "    out_path = results_path / 'optimized_clip_dimensions' / run_name / subject_name\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with open(out_path / 'params.json', 'w') as f:\n",
    "        f.write(json.dumps(params))\n",
    "\n",
    "    np.save(out_path / 'W.npy', W.cpu().numpy())\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])\n",
    "    \n",
    "    brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b929f-5d05-4b52-95bd-15e945e910c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K accuracy in PCA space\n",
    "\n",
    "top_k_values = [1]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "metric = 'cosine'\n",
    "fold = 'test'\n",
    "num_pca_components = 64\n",
    "\n",
    "full_top_1 = []\n",
    "W_top_1 = []\n",
    "pca64_top_1 = []\n",
    "pca128_top_1 = []\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    print(subject)\n",
    "\n",
    "    Y = folds[fold]['Y_all'][subject_id]\n",
    "    Y_pred = folds[fold]['Y_pred_all'][subject_id]\n",
    "    Y_pca_tranformed = torch.from_numpy(Y_pred_group_pca.transform(Y))\n",
    "    Y_pred_pca_transformed = torch.from_numpy(Y_pred_group_pca.transform(Y_pred))\n",
    "\n",
    "    stimulus_ids = folds[fold]['stimulus_ids_all'][subject_id]\n",
    "\n",
    "    unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "        stimulus_ids, return_index=True, return_inverse=True)\n",
    "\n",
    "    full_top_1.append(top_knn_test(\n",
    "        torch.from_numpy(Y)[unique_index],\n",
    "        torch.from_numpy(Y_pred), \n",
    "        unique_inverse, k=top_k_values, metric=metric\n",
    "    )[0])\n",
    "    W_top_1.append(top_knn_test(\n",
    "        F.relu(torch.from_numpy(Y) @ W.T)[unique_index],\n",
    "        F.relu(torch.from_numpy(Y_pred) @ W.T), \n",
    "        unique_inverse, k=top_k_values, metric=metric\n",
    "    )[0])\n",
    "    pca64_top_1.append(top_knn_test(\n",
    "        Y_pca_tranformed[unique_index][:, :64], \n",
    "        Y_pred_pca_transformed[:, :64], \n",
    "        unique_inverse, k=top_k_values, metric=metric\n",
    "    )[0])\n",
    "    pca128_top_1.append(top_knn_test(\n",
    "        Y_pca_tranformed[unique_index][:, :128], \n",
    "        Y_pred_pca_transformed[:, :128], \n",
    "        unique_inverse, k=top_k_values, metric=metric\n",
    "    )[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cef12b-48ec-4eb7-aadd-4655ce27986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.2\n",
    "plt.bar(np.arange(8) - bar_width * 1.5, full_top_1, width=bar_width, label='full CLIP space')\n",
    "plt.bar(np.arange(8) - bar_width * 0.5, W_top_1, width=bar_width, label='W-space 128 components')\n",
    "plt.bar(np.arange(8) + bar_width * 0.5, pca64_top_1, width=bar_width, label='pca 64 components')\n",
    "plt.bar(np.arange(8) + bar_width * 1.5, pca128_top_1, width=bar_width, label='pca 128 components')\n",
    "\n",
    "plt.ylabel('top 1 test accuracy')\n",
    "plt.xlabel('subject')\n",
    "plt.xticks(np.arange(8), [f'subj0{i+1}' for i in range(8)])\n",
    "plt.yticks([0.01 * i for i in range(25)])\n",
    "plt.grid(axis='y')\n",
    "\n",
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / 'all'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "plt.legend()\n",
    "file_name = 'wspace_vs_pca.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e9424-e532-419c-b0df-7edda98f55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_top_1, pca_top_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09d719-2ede-4e2b-99d2-20ffd5c5cb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724307d-b10d-4460-bfc5-4fd5034d1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, (subject_name, top_1_accuracy) in enumerate(results.items()):\n",
    "    x = np.arange(Y_full.shape[1] - 1) + 1\n",
    "    plt.plot(x, top_1_accuracy, label=subject_name)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(ticks=[i * 32 for i in range(17)])\n",
    "plt.grid(visible=True)\n",
    "plt.xlabel('num PCA components')\n",
    "plt.ylabel('top 1 accuracy')\n",
    "out_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/{fold}/{embedding_name}'\n",
    "file_name = 'pca_vs_top1.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d1fa3-1f4b-4634-8153-c25752e56aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b10af8-f9da-48b1-9e94-b52ddab2b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in range(8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688698f-61e2-4f6e-b36b-eb7ff21075c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'run32'\n",
    "subject_name = 'subj01'\n",
    "\n",
    "W = np.load(results_path / 'optimized_clip_dimensions' / run_name / subject_name / \"W.npy\")\n",
    "\n",
    "Y_full.shape\n",
    "\n",
    "Y_full_W = Y_full @ W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f83b94-a738-4b8a-9666-05caa783f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995e1a6-68b5-4b2e-8b9f-c84638841069",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full[top_k_ids[4]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ffdbd8-377f-4eca-993d-23732d6bf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc59fea-7798-4c47-ac7e-9a2ad8cf8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(Y_full_W[:, 25])[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e48f6-ecca-4efc-b056-aeb54aa8f25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_images_tsne(Y, Y_W, stimulus_ids, out_path, k=250, labels=None):\n",
    "\n",
    "    for dim in range(Y_W.shape[1]):\n",
    "        y = Y_W[:, dim]\n",
    "        top_stimulus_ids = np.argsort(y)[::-1][:k]\n",
    "        Y_top = Y[top_stimulus_ids]\n",
    "        with DisablePrints():\n",
    "            tsne = TSNE(\n",
    "                n_components=2, \n",
    "                metric='cosine', \n",
    "                init=\"pca\", \n",
    "                #learning_rate=\"auto\", \n",
    "                random_state=0,\n",
    "                verbose=0,\n",
    "            )\n",
    "            tsne.fit_transform(Y_top)\n",
    "        y = tsne.embedding_\n",
    "\n",
    "        image_size = 128\n",
    "        num_images = 40\n",
    "        S = image_size * num_images\n",
    "        full_image = np.zeros(shape=(S, S, 3), dtype=np.ubyte)\n",
    "\n",
    "        extent = 20\n",
    "        coords = np.linspace(-extent, extent, num_images)\n",
    "        grid = np.stack(np.meshgrid(coords, coords))\n",
    "        grid = rearrange(grid, 'd h w -> (h w) d')\n",
    "        grid.shape\n",
    "\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        neighbors = NearestNeighbors(metric='chebyshev')\n",
    "        neighbors.fit(y)\n",
    "\n",
    "        distances, ids = neighbors.kneighbors(grid, n_neighbors=1,)\n",
    "        distances = rearrange(distances, '(h w) d -> h w d', h=num_images)\n",
    "        ids = rearrange(ids, '(h w) d -> h w d', h=num_images)\n",
    "\n",
    "        neighbors = NearestNeighbors(metric=metric)\n",
    "\n",
    "        distance_threshold = extent / num_images\n",
    "        for i in tqdm(range(num_images)):\n",
    "            for j in range(num_images):\n",
    "                if distances[i, j] > distance_threshold:\n",
    "                    continue\n",
    "                stimulus_id = stimulus_ids[top_stimulus_ids[ids[i, j, 0]]]\n",
    "                stim_image = stimulus_images[stimulus_id]\n",
    "                stim_image = Image.fromarray(stim_image)\n",
    "                stim_image = stim_image.resize(size=(image_size, image_size), resample=PIL.Image.LANCZOS)\n",
    "                stim_image = np.array(stim_image)\n",
    "                full_image[i * image_size:(i + 1) * image_size, j * image_size:(j + 1) * image_size] = stim_image\n",
    "\n",
    "        out_path.mkdir(exist_ok=True, parents=True)\n",
    "        if labels is None:\n",
    "            Image.fromarray(full_image).save(out_path / f'dim-{dim}.png')\n",
    "        else:\n",
    "            Image.fromarray(full_image).save(out_path / f'dim-{dim}__label-{labels[dim]}.png')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dda3b0-afb5-462a-8b68-c12ef352d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_W = Y_full @ W.cpu().numpy().T\n",
    "Y_W = torch.from_numpy(Y_W)\n",
    "top_images_tsne(Y_full,\n",
    "                Y_W.numpy(), \n",
    "                np.arange(73000), \n",
    "                out_path=results_path / 'optimized_clip_dimensions' / run_name / 'all' / 'tsne_73k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d06f0c-e566-4204-ae8d-c4484af069a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_W.shape, Y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ad722-7d7b-4aed-8dfe-bb3d38be9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301c59f-5c44-4dc5-b093-8f517edde3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_W.sum(dim=1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4f84b-824e-457a-90dc-029f06d62b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563a7b1-54f5-4f23-b187-af3fdd2d72bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f8305-25f2-45d5-a1ff-861857089b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / subject_id\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with open(out_path / 'params.json', 'w') as f:\n",
    "    f.write(json.dumps(params))\n",
    "\n",
    "np.save(out_path / 'W.npy', W.cpu().numpy())\n",
    "\n",
    "brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38922ce5-4be2-484c-9e80-9e0ed3c21a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "num_components = 128\n",
    "size = 2\n",
    "num_cols = 8\n",
    "num_rows = int(num_components / num_cols)\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols * size, num_rows * size), \n",
    "                      sharex=False, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.925,)\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    #if labels is not None:\n",
    "    #    ax[i].set_title(labels[i])\n",
    "    ax[i].hist(Y_full_transformed[:, i])\n",
    "\n",
    "fig.suptitle('Histogram of 73k Stimulus Set Projected Onto Components')\n",
    "fig.supxlabel('Dimension Values')\n",
    "fig.supylabel('Number of Stimuli')\n",
    "file_name = 'component_histogram.png'\n",
    "plt.savefig(out_path / file_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e921ea6-e652-45fc-aad6-37452301384b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = 'run26'\n",
    "subject_id = 0\n",
    "\n",
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / f'subj0{subject_id+1}'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "W = torch.from_numpy(np.load(out_path / 'W.npy')).cuda()\n",
    "\n",
    "Y_val = torch.from_numpy(folds['val']['Y_all'][subject_id]).cuda()\n",
    "Y_val_pred = torch.from_numpy(folds['val']['Y_pred_all'][subject_id]).cuda()\n",
    "stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'][subject_id])\n",
    "\n",
    "Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])\n",
    "\n",
    "#brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caf38d-c2b0-48d6-87fd-8f0a72cfbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(10)[:0], torch.arange(10)[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37466d1-b9f1-42ee-a1e6-f46f80ef7b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "top_k_values = [1, 5, 10, 50, 100, 500]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "\n",
    "metric = 'cosine'\n",
    "\n",
    "unique_stimulus_ids_val, unique_index_val, unique_inverse_val = np.unique(\n",
    "            stimulus_ids_val, return_index=True, return_inverse=True)\n",
    "unique_stimulus_ids_test, unique_index_test, unique_inverse_test = np.unique(\n",
    "            stimulus_ids_test, return_index=True, return_inverse=True)\n",
    "\n",
    "Y_W, Y_pred_W = F.relu(Y_val @ W.T), F.relu(Y_val_pred @ W.T)\n",
    "top_knn_accuracy = top_knn_test(\n",
    "    Y_W[unique_index_val].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse_val, k=top_k_values, metric=metric)\n",
    "print('baseline', top_knn_accuracy)\n",
    "\n",
    "W_pruned = W\n",
    "\n",
    "top_knn_accuracies_val = []\n",
    "top_knn_accuracies_test = []\n",
    "pruned_dims = []\n",
    "\n",
    "for i in range(W.shape[0] - 1):\n",
    "    \n",
    "    pruned_j = -1\n",
    "    top_knn_accuracy_best = [0]\n",
    "    W_best = None\n",
    "    for j in range(W_pruned.shape[0]):\n",
    "        ids = torch.arange(W_pruned.shape[0])\n",
    "        ids = torch.cat([ids[:j], ids[(j + 1):]])\n",
    "        \n",
    "        Y_W, Y_pred_W = F.relu(Y_val @ W_pruned[ids].T), F.relu(Y_val_pred @ W_pruned[ids].T)\n",
    "\n",
    "        top_knn_accuracy = top_knn_test(\n",
    "            Y_W[unique_index_val].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse_val, k=top_k_values, metric=metric)\n",
    "        if top_knn_accuracy[0] > top_knn_accuracy_best[0]:\n",
    "            top_knn_accuracy_best = top_knn_accuracy\n",
    "            pruned_j = j\n",
    "            W_best = W_pruned[ids]\n",
    "    \n",
    "    top_knn_accuracies_val.append(top_knn_accuracy_best)\n",
    "    W_pruned = W_best\n",
    "    pruned_dims.append(pruned_j)\n",
    "    \n",
    "    Y_W, Y_pred_W = F.relu(Y_test @ W_pruned.T), F.relu(Y_test_pred @ W_pruned.T)\n",
    "    top_knn_accuracy_test = top_knn_test(\n",
    "        Y_W[unique_index_test].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse_test, k=top_k_values, metric=metric)\n",
    "    top_knn_accuracies_test.append(top_knn_accuracy_test)\n",
    "    \n",
    "    print(f'{pruned_j=}, {top_knn_accuracy_best=}, {top_knn_accuracy_test=}')\n",
    "\n",
    "top_knn_accuracies_test = np.array(top_knn_accuracies_test)\n",
    "top_knn_accuracies_val = np.array(top_knn_accuracies_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4865e-e0b1-4cfc-a320-d6e3a574c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "#plt.xticks(ticks=range(len(top_k_values)), labels=top_k_values)\n",
    "plt.title(f'Top 1 Accuracy After Pruning Dimensions')\n",
    "plt.xlabel('Number of Pruned Dimensions')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(range(top_knn_accuracies_test[:, 0].shape[0]), top_knn_accuracies_test[:, 0] * 100, label='test data')\n",
    "plt.plot(range(top_knn_accuracies_val[:, 0].shape[0]), top_knn_accuracies_val[:, 0] * 100, label='validation data')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "file_name = 'top_1_accuracy_pruned.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2deee-5f9d-4cce-8d8b-4056b1899a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.arange(128)\n",
    "corrected_pruned_dims = []\n",
    "for dim in pruned_dims:\n",
    "    corrected_pruned_dims.append(dims[dim])\n",
    "    dims = np.concatenate([dims[:dim], dims[(dim + 1):]])\n",
    "    \n",
    "np.array(corrected_pruned_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa85197-26e4-4f5d-ac5b-9ea88911debc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W[np.array(corrected_pruned_dims)], out_path / 'prune_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506180ca-0be1-4346-b08a-541435770118",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_knn_accuracies_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e88f3-1240-41dd-8b77-be74d11ec395",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_knn_accuracies_test[, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bb66c-03fc-47f6-a77b-54a50c2bdeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "top_k_values = [1, 5, 10, 50, 100, 500]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "\n",
    "metric = 'cosine'\n",
    "    \n",
    "Y_W, Y_pred_W = F.relu(Y_test @ W.T), F.relu(Y_test_pred @ W.T)\n",
    "\n",
    "unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "    stimulus_ids_test, return_index=True, return_inverse=True)\n",
    "D = W.shape[0]\n",
    "top_knn_accuracies = []\n",
    "for i in range(D):\n",
    "    y_W, y_pred_W = Y_W[:, i:i+1], Y_pred_W[:, i:i+1]\n",
    "    top_knn_accuracy = top_knn_test(\n",
    "        y_W[unique_index].cpu().numpy(), y_pred_W.cpu().numpy(), unique_inverse, k=top_k_values, metric=metric)\n",
    "    top_knn_accuracies.append(top_knn_accuracy)\n",
    "top_knn_accuracies = np.array(top_knn_accuracies)\n",
    "\n",
    "top_knn_accuracy = top_knn_test(\n",
    "    Y_W[unique_index].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse, k=top_k_values, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e75bd0-abb8-4772-adcd-20c2f9d9edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bccb6d-5e50-49e9-8aef-882cce8218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_knn_accuracies[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022d343-ec13-4f78-aad4-679f41ae3cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Y_full_transformed = Y_full @ W.cpu().numpy().T\n",
    "\n",
    "size = 2\n",
    "num_cols = 8\n",
    "num_rows = int(num_components / num_cols)\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols * size, num_rows * size), \n",
    "                      sharex=False, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.1, top=0.95, left=0.07)\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    y = Y_full_transformed[:, i].copy()\n",
    "    ax[i].hist(y[y > 0])\n",
    "\n",
    "fig.suptitle('Histogram of 73k Stimulus Set Projected Onto Optimized Components')\n",
    "fig.supxlabel('Dimension Values')\n",
    "fig.supylabel('Number of Stimuli')\n",
    "file_name = 'component_histogram.png'\n",
    "plt.savefig(out_path / file_name, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bbd54-a73a-4ab4-af90-7201e4727d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Y_pred_W_centered = (Y_pred_W - Y_pred_W.mean(axis=0))\n",
    "    Y_pred_W_norm = Y_pred_W_centered / torch.norm(Y_pred_W_centered, dim=0)\n",
    "    \n",
    "    Y_correlation = Y_pred_W_norm.T @ Y_pred_W_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad42e1-98f1-4f14-909c-de386a9a43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "recon_image = reconstruct(\n",
    "    stimulus_embeddings={'embedding': W[0:1].cuda().repeat(5, 1)},\n",
    "    hook_modules={'': 'embedding'},\n",
    "    model=vqgan_model,\n",
    "    vqgan_checkpoint=vqgan_checkpoint,\n",
    "    perceptor=full_model.visual.eval().requires_grad_(False).to(device),\n",
    "    device=torch.device('cuda'),\n",
    "    max_iterations=50,\n",
    "    embedding_iterations=50,\n",
    ")\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60516b-6f4a-4cd8-a48c-614bd203a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(i=(0, len(recon_image)-1))\n",
    "def show(i):\n",
    "    plt.imshow(recon_image[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a8047-603c-4ed8-859e-364c410ea600",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8fb05-f4da-4b54-a08b-438389a60e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_all = top_images_73k(W.cpu().numpy(), num_images=10)\n",
    "file_name = f'top_images_73k.png'\n",
    "Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85ac2e-6cef-4d81-9799-bc6ad148445f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_all = top_images_clip_retreival(W.cpu().numpy(), num_images=10)\n",
    "file_name = f'top_images_clip_retreival.png'\n",
    "Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63578bd-572d-48d8-8ede-60955560bc45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effc418-1f13-42ec-91c0-0eac69251bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "things_concepts = [\n",
    "    'made of metal, artificial, hard', \n",
    "    'food-related, eating-related, kitchen-related',\n",
    "    'animal-related, organic',\n",
    "    'clothing-related, fabric, covering',\n",
    "    'furniture-related, household-related, artifact',\n",
    "    'plant-related, green',\n",
    "    'outdoors-related',\n",
    "    'transportation, motorized, dynamic',\n",
    "    'wood-related, brownish',\n",
    "    'body part-related',\n",
    "    'colorful',\n",
    "    'valuable, special occasion-related',\n",
    "    'electronic, technology',\n",
    "    'sport-related, recreational activity-related',\n",
    "    'disc-shaped, round',\n",
    "    'tool-related',\n",
    "    'many small things, course pattern',\n",
    "    'paper-related, thin, flat, text-related',\n",
    "    'fluid-related, drink-related',\n",
    "    'long, thin',\n",
    "    'water-related, blue',\n",
    "    'powdery, fine-scale pattern',\n",
    "    'red',\n",
    "    'feminine (stereotypically), decorative',\n",
    "    'bathroom-related, sanitary',\n",
    "    'black, noble',\n",
    "    'weapon, danger-related, violence',\n",
    "    'musical instrument-related, noise-related',\n",
    "    'sky-related, flying-related, floating-related',\n",
    "    'spherical, ellipsoid, rounded, voluminous',\n",
    "    'repetitive',\n",
    "    'flat, patterned',\n",
    "    'white',\n",
    "    'thin, flat',\n",
    "    'disgusting, bugs',\n",
    "    'string-related',\n",
    "    'arms/legs/skin-related',\n",
    "    'shiny, transparent',\n",
    "    'construction-related, physical work-related',\n",
    "    'fire-related, heat-related',\n",
    "    'head-related, face-related',\n",
    "    'beams-related',\n",
    "    'seating-related, put things on top',\n",
    "    'container-related, hollow',\n",
    "    'child-related, toy-related',\n",
    "    'medicine-related',\n",
    "    'has grating',\n",
    "    'handicraft-related',\n",
    "    'cylindrical, conical'\n",
    "]\n",
    "\n",
    "things_concepts2 = [c.replace('-related', '') for c in things_concepts]\n",
    "things_concepts3 = [c.split(',')[0] for c in things_concepts2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159e54b-2282-4ae6-9bf0-e059a1fe3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.tokenize(things_concepts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae12b6f-ce95-493d-98ac-e42d259e0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    W_things_clip = full_model.encode_text(clip.tokenize(things_concepts2).to(device))\n",
    "W_things_clip = W_things_clip.cpu().numpy()\n",
    "#Y_dims[:, [133, 312, 92]] = 0.\n",
    "W_things_clip = W_things_clip / np.linalg.norm(W_things_clip, axis=1)[:, None]\n",
    "W_things_clip = torch.from_numpy(W_things_clip).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4c7e8-e102-430d-8f2c-a8e6e6b56c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W_things_clip\n",
    "\n",
    "for subject_id in range(8):\n",
    "    subject_name = f'subj0{subject_id + 1}'\n",
    "    \n",
    "    Y_val = torch.from_numpy(folds['val']['Y_all'][subject_id]).cuda()\n",
    "    Y_val_pred = torch.from_numpy(folds['val']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'][subject_id])\n",
    "\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])\n",
    "    \n",
    "    Y_val_W = F.relu(Y_val @ W.T)\n",
    "    Y_val_pred_W = F.relu(Y_val_pred @ W.T)\n",
    "    \n",
    "    r = pearsonr(Y_val_W, Y_val_pred_W, reduction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdebe0-7e54-4df8-b5a9-1a1fe42f93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492063fe-8b1f-46c9-bc66-b60c07c824f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15edc3-23f5-47d2-b220-521608e6bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for corr, concept in zip(r, things_concepts):\n",
    "    print(f'{concept} - {corr.cpu().item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13736bb-4504-442f-bde0-d1ca11647e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    W_things = full_model.encode_text(clip.tokenize(things_concepts).to(device))\n",
    "W_things = W_things.cpu().numpy()\n",
    "#Y_dims[:, [133, 312, 92]] = 0.\n",
    "W_things = W_things / np.linalg.norm(W_things, axis=1)[:, None]\n",
    "\n",
    "images_all = top_images_73k(W_things, num_images=10)\n",
    "\n",
    "out_path = results_path / 'things_dimensions'\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "file_name = f'top_images_73k_v1.png'\n",
    "Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668fc28-5ef9-4e8d-a240-ebec03a0f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_things_torch = np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3454183-8fa6-422e-af43-3fea3a5077d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "W_things_ridge = np.load(nsd_path / 'derivatives/things/ridge_weights.npy').astype(np.float32)\n",
    "W_things_ridge = W_things_ridge / np.linalg.norm(W_things_ridge, axis=1)[:, None]\n",
    "W_things_torch = np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)\n",
    "W_things_torch = W_things_torch / np.linalg.norm(W_things_torch, axis=1)[:, None]\n",
    "\n",
    "image_labels = [c.replace(',', '\\n') for c in things_concepts]\n",
    "fold = 'val'\n",
    "\n",
    "for fold in ('val', 'test'):\n",
    "    for subject_id in range(8):\n",
    "        out_path = results_path / 'things'\n",
    "\n",
    "        Y = torch.from_numpy(folds[fold]['Y_all'][subject_id]).cuda()\n",
    "        Y_pred = torch.from_numpy(folds[fold]['Y_pred_all'][subject_id]).cuda()\n",
    "        stimulus_ids = torch.from_numpy(folds[fold]['stimulus_ids_all'][subject_id])\n",
    "        \n",
    "        top_images_tsne_73k(W_things_torch, out_path / f'torch_weights/subj0{subject_id + 1}/{fold}/tsne', k=250)\n",
    "        break\n",
    "\n",
    "        brain_decodability_evaluation(Y, Y_pred, Y_full, stimulus_ids,  W_things_clip, \n",
    "                                      out_path / f'clip_weights/subj0{subject_id + 1}/{fold}',\n",
    "                                      things_concepts3, image_labels)\n",
    "        brain_decodability_evaluation(Y, Y_pred, Y_full, stimulus_ids,  torch.from_numpy(W_things_ridge).cuda(), \n",
    "                                      out_path / f'ridge_weights/subj0{subject_id + 1}/{fold}',\n",
    "                                      things_concepts3, image_labels)\n",
    "        brain_decodability_evaluation(Y, Y_pred, Y_full, stimulus_ids,  torch.from_numpy(W_things_torch).cuda(), \n",
    "                                      out_path / f'torch_weights/subj0{subject_id + 1}/{fold}',\n",
    "                                      things_concepts3, image_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4cfab-6934-47fe-9282-20e95384af76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a6af61-8230-4158-a46c-4a3268674476",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Mask Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b33fda-e5ad-4808-835b-4a7d452d704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec69116-fb6b-4d5e-9ca5-62a5ec7f1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74a02d-1f0c-4270-a704-91df439a263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_all:\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef6617-1849-4dad-8cce-efe0bf672cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dicts = [model.state_dict() for model in models_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27855713-72cb-4c4b-b2dd-f6499771ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatmap(subject_id, m, component_id, out_path, mask_value=-1, scatter_params=None):\n",
    "    if scatter_params is None:\n",
    "        scatter_params = {'vmax': 1, 'vmin': 0, 'cmap': 'gray'}\n",
    "    volume = nsd.reconstruct_volume(\n",
    "            subject_id, \n",
    "            m, \n",
    "            indices_all[subject_id],\n",
    "            mask_value\n",
    "        ).T.numpy()\n",
    "    D = volume.shape[2]\n",
    "    \n",
    "    fsavg_data = nsd.to_fs_average_space(subject_id, volume, interp_type='nearest')\n",
    "    nsd.flat_scatter_plot(fsavg_data['lh'], fsavg_data['rh'], mask_value=mask_value, **scatter_params)\n",
    "    plt.savefig(out_path, bbox_inches='tight')\n",
    "    \n",
    "def compare_pearsonr(r_original, r_masked, labels, out_file_path):\n",
    "    num_components = r_original.shape[0]\n",
    "    width = 0.45\n",
    "    plt.figure(figsize=(0.25 * num_components, 3))\n",
    "    plt.title(\"Pearson Correlation Comparison After Mask Optimization\")\n",
    "    plt.xlabel(\"Things Dimension\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.bar(np.arange(num_components), r_original, \n",
    "            tick_label=labels, \n",
    "            width=width,\n",
    "            label='Original',)\n",
    "    plt.bar(np.arange(num_components) + width, r_masked, width=width, label='Masked')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.savefig(out_file_path, bbox_inches='tight')\n",
    "\n",
    "\n",
    "def pearsonr_retained(r_original, r_masked, labels, out_file_path):\n",
    "    num_components = r_original.shape[0]\n",
    "    plt.figure(figsize=(num_components*0.25,) * 2)\n",
    "    plt.imshow(r_masked / r_original)\n",
    "    plt.yticks(ticks=np.arange(num_components), labels=labels)\n",
    "    plt.xticks(rotation=45, ha='right', ticks=np.arange(num_components), labels=labels)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('concept')\n",
    "    plt.xlabel('mask')\n",
    "    plt.title(\"Fraction of pearson correlation retained after masking\")\n",
    "    plt.savefig(out_file_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef189b78-a0a5-458d-a615-ab3e4383a964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "\n",
    "\n",
    "def concept_linear_model(W, run_name, subject_id, labels, short_labels=None):\n",
    "    if short_labels is None:\n",
    "        short_labels = labels\n",
    "    run_path = results_path / f'concept_linear_models/torch_weights/{run_name}/subj0{subject_id + 1}'\n",
    "    run_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    flatmap_path = results_path / f'concept_linear_models/torch_weights/{run_name}/flatmaps'\n",
    "    flatmap_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    X = X_all[subject_id]\n",
    "    Y = Y_all[subject_id]\n",
    "    Y_pred = Y_pred_all[subject_id]\n",
    "\n",
    "    X_test = folds['test']['X_all'][subject_id]\n",
    "    Y_test = folds['test']['Y_all'][subject_id]\n",
    "    Y_pred_test = folds['test']['Y_pred_all'][subject_id]\n",
    "    \n",
    "    Y_test_W = torch.from_numpy(Y_test @ W.T)\n",
    "    r_original = pearsonr(F.relu(Y_test_W).cpu(), F.relu(torch.from_numpy(Y_pred_test @ W.T)).cpu(), reduction=None).numpy()\n",
    "\n",
    "    M = []\n",
    "    logs_all = []\n",
    "    r_all = []\n",
    "    for component_id, w in enumerate(W):\n",
    "        Y_w = Y @ w\n",
    "        Y_test_w = Y_test @ w\n",
    "        Y_w[Y_w < 0] = 0.\n",
    "        Y_test_w[Y_test_w < 0] = 0.\n",
    "        \n",
    "        model = LassoCV()\n",
    "        model.fit(X, Y_w)\n",
    "        \n",
    "        Y_test_w_pred = model.predict(X_test)\n",
    "        M.append(model.coef_)\n",
    "        \n",
    "        r = pearsonr(torch.from_numpy(Y_test_w), torch.from_numpy(Y_test_w_pred))\n",
    "        r_all.append(r)\n",
    "        \n",
    "        file_name = f'subj0{subject_id + 1}__component-{component_id}'\n",
    "        scatter_params = dict(\n",
    "            vmax=0.003, \n",
    "            vmin=-0.003, \n",
    "            cmap='RdBu', \n",
    "            mask_color='gray', \n",
    "            bottomleft_text=f'{component_id=}\\nconcept={labels[component_id]}'\n",
    "        )\n",
    "        plot_flatmap(subject_id, \n",
    "                     torch.from_numpy(model.coef_), \n",
    "                     component_id, \n",
    "                     flatmap_path / file_name, scatter_params=scatter_params)\n",
    "                        \n",
    "    r_all = np.stack(r_all)\n",
    "    compare_pearsonr(r_original, r_all, short_labels, run_path / 'pearsonr_change.png')\n",
    "    M = np.stack(M)\n",
    "    np.save(run_path / 'mask.npy', M)\n",
    "    \n",
    "        \n",
    "W_things = np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)\n",
    "for subject_id in range(8):\n",
    "    num_components = 5\n",
    "    concept_linear_model(\n",
    "        W=W_things[:num_components],\n",
    "        run_name='run-3',\n",
    "        subject_id=subject_id,\n",
    "        labels=things_concepts[:num_components],\n",
    "        short_labels=things_concepts3[:num_components]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750adcfe-dc9c-40a4-bd27-b283856a9962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def mask_optimization(W, run_name, subject_id, lr, num_iterations, mask_sparse_alpha, mask_loss_weight, freeze_model=True, mask_init=None):\n",
    "    if mask_init is None:\n",
    "        mask_init = 1 - 0.5 ** (1. / mask_sparse_alpha)\n",
    "    params = locals().copy()\n",
    "    del params['W']\n",
    "    print(params)\n",
    "\n",
    "    run_path = results_path / f'mask_optimization/torch_weights/{run_name}/subj0{subject_id + 1}'\n",
    "    run_path.mkdir(exist_ok=True, parents=True)\n",
    "    flatmap_path = results_path / f'mask_optimization/torch_weights/{run_name}/flatmaps'\n",
    "    flatmap_path.mkdir(exist_ok=True, parents=True)\n",
    "    with open(run_path / 'params.json', 'w') as f:\n",
    "        f.write(json.dumps(params))\n",
    "\n",
    "    model = models_all[subject_id]\n",
    "    model.normalize = True\n",
    "    model.eval()\n",
    "\n",
    "    X = torch.from_numpy(X_all[subject_id]).cuda()\n",
    "    Y = torch.from_numpy(Y_all[subject_id]).cuda()\n",
    "    Y_pred = torch.from_numpy(Y_pred_all[subject_id])\n",
    "\n",
    "    X_test = torch.from_numpy(folds['test']['X_all'][subject_id]).cuda()\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_pred_test = torch.from_numpy(folds['test']['Y_pred_all'][subject_id])\n",
    "\n",
    "    M = []\n",
    "    logs_all = []\n",
    "    for component_id, w in enumerate(W):\n",
    "        logs = []\n",
    "        print(f'{component_id=}')\n",
    "        \n",
    "        model.load_state_dict({\n",
    "            k: v.clone() \n",
    "            for k, v in state_dicts_all[subject_id].items()\n",
    "        })\n",
    "        model.cuda()\n",
    "        \n",
    "        w = w.cuda()\n",
    "        Y_w = Y @ w\n",
    "        Y_test_w = Y_test @ w\n",
    "\n",
    "        m = torch.full((X.shape[1],), mask_init).cuda()\n",
    "        m.requires_grad = True\n",
    "        \n",
    "        if freeze_model:\n",
    "            optim = torch.optim.Adam([m], lr=lr)\n",
    "        else:\n",
    "            optim = torch.optim.Adam([\n",
    "                {'params': [m], 'lr': lr}, \n",
    "                {'params': model.parameters(), 'lr': 1e-4},\n",
    "            ])\n",
    "            \n",
    "        \n",
    "        m_best = None\n",
    "        r_best = 0\n",
    "        i_best = 0\n",
    "        for i in range(num_iterations):\n",
    "            mask_exp = (1 - (1 - m) ** mask_sparse_alpha)\n",
    "            Y_pred_w = model(X * m) @ w\n",
    "\n",
    "            #r2_loss = -r2_score(F.relu(Y_w), F.relu(Y_pred_w))\n",
    "            r_loss = -pearsonr(F.relu(Y_w), F.relu(Y_pred_w))\n",
    "            mask_loss = (mask_exp).mean()\n",
    "            mask_mean = m.mean()\n",
    "            loss = r_loss + mask_loss * mask_loss_weight\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            def fmt_loss(l):\n",
    "                return l.cpu().detach().item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                m = torch.clamp(m, 0., 1., out=m)\n",
    "                \n",
    "            if i > 350:\n",
    "                if -r_loss > r_best:\n",
    "                    i_best = i\n",
    "                    r_best = -r_loss\n",
    "                    m_best = m\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                with torch.no_grad():\n",
    "                    Y_pred_test_w = model(X_test * m) @ w\n",
    "                    m_masked = m.clone()\n",
    "                    m_masked[m >= 0.5] = 1.\n",
    "                    m_masked[m < 0.5] = 0.\n",
    "                    Y_pred_test_w = model(X_test * m) @ w\n",
    "                    Y_pred_test_w_masked = model(X_test * m_masked) @ w\n",
    "                    test_r_loss = pearsonr(F.relu(Y_test_w), F.relu(Y_pred_test_w))\n",
    "                    test_r_loss_masked = pearsonr(F.relu(Y_test_w), F.relu(Y_pred_test_w_masked))\n",
    "                    iter_logs = {\n",
    "                        'iteration': i, \n",
    "                        'val_r': fmt_loss(-r_loss), \n",
    "                        'test_r': fmt_loss(test_r_loss), \n",
    "                        'test_r_masked': fmt_loss(test_r_loss_masked), \n",
    "                        'mask_mean': fmt_loss(mask_mean.detach().cpu())\n",
    "                    }\n",
    "                    print(', '.join([f'{k}={v:.3f}' for k, v in iter_logs.items()]))\n",
    "\n",
    "                    bins = (0.000, 0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 1.0)\n",
    "                    m_hist, _ = np.histogram(m.detach().cpu(), bins=bins)\n",
    "                    print('mask hist', [f'{b} {v}' for b, v in zip(bins, m_hist)])\n",
    "                    iter_logs['mask'] = m.detach().cpu()\n",
    "                    logs.append(iter_logs)\n",
    "\n",
    "        logs_all.append(logs)\n",
    "        print(f'{i_best=}, {r_best=}')\n",
    "        m = m_best\n",
    "        m.requires_grad = False\n",
    "        M.append(m)\n",
    "        if not freeze_model:\n",
    "            torch.save(model.state_dict(), run_path / f'mask__component-{component_id}.pkl')\n",
    "        np.save(run_path / f'mask__component-{component_id}.npy', m.cpu().numpy())\n",
    "        model.cpu()\n",
    "    \n",
    "        volume = nsd.reconstruct_volume(\n",
    "            subject_id, \n",
    "            m.cpu(), \n",
    "            indices_all[subject_id],\n",
    "            -1\n",
    "        ).T.numpy()\n",
    "        D = volume.shape[2]\n",
    "\n",
    "        things_concept = things_concepts[component_id]\n",
    "        text = f'{component_id=}\\n{things_concept=}'\n",
    "\n",
    "        fsavg_data = nsd.to_fs_average_space(subject_id, volume, interp_type='nearest')\n",
    "        nsd.flat_scatter_plot(fsavg_data['lh'], fsavg_data['rh'], vmax=1, vmin=0, mask_value=-1, cmap='gray', bottomleft_text=text)\n",
    "        file_name = f'subj0{subject_id + 1}__component-{component_id}'\n",
    "        plt.savefig(flatmap_path / file_name, bbox_inches='tight')\n",
    "                   \n",
    "    M = torch.stack(M).cpu()\n",
    "    np.save(run_path / 'mask.npy', M.numpy())\n",
    "\n",
    "W_things = torch.from_numpy(np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)).cuda()\n",
    "\n",
    "for subject_id in range(8):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    models_all[subject_id].load_state_dict(state_dicts_all[subject_id])\n",
    "    mask_optimization(\n",
    "        W=W_things[:20],\n",
    "        run_name='run-44',\n",
    "        subject_id=subject_id,\n",
    "        lr=0.002, \n",
    "        num_iterations=1000,\n",
    "        mask_sparse_alpha=1.,\n",
    "        mask_loss_weight=5,\n",
    "        freeze_model=True,\n",
    "        mask_init=.5,\n",
    "    )\n",
    "    models_all[subject_id].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b721b5-1ce3-4de6-8ace-86acac2f8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "np.histogram(x, bins=(0.001, 0.01, 0.1, 0.2, 0.5, 0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11016c74-04ab-4a56-ad97-7ecc3dac8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.stack(M).cpu()\n",
    "np.save(run_path / 'mask.npy', M.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc2bc9-bd99-4563-8be9-19ccea02eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.models.components_3d import GaussianSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c06e02-673a-4cf5-9390-94aceafd807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.tensor([1, 5, 10, 10, 5, 1])\n",
    "kernel = kernel[None, None, :] * kernel[None, :, None] * kernel[:, None, None]\n",
    "kernel = kernel / kernel.sum() * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aba1fb-21e9-4223-aa11-850e60c70bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def plot_flatmap(subject_id, m, ):\n",
    "    volume = nsd.reconstruct_volume(\n",
    "            subject_id, \n",
    "            torch.from_numpy(m), \n",
    "            indices_all[subject_id],\n",
    "            -1\n",
    "        ).T.numpy()\n",
    "    D = volume.shape[2]\n",
    "\n",
    "    things_concept = things_concepts[component_id]\n",
    "    text = f'{component_id=}\\n{things_concept=}'\n",
    "\n",
    "    fsavg_data = nsd.to_fs_average_space(subject_id, volume, interp_type='nearest')\n",
    "    nsd.flat_scatter_plot(fsavg_data['lh'], fsavg_data['rh'], vmax=1, vmin=0, mask_value=-1, cmap='gray', bottomleft_text=text)\n",
    "    file_name = f'subj0{subject_id + 1}__component-{component_id}'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "\n",
    "\n",
    "for subject_id in range(2):\n",
    "    run_name = 'run-44'\n",
    "\n",
    "    run_path = results_path / f'mask_optimization/torch_weights/{run_name}/subj0{subject_id + 1}'\n",
    "    run_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    M = np.load(run_path / 'mask.npy')\n",
    "\n",
    "    W = np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)\n",
    "    W = torch.from_numpy(W).cuda()\n",
    "\n",
    "    model = models_all[subject_id].cuda()\n",
    "    model.normalize = True\n",
    "    model.eval()\n",
    "\n",
    "    X = torch.from_numpy(X_all[subject_id]).cuda()\n",
    "    Y = torch.from_numpy(Y_all[subject_id]).cuda()\n",
    "    Y_pred = torch.from_numpy(Y_pred_all[subject_id])\n",
    "\n",
    "    X_test = torch.from_numpy(folds['test']['X_all'][subject_id]).cuda()\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_pred_test = torch.from_numpy(folds['test']['Y_pred_all'][subject_id])\n",
    "\n",
    "    out_path = results_path / f'mask_optimization/torch_weights/{run_name}/smooth_flatmaps'\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    kernel = torch.tensor([1, 5, 10, 10, 5, 1])\n",
    "    kernel = kernel[None, None, :] * kernel[None, :, None] * kernel[:, None, None]\n",
    "    kernel = kernel / kernel.sum() * 5\n",
    "\n",
    "    for component_id, m in enumerate(M):\n",
    "        volume = nsd.reconstruct_volume(\n",
    "            subject_id, \n",
    "            torch.from_numpy(m), \n",
    "            indices_all[subject_id],\n",
    "            -1\n",
    "        ).T.numpy()\n",
    "        volume_smooth = volume.copy()\n",
    "        volume_smooth[volume_smooth == -1] = 0\n",
    "        D = volume.shape[2]\n",
    "        \n",
    "        volume_smooth = F.conv3d(torch.from_numpy(volume_smooth[None, None]), kernel[None, None], padding='same')[0, 0].numpy()\n",
    "        \n",
    "        volume_smooth[volume_smooth >= 0.5] = 1\n",
    "        volume_smooth[volume_smooth < 0.5] = 0\n",
    "        #volume_smooth = binary_dilation(volume_smooth, iterations=1).astype(float)\n",
    "        volume_smooth[volume == -1] = -1              \n",
    "\n",
    "        things_concept = things_concepts[component_id]\n",
    "        text = f'{component_id=}\\n{things_concept=}'\n",
    "\n",
    "        fsavg_data = nsd.to_fs_average_space(subject_id, volume_smooth, interp_type='nearest')\n",
    "        nsd.flat_scatter_plot(fsavg_data['lh'], fsavg_data['rh'], vmax=1, vmin=0, mask_value=-1, cmap='gray', bottomleft_text=text)\n",
    "        file_name = f'subj0{subject_id + 1}__component-{component_id}'\n",
    "        plt.savefig(out_path / file_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37653f52-46a8-4157-9eb4-d76622b53cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M_fsavg_all = []\n",
    "for subject_id in range(2):\n",
    "    run_name = 'run-44'\n",
    "\n",
    "    run_path = results_path / f'mask_optimization/torch_weights/{run_name}/subj0{subject_id + 1}'\n",
    "    run_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    M = np.load(run_path / 'mask.npy')\n",
    "    \n",
    "    kernel = torch.tensor([1, 5, 10, 10, 5, 1])\n",
    "    kernel = kernel[None, None, :] * kernel[None, :, None] * kernel[:, None, None]\n",
    "    kernel = kernel / kernel.sum() * 5\n",
    "    \n",
    "    M_fsavg = []\n",
    "    for component_id, m in enumerate(M):\n",
    "        volume = nsd.reconstruct_volume(\n",
    "            subject_id, \n",
    "            torch.from_numpy(m), \n",
    "            indices_all[subject_id],\n",
    "            -1\n",
    "        ).T.numpy()\n",
    "        D = volume.shape[2]\n",
    "        \n",
    "        volume_smooth = volume.copy()\n",
    "        volume_smooth[volume_smooth == -1] = 0\n",
    "        D = volume.shape[2]\n",
    "        \n",
    "        volume_smooth = F.conv3d(torch.from_numpy(volume_smooth[None, None]), kernel[None, None], padding='same')[0, 0].numpy()\n",
    "        \n",
    "        volume_smooth[volume_smooth >= 0.5] = 1\n",
    "        volume_smooth[volume_smooth < 0.5] = 0\n",
    "        #volume_smooth = binary_dilation(volume_smooth, iterations=1).astype(float)\n",
    "        volume_smooth[volume == -1] = -1              \n",
    "\n",
    "        things_concept = things_concepts[component_id]\n",
    "        text = f'{component_id=}\\n{things_concept=}'\n",
    "\n",
    "        fsavg_data = nsd.to_fs_average_space(subject_id, volume_smooth, interp_type='nearest')\n",
    "        M_fsavg.append(np.concatenate([fsavg_data['lh'], fsavg_data['rh']]))\n",
    "    M_fsavg_all.append(np.stack(M_fsavg))\n",
    "    \n",
    "M_fsavg_all = np.stack(M_fsavg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5df682-d75c-4754-a8af-b55c2841ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(A, B):\n",
    "    intersection = torch.einsum('...i,...i -> ...', A, B)\n",
    "    union = A.sum(axis=-1) + B.sum(axis=-1) - intersection\n",
    "    return intersection / union\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9784b1-77d6-4e71-afad-63ad9ac30772",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_fsavg_all[None, :, None, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f1b7e-60d9-4891-89d2-7d1b2860aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_fsavg_all[M_fsavg_all < 0.5] = 0.\n",
    "M_fsavg_all[M_fsavg_all >= 0.5] = 1.\n",
    "M_fsavg_jaccard = jaccard(torch.from_numpy(M_fsavg_all[:, None, :, None]), torch.from_numpy(M_fsavg_all[None, :, None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13da6f-67b9-4cf1-a394-fff6ca817972",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components, M_fsavg_jaccard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51239b2c-8751-4193-aa42-557e30f2ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_fsavg_jaccard\n",
    "\n",
    "# self similarity\n",
    "\n",
    "def jaccard_rsm_plot(subject_A, subject_B, out_path, vmax=1.):\n",
    "    jaccard_matrix = M_fsavg_jaccard[subject_A, subject_B]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(jaccard_matrix, vmin=0, vmax=vmax)\n",
    "\n",
    "    plt.title(f\"Jaccard Index of Concept Brain Masks\\nsubj0{subject_A+1} versus subj0{subject_B+1}\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(rotation=45, ha='right', labels=things_concepts3[:num_components], ticks=np.arange(num_components))\n",
    "    plt.yticks(labels=things_concepts3[:num_components], ticks=np.arange(num_components))\n",
    "\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    file_name = f'subj0{subject_A+1}__vs__subj0{subject_B+1}'\n",
    "    plt.savefig(out_path / file_name, bbox_inches='tight')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "num_components = M_fsavg_jaccard.shape[2]\n",
    "diag = torch.eye(num_components).bool()\n",
    "for subject_A in range(2):\n",
    "    jaccard_rsm_plot(subject_A, subject_A, out_path=results_path / f'mask_optimization/torch_weights/{run_name}/smoothed_rsms/jaccard/same', vmax=None)\n",
    "\n",
    "diff = []\n",
    "same = []\n",
    "for subject_A in range(2):\n",
    "    for subject_B in range(subject_A + 1, 2):\n",
    "        jaccard_matrix = M_fsavg_jaccard[subject_A, subject_B]\n",
    "        diff = np.concatenate([diff, jaccard_matrix[~diag]])\n",
    "        same = np.concatenate([same, jaccard_matrix[diag]])\n",
    "        jaccard_rsm_plot(subject_A, subject_B, out_path=results_path / f'mask_optimization/torch_weights/{run_name}/smoothed_rsms/jaccard/different', vmax=None)\n",
    "        \n",
    "diff.mean(), same.mean(), diff.std(), same.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab720de4-e005-4771-be8c-7e08385de21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "ztest(diff, same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297214f-2a3c-4116-a78d-fc42a5809e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for subject_id in range(2):\n",
    "    run_name = 'run-44'\n",
    "\n",
    "    run_path = results_path / f'mask_optimization/torch_weights/{run_name}'\n",
    "    run_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    M = torch.from_numpy(np.load(run_path / f'subj0{subject_id+1}/mask.npy')).cuda()\n",
    "    num_components = M.shape[0]\n",
    "\n",
    "    W = np.load(nsd_path / 'derivatives/things/torch_weights.npy').astype(np.float32)[:num_components]\n",
    "    W = torch.from_numpy(W).cuda()\n",
    "\n",
    "    model = models_all[subject_id]\n",
    "    model.normalize = True\n",
    "    model.eval()\n",
    "    model.load_state_dict(state_dicts_all[subject_id])\n",
    "    model.cuda()\n",
    "\n",
    "    X_test = torch.from_numpy(folds['test']['X_all'][subject_id]).cuda()\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_pred_test = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "\n",
    "    Y_test_W = Y_test @ W.T\n",
    "    r_original = pearsonr(F.relu(Y_test_W), F.relu(Y_pred_test @ W.T), reduction=None)\n",
    "    r_original = r_original[:num_components].cpu().numpy()\n",
    "    \n",
    "    r_soft_masked = []\n",
    "    r_hard_masked = []\n",
    "    for i in range(num_components):\n",
    "        m = M[i]\n",
    "        m_hard = m.clone()\n",
    "        m_hard[m >= 0.5] = 1.\n",
    "        m_hard[m < 0.5] = 0.\n",
    "        Y_test_w = Y_test_W.T[i]\n",
    "        with torch.no_grad():\n",
    "            Y_test_pred_W_soft = model(X_test * m) @ W.T\n",
    "            Y_test_pred_W_hard = model(X_test * m_hard) @ W.T\n",
    "        r_soft_masked.append(pearsonr(F.relu(Y_test_W), F.relu(Y_test_pred_W_soft), reduction=None))\n",
    "        r_hard_masked.append(pearsonr(F.relu(Y_test_W), F.relu(Y_test_pred_W_hard), reduction=None))\n",
    "    r_soft_masked = torch.stack(r_soft_masked).cpu().numpy()\n",
    "    r_hard_masked = torch.stack(r_hard_masked).cpu().numpy()\n",
    "    model.cpu()\n",
    "    \n",
    "    (run_path / f'masked_r').mkdir(exist_ok=True)\n",
    "    (run_path / f'masked_r_binary').mkdir(exist_ok=True)\n",
    "    (run_path / f'pearsonr_retained').mkdir(exist_ok=True)\n",
    "    (run_path / f'pearsonr_retained_binary').mkdir(exist_ok=True)\n",
    "    compare_pearsonr(r_original, np.diagonal(r_soft_masked), things_concepts3[:num_components],\n",
    "                     run_path / f'masked_r/subj0{subject_id+1}.png')\n",
    "    compare_pearsonr(r_original, np.diagonal(r_hard_masked), things_concepts3[:num_components],\n",
    "                     run_path / f'masked_r_binary/subj0{subject_id+1}.png')\n",
    "\n",
    "    pearsonr_retained(r_original, r_soft_masked, things_concepts3[:num_components], run_path / f'pearsonr_retained/subj0{subject_id+1}.png')\n",
    "    pearsonr_retained(r_original, r_hard_masked, things_concepts3[:num_components], run_path / f'pearsonr_retained_binary/subj0{subject_id+1}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176f77b-ecf6-4d93-a1c0-d5c0ed4fff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_means = M.mean(axis=1)\n",
    "\n",
    "width = 0.45\n",
    "plt.figure(figsize=(0.25 * num_components, 3))\n",
    "plt.title(\"Mean of All Mask Values\")\n",
    "plt.xlabel(\"Things Dimension\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.bar(np.arange(num_components), M_means, \n",
    "        tick_label=[c.split(',')[0].replace('-related', '') for c in things_concepts[:num_components]])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.savefig(run_path / 'mask_mean.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f61ddf-7281-4be1-b69b-492917f7857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_norm = M / torch.linalg.norm(M, axis=1, keepdim=True)\n",
    "M_rsm = M_norm @ M_norm.T\n",
    "\n",
    "plt.figure(figsize=(num_components * 0.2,) * 2) \n",
    "plt.imshow(M_rsm, cmap='gray', vmin=0, vmax=1)\n",
    "plt.xticks(rotation=45, ha='right', labels=things_concepts2[:num_components], ticks=np.arange(num_components))\n",
    "plt.yticks(labels=things_concepts2[:num_components], ticks=np.arange(num_components))\n",
    "plt.colorbar()\n",
    "file_name = 'cosine_similarity_rsm.png'\n",
    "plt.savefig(run_path / file_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3286c-064a-4a2c-85c6-d3dc91d40d62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc57c7-ec70-4bb3-aeae-f82bdf7dc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT-B=32'\n",
    "stimulus_key = 'embedding'\n",
    "\n",
    "save_key = stimulus_key\n",
    "save_model_name = model_name\n",
    "\n",
    "stimulus_file = h5py.File(nsd_path / f'derivatives/stimulus_embeddings/{model_name}.hdf5', 'r')\n",
    "x = stimulus_file[stimulus_key][:]\n",
    "\n",
    "stimulus_file_text = h5py.File(nsd_path / f'derivatives/stimulus_embeddings/{model_name}-text.hdf5', 'r')\n",
    "x_text = stimulus_file_text[stimulus_key][:]\n",
    "x_text = x_text / np.linalg.norm(x_text, axis=-1, keepdims=True)\n",
    "\n",
    "ids = np.stack([np.arange(73000) for _ in range(5)], axis=-1)\n",
    "print(ids.shape)\n",
    "\n",
    "#random_ids = np.arange(73000)\n",
    "#np.random.shuffle(random_ids)\n",
    "#print(random_ids)\n",
    "#x_text = x_text[random_ids]\n",
    "\n",
    "text_dists = np.einsum('ni,nti->nt', x, x_text)\n",
    "print(text_dists)\n",
    "\n",
    "neighbors = NearestNeighbors(metric='cosine')\n",
    "neighbors.fit(x)\n",
    "\n",
    "all_captions = np.array([nsd.load_coco(i)[:5] for i in range(73000)])\n",
    "best_captions = all_captions[np.arange(73000), np.argmax(text_dists, axis=1)]\n",
    "\n",
    "#top_knn_test(x, x_text.reshape(-1, 512), ids.flatten(), k=[1, 5, 10], metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63dfc1-01ff-4bcc-8be4-b07a7ea071c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = x_text[:1000, 0]\n",
    "x_sample_rsm = (x_sample[:, None] * x_sample[None, :]).sum(axis=-1)\n",
    "plt.hist(x_sample_rsm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a69e96-b0b8-4b05-8f4b-e7c85b06f805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(best_captions[:100]):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ff42b-04f2-4fc9-bf1a-2d44f47d6247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "prompt_header = 'Decompose the following sentence into multiple independent descriptive sentences. Write one sentence per line.'\n",
    "\n",
    "ids = np.arange(73000)\n",
    "random_ids = np.random.choice(ids, size=1000)\n",
    "random_captions = best_captions[random_ids]\n",
    "\n",
    "raw_responses = []\n",
    "responses = []\n",
    "for i, sentence in enumerate(random_captions):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    sentence = sentence.strip().capitalize()\n",
    "    if not sentence.endswith('.'):\n",
    "        sentence += '.'\n",
    "    response = openai.Completion.create(\n",
    "      model=\"text-davinci-003\",\n",
    "      prompt=f\"{prompt_header}\\n\\nInput:\\n{sentence}\\n\\nOutput:\",\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    raw_response = response.choices[0].text\n",
    "    raw_responses.append(raw_response)\n",
    "    \n",
    "    sentences = raw_response.strip().split('\\n')\n",
    "    sentences = [s for s in sentences if s != '']\n",
    "    responses.append(response.choices[0].text.strip().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49daa4e0-0ffa-467f-a36f-50e0f5870ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = ''\n",
    "for caption, response in zip(random_captions, raw_responses):\n",
    "    out += f'{caption}'\n",
    "    out += '\\n'.join([f'\\t{r}' for r in response.split('\\n')])\n",
    "    out += '\\n\\n'\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0d783-5bdb-43a6-a520-87434cac1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nsd_path / 'derivatives/responses.txt', 'r') as f:\n",
    "    random_captions = []\n",
    "    responses = []\n",
    "    lines = [\n",
    "        [l.strip() for l in s.split('\\n')] \n",
    "        for s in f.read().split('\\n\\n')\n",
    "    ]\n",
    "    random_captions = [l[0] for l in lines]\n",
    "    responses = [l[1:] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc3a2-6275-45ea-afa5-a2f493270105",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses\n",
    "caption_concepts = sum(responses, start=[])\n",
    "tokens = clip.tokenize(caption_concepts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    W_captions = full_model.encode_text(tokens).float()\n",
    "    \n",
    "W_captions = W_captions / torch.linalg.norm(W_captions, dim=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30814f5d-8dfb-469c-adf5-51d26ec0583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.norm(W_captions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c118bc7-d1dc-49a7-983c-ddfbbe02e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in range(8):\n",
    "    subject_name = f'subj0{subject_id + 1}'\n",
    "    \n",
    "    Y_val = torch.from_numpy(folds['val']['Y_all'][subject_id]).cuda()\n",
    "    Y_val_pred = torch.from_numpy(folds['val']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'])\n",
    "\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'])\n",
    "    \n",
    "    Y_val_W = F.relu(Y_val @ W_captions.T)\n",
    "    Y_val_pred_W = F.relu(Y_val_pred @ W_captions.T)\n",
    "    \n",
    "    r = pearsonr(Y_val_W, Y_val_pred_W, reduction=None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97296b19-9714-4c1b-bf25-4648ff4051be",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds['test']['stimulus_ids_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb455e-fe59-468a-a882-2e0ef73ccc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.cluster import hierarchy\n",
    "from research.metrics.loss_functions import ContrastiveDistanceLoss\n",
    "from research.metrics.metrics import squared_euclidean_distance\n",
    "\n",
    "run_name = 'run1'\n",
    "\n",
    "contrastive_criterion = ContrastiveDistanceLoss(squared_euclidean_distance)\n",
    "\n",
    "for subject_id in range(8):\n",
    "    params = {\n",
    "        'subject_id': subject_id,\n",
    "        'lr': 0.0001, \n",
    "        'num_iterations': 10000,\n",
    "        'mask_init': 0.5,\n",
    "        'mask_loss_weight': 0.000\n",
    "    }\n",
    "    locals().update(params)\n",
    "\n",
    "    if subject_id == 'all':\n",
    "        subject_name = subject_id\n",
    "        Y = torch.cat([torch.from_numpy(Y_all[subject_id]).cuda() for subject_id in range(8)])\n",
    "        Y_pred = torch.cat([torch.from_numpy(Y_pred_all[subject_id]).cuda() for subject_id in range(8)])\n",
    "\n",
    "        Y_test = torch.cat([torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda() for subject_id in range(8)])\n",
    "        Y_test_pred = torch.cat([torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda() for subject_id in range(8)])\n",
    "        stimulus_ids_test = torch.cat([torch.from_numpy(folds['test']['stimulus_ids_all']) for subject_id in range(8)])\n",
    "    else:\n",
    "        subject_name = f'subj0{subject_id + 1}'\n",
    "        Y = torch.from_numpy(Y_all[subject_id]).cuda()\n",
    "        Y_pred = torch.from_numpy(Y_pred_all[subject_id]).cuda()\n",
    "\n",
    "        Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "        Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "        stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'])\n",
    "\n",
    "    dataset = TensorDataset(Y, Y_pred)\n",
    "    dataloader = DataLoader(dataset, batch_size=3000)\n",
    "    data_iterator = get_data_iterator(dataloader)\n",
    "    \n",
    "    num_components = W_captions.shape[0]\n",
    "    m = torch.full((num_components,), mask_init).cuda()\n",
    "    m.requires_grad = True\n",
    "\n",
    "    optim = torch.optim.Adam([m], lr=lr)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        W_m = W_captions * m[:, None]\n",
    "        Y_batch, Y_pred_batch = next(data_iterator)\n",
    "        Y_W = F.relu(Y_batch @ W_m.T)\n",
    "        Y_pred_W = F.relu(Y_pred_batch @ W_m.T)\n",
    "\n",
    "        contrastive_loss = contrastive_criterion(Y_W, Y_pred_W)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            m = torch.clamp(m, 0., 1., out=m)\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                W_m = W_captions[m >= 0.5]\n",
    "                Y_W, Y_pred_W = F.relu(Y_test @ W_m.T), F.relu(Y_pred_test @ W_m.T)\n",
    "                \n",
    "                top_k_values = [1, 5, 10, 50, 100, 500]\n",
    "                \n",
    "                unique_stimulus_ids, unique_index, unique_inverse = np.unique(\n",
    "                    stimulus_ids_test, return_index=True, return_inverse=True)\n",
    "                top_knn_accuracy = top_knn_test(Y_W[unique_index].cpu().numpy(), Y_pred_W.cpu().numpy(), \n",
    "                                                unique_inverse, k=top_k_values, metric='euclidean')\n",
    "                print(i, top_knn_accuracy, (m >= 0.5).float().sum().cpu().item(), m.shape[0])\n",
    "                bins = (0.000, 0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 1.0)\n",
    "                m_hist, _ = np.histogram(m.detach().cpu(), bins=bins)\n",
    "                print('mask hist', [f'{b} {v}' for b, v in zip(bins, m_hist)])\n",
    "                print(f'{contrastive_loss.item()=}')\n",
    "        \n",
    "        loss = contrastive_loss + m.mean() * mask_loss_weight\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    out_path = results_path / 'masked_text_concepts' / run_name / subject_name\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with open(out_path / 'params.json', 'w') as f:\n",
    "        f.write(json.dumps(params))\n",
    "\n",
    "    np.save(out_path / 'W.npy', W_captions.cpu().numpy())\n",
    "    concept_mask = m > 0.5\n",
    "    brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, W[m > 0.5], out_path,\n",
    "                                 labels=np.array(caption_concepts)[concept_mask])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bbce46-dd49-4f53-b484-3593b6332b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc3b54-ada1-4105-9c7d-2c80c963c825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(zip(caption_concepts, r.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df68293-5e59-40e2-a466-c13d10425210",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2323726-398e-4eb4-b3ef-dae6577e6521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, r_sorted_ids = r.sort()\n",
    "n = 128\n",
    "top_ids =  r_sorted_ids.cpu().numpy()[::-1].copy()\n",
    "top_n_ids = r_sorted_ids.cpu().numpy()[-n:][::-1].copy()\n",
    "\n",
    "with open(nsd_path / 'derivatives/response_correlation.txt', 'w') as f:\n",
    "    f.write('\\n'.join([f'{b:.3f} {a}' for a, b in zip(np.array(caption_concepts)[top_ids], r[top_ids].cpu().numpy())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb060813-fb89-497a-acef-d5ad7052ca25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84225cf4-d7a2-4edc-931d-d6f5b84ed792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "top_k_values = [1]\n",
    "chance_accuracy = [k / N for k in top_k_values]\n",
    "\n",
    "metric = 'euclidean'\n",
    "\n",
    "unique_stimulus_ids_val, unique_index_val, unique_inverse_val = np.unique(\n",
    "            stimulus_ids_val, return_index=True, return_inverse=True)\n",
    "unique_stimulus_ids_test, unique_index_test, unique_inverse_test = np.unique(\n",
    "            stimulus_ids_test, return_index=True, return_inverse=True)\n",
    "\n",
    "top_knn_accuracies_val = []\n",
    "top_knn_accuracies_test = []\n",
    "pruned_dims = []\n",
    "\n",
    "num_dims = 128\n",
    "\n",
    "chosen_ids = []\n",
    "for i in range(num_dims):\n",
    "    print(f'choosing dim {i}')\n",
    "    top_knn_accuracy_best = [0]\n",
    "    for j in range(W_captions.shape[0]):\n",
    "        if j in chosen_ids:\n",
    "            continue\n",
    "            \n",
    "        Y_W, Y_pred_W = F.relu(Y_val @ W_captions[chosen_ids + [j]].T), F.relu(Y_val_pred @ W_captions[chosen_ids + [j]].T)\n",
    "\n",
    "        top_knn_accuracy = top_knn_test(\n",
    "            Y_W[unique_index_val].cpu().numpy(), Y_pred_W.cpu().numpy(), unique_inverse_val, k=top_k_values, metric=metric)\n",
    "        \n",
    "        if top_knn_accuracy[0] > top_knn_accuracy_best[0]:\n",
    "            top_knn_accuracy_best = top_knn_accuracy\n",
    "            best_j = j\n",
    "            print(j, top_knn_accuracy_best)\n",
    "    chosen_ids.append(best_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad41db-08a3-4678-9a75-ae1ecbc31de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4f043-424a-458e-89b7-6714e24f9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categories into dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=['category_id', 'category_name', 'concept', 'concept_id'])\n",
    "\n",
    "version = 'version1'\n",
    "categories_path = nsd_path / f'derivatives/gpt_categories/{version}'\n",
    "for category_file in categories_path.iterdir():\n",
    "    file_name = category_file.name.split('.')[0]\n",
    "    category_id = int(file_name[:2])\n",
    "    category_name = file_name[3:]\n",
    "    print(category_id, category_name)\n",
    "    with open(category_file, 'r') as f:\n",
    "        concepts = f.read().split('\\n')\n",
    "        for concept_id, concept in enumerate(concepts):\n",
    "            df.loc[len(df)] = [category_id, category_name, concept.lower(), concept_id]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ccb69-2447-4f5c-8782-30ca49e3c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in range(8):\n",
    "    subject_name = f'subj0{subject_id + 1}'\n",
    "    \n",
    "    Y_val = torch.from_numpy(folds['val']['Y_all'][subject_id]).cuda()\n",
    "    Y_val_pred = torch.from_numpy(folds['val']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_val = torch.from_numpy(folds['val']['stimulus_ids_all'])\n",
    "\n",
    "    Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "    Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "    stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'])\n",
    "    \n",
    "    Y_val_W = F.relu(Y_val @ W_captions.T)\n",
    "    Y_val_pred_W = F.relu(Y_val_pred @ W_captions.T)\n",
    "    \n",
    "    r = pearsonr(Y_val_W, Y_val_pred_W, reduction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8282b6-d00d-4fb4-aa74-2c7ce91de466",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc64b7-3228-4eef-93b4-c4058f093109",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = nsd_path / f'derivatives/figures/decoding/{model_name}/{group_name}/val/{embedding_name}/gpt_concepts'\n",
    "results_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbc943-ce39-4a4a-9798-830c5a2e35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459a122-40c7-4eca-a936-eb589cecc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#responses\n",
    "#caption_concepts = sum(responses, start=[])\n",
    "tokens = clip.tokenize(list(df['concept'])).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    W_captions = full_model.encode_text(tokens).float()\n",
    "    \n",
    "W_captions = W_captions / torch.linalg.norm(W_captions, dim=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041cbb6-2990-4e01-a951-7f68b0fcb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83cb6c-1d5e-40db-91bb-9e27e6faf1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(df['concept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652f1d2-c414-4c7f-beb4-96481e02dc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_W = Y_full @ W_captions.cpu().numpy().T\n",
    "Y_W = torch.from_numpy(Y_W)\n",
    "top_images_tsne(Y_full,\n",
    "                Y_W.numpy(), \n",
    "                np.arange(73000),\n",
    "                out_path=results_path / 'gpt_concepts' / 'tsne',\n",
    "                labels=list(df['concept']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14eeb42-591e-4186-8199-bf5b4b879176",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, r_sorted_ids = r.sort()\n",
    "n = 128\n",
    "top_ids =  r_sorted_ids.cpu().numpy()[::-1].copy()\n",
    "top_n_ids = r_sorted_ids.cpu().numpy()[-n:][::-1].copy()\n",
    "\n",
    "with open(results_path / 'decodability_pearsonr.txt', 'w') as f:\n",
    "    f.write('\\n'.join(\n",
    "        [f'{b:.3f} {a}' for a, b in zip(np.array(list(df['concept']))[top_ids], r[top_ids].cpu().numpy())]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109292cd-a048-43a6-b9b3-7ebfd0995ce2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cb1a6-e31d-4615-b968-60cd9f983c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_captions = []\n",
    "remove_chars = '.,\"'\n",
    "word_occurences = {}\n",
    "for stimulus_id, caption in enumerate(best_captions):\n",
    "    caption = caption.lower().strip()\n",
    "    \n",
    "    for char in remove_chars:\n",
    "        caption = caption.replace(char, '')\n",
    "    \n",
    "    for word in caption.split(' '):\n",
    "        if word not in word_occurences:\n",
    "            word_occurences[word] = [stimulus_id]\n",
    "        else:\n",
    "            word_occurences[word].append(stimulus_id)\n",
    "    \n",
    "    cleaned_captions.append(caption)\n",
    "    \n",
    "word_occurences = dict(sorted(list(word_occurences.items()), key=lambda pair: -len(pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1cf06-b491-410b-88e1-cb75d364ce71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(word_occurences))\n",
    "word_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadb85d-3fb3-4885-a7de-6ae691e544cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_occurences = {}\n",
    "for word, occurences in word_occurences.items():\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    if lemma not in lemma_occurences:\n",
    "        lemma_occurences[lemma] = occurences\n",
    "    else:\n",
    "        lemma_occurences[lemma] += occurences\n",
    "        \n",
    "lemma_occurences = dict(sorted(list(lemma_occurences.items()), key=lambda pair: -len(pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1907f5-199c-457b-964a-f9f733ef25c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stimulus_lemmas = [[] for _ in range(73000)]\n",
    "for lemma, lemma_stimulus_ids in lemma_occurences.items():\n",
    "    for stimulus_id in lemma_stimulus_ids:\n",
    "        stimulus_lemmas[stimulus_id].append(lemma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88faad-4312-4156-b613-2feeec0b0c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
    "    'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', \n",
    "    'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "    'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '', \"it's\", 'ha'\n",
    "]\n",
    "\n",
    "def top_lemmas_73k(Y_dims, num_images=10, min_occurences=3,):\n",
    "    Y_full_activations = Y_full @ Y_dims.T\n",
    "    \n",
    "    lemmas_all = []\n",
    "    for i in range(Y_dims.shape[0]):\n",
    "        y = Y_full_activations[:, i]\n",
    "        stimulus_ids = np.argsort(y)[::-1]\n",
    "        lemmas = []\n",
    "        for stimulus_id in stimulus_ids[:num_images]:\n",
    "            lemmas += set(list(stimulus_lemmas[stimulus_id]))\n",
    "        lemmas = list(zip(*np.unique(lemmas, return_counts=True)))\n",
    "        lemmas = [lemma for lemma in lemmas if lemma[1] > min_occurences and lemma[0] not in stopwords]\n",
    "        lemmas = sorted(lemmas, key=lambda pair: -pair[1])\n",
    "        lemmas_all.append(lemmas)\n",
    "    return lemmas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94278cd-50a7-4e38-994c-2027287b3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'run26'\n",
    "subject_id = 0\n",
    "\n",
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / f'subj0{subject_id+1}'\n",
    "W = np.load(out_path / 'W.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d84ea5-0583-45e9-a5f2-8e7bc5cbfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_images = top_images_73k(W, num_images=250, concatenate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993895f-369b-4d32-93ba-5e3a9a5cbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@interact(w_id=(0, 127), randomize=False)\n",
    "def show(w_id, randomize):\n",
    "    ids = np.arange(250)\n",
    "    np.random.shuffle(ids)\n",
    "    w_images = np.stack([W_images[w_id][i] for i in ids[:25]])\n",
    "    w_images = rearrange(w_images, '(i1 i2) w h c -> (i1 w) (i2 h) c', i1=5)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(w_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05750f63-81b0-4c92-892f-84cea5d6ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'run26'\n",
    "subject_id = 0\n",
    "\n",
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / f'subj0{subject_id+1}'\n",
    "W = np.load(out_path / 'W.npy')\n",
    "num_images = 250\n",
    "min_occurences = 10\n",
    "W_lemmas = top_lemmas_73k(W, num_images=num_images, min_occurences=min_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ed59b-8aa1-42c4-9595-405edc5e5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_per_line = 30\n",
    "labels = []\n",
    "for lemmas in W_lemmas:\n",
    "    lemmas = [lemma[0] for lemma in lemmas]\n",
    "    lines = []\n",
    "    line = lemmas[0]\n",
    "    for lemma in lemmas[1:]:\n",
    "        if len(f'{line}, {lemma}') > characters_per_line:\n",
    "            lines.append(line)\n",
    "            line = lemma\n",
    "        else:\n",
    "            line = f'{line}, {lemma}'\n",
    "    label = '\\n'.join(lines)\n",
    "    labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c2602-15d2-4b2b-b7df-9afa60d6008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_all = top_images_73k(W, num_images=10, labels=labels, font_size=20)\n",
    "file_name = f'top_images_73k_lemmas_images-{num_images}_occurences-{min_occurences}.png'\n",
    "Image.fromarray(images_all).save(out_path / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d97c4b-ecae-434d-a09b-5b1a41782668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_lemmas_union = []\n",
    "for lemmas in W_lemmas:\n",
    "    W_lemmas_union += [lemma[0] for lemma in lemmas]\n",
    "W_lemmas_union = list(set(W_lemmas_union))\n",
    "len(W_lemmas_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226535b3-f447-48d8-bf65-d7c25f98e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a66db-c1f4-42a4-9139-89bce17be608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C = np.zeros((len(W_lemmas_union), W.shape[0]))\n",
    "for w_id, w_lemmas in enumerate(W_lemmas):\n",
    "    w_lemmas = [l[0] for l in w_lemmas]\n",
    "    print(w_id, w_lemmas)\n",
    "    for lemma_id, lemma in enumerate(W_lemmas_union):\n",
    "        if lemma in w_lemmas:\n",
    "            print(lemma, lemma_id, w_id)\n",
    "            C[w_id, lemma_id] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45d4f9-b2c2-437d-a8db-6033dfc4a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lemmas[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bfa15-e25c-4f85-8f63-bdef7083155a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[len(lemmas) for lemmas in W_lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c7c18-d4e6-4843-a713-9aa7c6e8cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lemmas_union[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6db50-05a5-45d9-93e3-23179316d146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc860d-b188-4efc-9561-1262bd687135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 24))\n",
    "plt.yticks(ticks=[i*5 for i in range(len(W_lemmas_union) // 5)])\n",
    "plt.imshow(C)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97c1d9-bef3-484c-ae29-c86a82d128e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "model = RidgeCV(fit_intercept=False)\n",
    "\n",
    "model.fit(C.T, W)\n",
    "W.shape, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32c182-a621-486e-a030-dffa716cc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = model.coef_\n",
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20cce8-a612-4ee0-a673-8ee694f82222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_lemmas_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddd417-1550-4d06-91ca-a1d44ec4c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "out_path = results_path / 'optimized_clip_dimensions' / run_name / f'subj0{subject_id+1}' / 'lemmas'\n",
    "subject_id = 0\n",
    "\n",
    "Y = torch.from_numpy(Y_all[subject_id]).cuda()\n",
    "Y_pred = torch.from_numpy(Y_pred_all[subject_id]).cuda()\n",
    "\n",
    "Y_test = torch.from_numpy(folds['test']['Y_all'][subject_id]).cuda()\n",
    "Y_test_pred = torch.from_numpy(folds['test']['Y_pred_all'][subject_id]).cuda()\n",
    "stimulus_ids_test = torch.from_numpy(folds['test']['stimulus_ids_all'][subject_id])\n",
    "        \n",
    "brain_decodability_evaluation(Y_test, Y_test_pred, Y_full, stimulus_ids_test, torch.from_numpy(E.T).float().cuda(), out_path, labels=W_lemmas_union)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-decode",
   "language": "python",
   "name": "neuro-decode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
