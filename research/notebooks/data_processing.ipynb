{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"X:\\\\Datasets\\\\EEG\\\\Things-supplementary\\\\Inversions\\\\things_stimulus01_inverted.hdf5\")\n",
    "dataset = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "N = dataset['xtrain'].shape[0]\n",
    "@interact(i=(0, N-1))\n",
    "def show(i):\n",
    "    img = dataset['xtrain'][i]\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    \n",
    "    label_id = dataset['ytrain'][i]\n",
    "    \n",
    "    print(classes[label_id])\n",
    "    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from research.imagenet_classes import classes\n",
    "\n",
    "dataset_path = Path(\"G:\\\\Github Repositories\\\\exploiting-gan-internal-capacity\\\\inverses\\\\dataset.hdf5\")\n",
    "dataset = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "N = dataset['xtrain'].shape[0]\n",
    "@interact(i=(0, N-1))\n",
    "def show(i):\n",
    "    img = dataset['xtrain'][i]\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    \n",
    "    label_id = dataset['ytrain'][i]\n",
    "    \n",
    "    print(classes[label_id])\n",
    "    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dd253cab4f46a39d9261c6222839aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=13053, description='i', max=26106), Output()), _dom_classes=('widget-intâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from research.things_dataset import ThingsDataset\n",
    "from research.imagenet_classes import imagenet_classes\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "things_dataset = ThingsDataset(\n",
    "    root=\"X:\\\\Datasets\\\\EEG\\\\Things-concepts-and-images\\\\\",\n",
    "    transform=transform\n",
    ")\n",
    "things_dataset.image_concept_ids\n",
    "\n",
    "#print(things_dataset.image_concept_ids)\n",
    "\n",
    "@interact(i=(0, len(things_dataset)-1))\n",
    "def show(i):\n",
    "    image = things_dataset[i]\n",
    "    print(image['unique_id'])\n",
    "    data = image['data']\n",
    "    \n",
    "    #x = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(data)\n",
    "    #y_pred = resnext101_32x8d(x[None])\n",
    "    #_, class_id = torch.max(y_pred, 1)\n",
    "    #class_id = int(class_id)\n",
    "    #print(\"predicted:\", imagenet_classes[class_id])\n",
    "    \n",
    "    data = torch.moveaxis(data , 0, -1)\n",
    "    plt.imshow(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnext101_32x8d = models.resnext101_32x8d(pretrained=True)\n",
    "resnet152 = models.resnet152(pretrained=True)\n",
    "resnet101 = models.resnet101(pretrained=True)\n",
    "resnext101_32x8d.eval()\n",
    "resnet152.eval()\n",
    "resnet101.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = resnet152\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = model.to(device)\n",
    "dataloader = DataLoader(things_dataset, batch_size=8)\n",
    "for batch in dataloader:\n",
    "    data = batch['data']\n",
    "    \n",
    "    x = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(data)\n",
    "    x = x.to(device)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# For faster load times, download these files locally and use the local paths instead.\n",
    "enc = load_model(\"X:\\Models\\dall-e\\encoder.pkl\", device)\n",
    "dec = load_model(\"X:\\Models\\dall-e\\decoder.pkl\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "@interact(i=(0, len(things_dataset)-1))\n",
    "def show(i):\n",
    "    image = things_dataset[i]\n",
    "    print(image['concept_name'])\n",
    "    data = image['data']\n",
    "    data = map_pixels(data[None])\n",
    "    data = data.to(device)\n",
    "    \n",
    "    z_logits = enc(data)\n",
    "    print(z_logits.shape)\n",
    "    \n",
    "    z = torch.argmax(z_logits, axis=1)\n",
    "    z = F.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()\n",
    "    print(z.shape)\n",
    "\n",
    "    x_stats = dec(z).float()\n",
    "    x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n",
    "    x_rec = T.ToPILImage(mode='RGB')(x_rec[0])\n",
    "\n",
    "    display_markdown('Reconstructed image:')\n",
    "    display(x_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('yarn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_missing_synsets = things_dataset.concepts[things_dataset.concepts['Wordnet ID4'].isnull()]\n",
    "\n",
    "print(concepts_missing_synsets[[\"uniqueID\",  \"WordNet Synonyms\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from research.imagenet_wordnet_ids import imagenet_wordnet_ids_raw\n",
    "\n",
    "\n",
    "things_wordnet_ids = list(things_dataset.concepts['Wordnet ID4'])\n",
    "\n",
    "things_synsets = [wn.synset(wordnet_id) if isinstance(wordnet_id, str) else None \n",
    "                  for wordnet_id in things_wordnet_ids]\n",
    "\n",
    "imagenet_synsets = [\n",
    "    wn.synset_from_pos_and_offset(wordnet_id[0], int(wordnet_id[1:]))\n",
    "    for wordnet_id, _ in imagenet_wordnet_ids_raw.values()\n",
    "]\n",
    "\n",
    "similarity_matrix = np.zeros(shape=(len(imagenet_synsets), len(things_synsets)))\n",
    "for i, imagenet_synset in enumerate(imagenet_synsets):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    for j, things_synset in enumerate(things_synsets):\n",
    "        if imagenet_synset and things_synset:\n",
    "            sim = imagenet_synset.path_similarity(things_synset)\n",
    "            similarity_matrix[i, j] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.argsort()[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "k_args = similarity_matrix.argsort(axis=0)\n",
    "print(k_args.shape)\n",
    "\n",
    "for imagenet_synset, k_arg in zip(imagenet_synsets, list(k_args)):\n",
    "    print(imagenet_synset, [things_synsets[arg] for arg in k_arg])\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"imagenet_things_similarity_01.csv\", similarity_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.loadtxt(\"imagenet_things_similarity_01.csv\", dtype=float, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.isnan(things_wordnet_ids[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"john.n.02\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
